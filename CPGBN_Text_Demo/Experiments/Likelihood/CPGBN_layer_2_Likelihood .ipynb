{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "Preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Tensorflow initial finished\n",
      "CUDA initial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.RandomState(1)\n",
    "\n",
    "realmin = 2.2e-10\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "#====================== Load data ======================#\n",
    "import cPickle\n",
    "\n",
    "DATA = cPickle.load(open(\"./TREC_3k-12-6.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = DATA['Vocabulary']\n",
    "data_vab_count_list    = DATA['Vab_count']\n",
    "data_vab_length        = DATA['Vab_Size']\n",
    "data_label             = DATA['Label']\n",
    "data_train_list        = DATA['Train_Origin']\n",
    "data_train_label       = np.array(DATA['Train_Label'])\n",
    "data_train_split       = DATA['Train_Word_Split']\n",
    "data_train_list_index  = DATA['Train_Word2Index']\n",
    "data_test_list         = DATA['Test_Origin']\n",
    "data_test_label        = np.array(DATA['Test_Label'])\n",
    "data_test_split        = DATA['Test_Word_Split']\n",
    "data_test_list_index   = DATA['Test_Word2Index']\n",
    "data_value             = 10\n",
    "\n",
    "print 'Load data'\n",
    "\n",
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_train_list)): \n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0)\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "\n",
    "print 'Preprocess finished'\n",
    "\n",
    "batch_len_tr        = batch_len\n",
    "batch_rows_tr       = batch_rows\n",
    "batch_cols_tr       = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "#======================= Setting =======================#\n",
    "Setting = {}\n",
    "Setting['N_train']    = len(data_train_list) - delete_count\n",
    "Setting['N_test']     = len(data_test_list)\n",
    "# 1-th layer\n",
    "Setting['K1']         = 32\n",
    "Setting['K1_V1']      = DATA['Vab_Size']\n",
    "Setting['K1_V2']      = np.max(batch_len) + 2  # padding\n",
    "Setting['K1_S3']      = DATA['Vab_Size']\n",
    "Setting['K1_S4']      = 3\n",
    "Setting['K1_S1']      = Setting['K1_V1'] + 1 - Setting['K1_S3'] # 1\n",
    "Setting['K1_S2']      = Setting['K1_V2'] + 1 - Setting['K1_S4'] # 26\n",
    "# 2-th layer\n",
    "Setting['K2']         = 16\n",
    "\n",
    "Setting['Iter']       = 200\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "#======================= SuperParams =======================#\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.05 # Phi\n",
    "\n",
    "#======================= Tensorflow Initial =======================#\n",
    "# Initial Graph\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) #HWC\n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print 'Tensorflow initial finished'\n",
    "\n",
    "#====================== CUDA Initial ======================#\n",
    "# Noteï¼Œ do not add any cuda operation among CUDA initial such as Tensorflow!!!!!!!!!!!!!!!!!!\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"CUDA initial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 0.284114122391 seconds\n",
      "Likelihood -519.0487388757961 takes 34.09922719 seconds\n",
      "epoch 1 takes 0.305480003357 seconds\n",
      "epoch 2 takes 0.30645608902 seconds\n",
      "epoch 3 takes 0.320538043976 seconds\n",
      "epoch 4 takes 0.297740936279 seconds\n",
      "epoch 5 takes 0.349878787994 seconds\n",
      "epoch 6 takes 0.299003124237 seconds\n",
      "epoch 7 takes 0.300645112991 seconds\n",
      "epoch 8 takes 0.329251050949 seconds\n",
      "epoch 9 takes 0.301178216934 seconds\n",
      "epoch 10 takes 0.293302059174 seconds\n",
      "epoch 11 takes 0.308120012283 seconds\n",
      "epoch 12 takes 0.290215015411 seconds\n",
      "epoch 13 takes 0.295328855515 seconds\n",
      "epoch 14 takes 0.2885658741 seconds\n",
      "epoch 15 takes 0.311085939407 seconds\n",
      "epoch 16 takes 0.280186891556 seconds\n",
      "epoch 17 takes 0.293563842773 seconds\n",
      "epoch 18 takes 0.302922010422 seconds\n",
      "epoch 19 takes 0.298038005829 seconds\n",
      "epoch 20 takes 0.29408288002 seconds\n",
      "epoch 21 takes 0.291043996811 seconds\n",
      "epoch 22 takes 0.309056043625 seconds\n",
      "epoch 23 takes 0.304363012314 seconds\n",
      "epoch 24 takes 0.311599969864 seconds\n",
      "epoch 25 takes 0.328197002411 seconds\n",
      "epoch 26 takes 0.315973997116 seconds\n",
      "epoch 27 takes 0.306700944901 seconds\n",
      "epoch 28 takes 0.305424928665 seconds\n",
      "epoch 29 takes 0.325140953064 seconds\n",
      "epoch 30 takes 0.303297042847 seconds\n",
      "epoch 31 takes 0.355803966522 seconds\n",
      "epoch 32 takes 0.365043878555 seconds\n",
      "epoch 33 takes 0.366182088852 seconds\n",
      "epoch 34 takes 0.328043937683 seconds\n",
      "epoch 35 takes 0.281622171402 seconds\n",
      "epoch 36 takes 0.316325902939 seconds\n",
      "epoch 37 takes 0.316939115524 seconds\n",
      "epoch 38 takes 0.321954965591 seconds\n",
      "epoch 39 takes 0.326133966446 seconds\n",
      "epoch 40 takes 0.30575299263 seconds\n",
      "epoch 41 takes 0.288761138916 seconds\n",
      "epoch 42 takes 0.297239780426 seconds\n",
      "epoch 43 takes 0.306931972504 seconds\n",
      "epoch 44 takes 0.285305023193 seconds\n",
      "epoch 45 takes 0.289613962173 seconds\n",
      "epoch 46 takes 0.299072027206 seconds\n",
      "epoch 47 takes 0.329538822174 seconds\n",
      "epoch 48 takes 0.368638038635 seconds\n",
      "epoch 49 takes 0.369626998901 seconds\n",
      "epoch 50 takes 0.318510055542 seconds\n",
      "Likelihood -228.99600611517525 takes 35.3159980774 seconds\n",
      "epoch 51 takes 0.291243076324 seconds\n",
      "epoch 52 takes 0.302226781845 seconds\n",
      "epoch 53 takes 0.283319950104 seconds\n",
      "epoch 54 takes 0.302228927612 seconds\n",
      "epoch 55 takes 0.30278301239 seconds\n",
      "epoch 56 takes 0.308023929596 seconds\n",
      "epoch 57 takes 0.32297205925 seconds\n",
      "epoch 58 takes 0.322055101395 seconds\n",
      "epoch 59 takes 0.318516016006 seconds\n",
      "epoch 60 takes 0.325773000717 seconds\n",
      "epoch 61 takes 0.312516927719 seconds\n",
      "epoch 62 takes 0.315136909485 seconds\n",
      "epoch 63 takes 0.310137987137 seconds\n",
      "epoch 64 takes 0.315166950226 seconds\n",
      "epoch 65 takes 0.298429012299 seconds\n",
      "epoch 66 takes 0.285688877106 seconds\n",
      "epoch 67 takes 0.291069984436 seconds\n",
      "epoch 68 takes 0.307102918625 seconds\n",
      "epoch 69 takes 0.288885116577 seconds\n",
      "epoch 70 takes 0.30867600441 seconds\n",
      "epoch 71 takes 0.308434009552 seconds\n",
      "epoch 72 takes 0.308110952377 seconds\n",
      "epoch 73 takes 0.298773050308 seconds\n",
      "epoch 74 takes 0.296557188034 seconds\n",
      "epoch 75 takes 0.313025951385 seconds\n",
      "epoch 76 takes 0.301247835159 seconds\n",
      "epoch 77 takes 0.288758993149 seconds\n",
      "epoch 78 takes 0.307457208633 seconds\n",
      "epoch 79 takes 0.326328992844 seconds\n",
      "epoch 80 takes 0.303138017654 seconds\n",
      "epoch 81 takes 0.301187038422 seconds\n",
      "epoch 82 takes 0.288396120071 seconds\n",
      "epoch 83 takes 0.299551010132 seconds\n",
      "epoch 84 takes 0.289934873581 seconds\n",
      "epoch 85 takes 0.385429859161 seconds\n",
      "epoch 86 takes 0.364062070847 seconds\n",
      "epoch 87 takes 0.374540805817 seconds\n",
      "epoch 88 takes 0.386037111282 seconds\n",
      "epoch 89 takes 0.367126941681 seconds\n",
      "epoch 90 takes 0.375447034836 seconds\n",
      "epoch 91 takes 0.374134063721 seconds\n",
      "epoch 92 takes 0.370431184769 seconds\n",
      "epoch 93 takes 0.364378929138 seconds\n",
      "epoch 94 takes 0.368403911591 seconds\n",
      "epoch 95 takes 0.367233991623 seconds\n",
      "epoch 96 takes 0.372614860535 seconds\n",
      "epoch 97 takes 0.364989042282 seconds\n",
      "epoch 98 takes 0.361586093903 seconds\n",
      "epoch 99 takes 0.388561964035 seconds\n",
      "epoch 100 takes 0.377061128616 seconds\n",
      "Likelihood -227.9522310423035 takes 35.8249239922 seconds\n",
      "epoch 101 takes 0.312741994858 seconds\n",
      "epoch 102 takes 0.342457056046 seconds\n",
      "epoch 103 takes 0.327348947525 seconds\n",
      "epoch 104 takes 0.313580036163 seconds\n",
      "epoch 105 takes 0.294214010239 seconds\n",
      "epoch 106 takes 0.308412075043 seconds\n",
      "epoch 107 takes 0.294512987137 seconds\n",
      "epoch 108 takes 0.297570943832 seconds\n",
      "epoch 109 takes 0.296140909195 seconds\n",
      "epoch 110 takes 0.34339094162 seconds\n",
      "epoch 111 takes 0.333395004272 seconds\n",
      "epoch 112 takes 0.33878993988 seconds\n",
      "epoch 113 takes 0.327100992203 seconds\n",
      "epoch 114 takes 0.342035055161 seconds\n",
      "epoch 115 takes 0.329860210419 seconds\n",
      "epoch 116 takes 0.359580993652 seconds\n",
      "epoch 117 takes 0.312173128128 seconds\n",
      "epoch 118 takes 0.312114953995 seconds\n",
      "epoch 119 takes 0.298984050751 seconds\n",
      "epoch 120 takes 0.309422016144 seconds\n",
      "epoch 121 takes 0.315161943436 seconds\n",
      "epoch 122 takes 0.287606954575 seconds\n",
      "epoch 123 takes 0.28441119194 seconds\n",
      "epoch 124 takes 0.311325073242 seconds\n",
      "epoch 125 takes 0.291229963303 seconds\n",
      "epoch 126 takes 0.30614900589 seconds\n",
      "epoch 127 takes 0.315325975418 seconds\n",
      "epoch 128 takes 0.286149024963 seconds\n",
      "epoch 129 takes 0.294910907745 seconds\n",
      "epoch 130 takes 0.343351840973 seconds\n",
      "epoch 131 takes 0.308559894562 seconds\n",
      "epoch 132 takes 0.323596954346 seconds\n",
      "epoch 133 takes 0.33268904686 seconds\n",
      "epoch 134 takes 0.327894926071 seconds\n",
      "epoch 135 takes 0.329092979431 seconds\n",
      "epoch 136 takes 0.352936029434 seconds\n",
      "epoch 137 takes 0.413790941238 seconds\n",
      "epoch 138 takes 0.388597011566 seconds\n",
      "epoch 139 takes 0.375504016876 seconds\n",
      "epoch 140 takes 0.372161149979 seconds\n",
      "epoch 141 takes 0.380460977554 seconds\n",
      "epoch 142 takes 0.380887985229 seconds\n",
      "epoch 143 takes 0.370439052582 seconds\n",
      "epoch 144 takes 0.373600006104 seconds\n",
      "epoch 145 takes 0.387979984283 seconds\n",
      "epoch 146 takes 0.380536079407 seconds\n",
      "epoch 147 takes 0.372013092041 seconds\n",
      "epoch 148 takes 0.370275020599 seconds\n",
      "epoch 149 takes 0.381464004517 seconds\n",
      "epoch 150 takes 0.377543926239 seconds\n",
      "Likelihood -227.54051280170842 takes 39.3284139633 seconds\n",
      "epoch 151 takes 0.288751125336 seconds\n",
      "epoch 152 takes 0.305284976959 seconds\n",
      "epoch 153 takes 0.300201892853 seconds\n",
      "epoch 154 takes 0.298846960068 seconds\n",
      "epoch 155 takes 0.29278087616 seconds\n",
      "epoch 156 takes 0.293006896973 seconds\n",
      "epoch 157 takes 0.294955968857 seconds\n",
      "epoch 158 takes 0.293147802353 seconds\n",
      "epoch 159 takes 0.295781135559 seconds\n",
      "epoch 160 takes 0.302352905273 seconds\n",
      "epoch 161 takes 0.310818910599 seconds\n",
      "epoch 162 takes 0.302989006042 seconds\n",
      "epoch 163 takes 0.307681083679 seconds\n",
      "epoch 164 takes 0.298798084259 seconds\n",
      "epoch 165 takes 0.318437099457 seconds\n",
      "epoch 166 takes 0.284376859665 seconds\n",
      "epoch 167 takes 0.293666124344 seconds\n",
      "epoch 168 takes 0.29807806015 seconds\n",
      "epoch 169 takes 0.31370306015 seconds\n",
      "epoch 170 takes 0.287257909775 seconds\n",
      "epoch 171 takes 0.308423995972 seconds\n",
      "epoch 172 takes 0.295341014862 seconds\n",
      "epoch 173 takes 0.283872842789 seconds\n",
      "epoch 174 takes 0.293487071991 seconds\n",
      "epoch 175 takes 0.314301967621 seconds\n",
      "epoch 176 takes 0.305837154388 seconds\n",
      "epoch 177 takes 0.309679031372 seconds\n",
      "epoch 178 takes 0.317864894867 seconds\n",
      "epoch 179 takes 0.315603017807 seconds\n",
      "epoch 180 takes 0.326844930649 seconds\n",
      "epoch 181 takes 0.331274986267 seconds\n",
      "epoch 182 takes 0.309190988541 seconds\n",
      "epoch 183 takes 0.294260025024 seconds\n",
      "epoch 184 takes 0.306349039078 seconds\n",
      "epoch 185 takes 0.301719903946 seconds\n",
      "epoch 186 takes 0.311028003693 seconds\n",
      "epoch 187 takes 0.298367977142 seconds\n",
      "epoch 188 takes 0.314706802368 seconds\n",
      "epoch 189 takes 0.312860012054 seconds\n",
      "epoch 190 takes 0.303029060364 seconds\n",
      "epoch 191 takes 0.324414014816 seconds\n",
      "epoch 192 takes 0.30948805809 seconds\n",
      "epoch 193 takes 0.296447038651 seconds\n",
      "epoch 194 takes 0.30882191658 seconds\n",
      "epoch 195 takes 0.383234024048 seconds\n",
      "epoch 196 takes 0.388696908951 seconds\n",
      "epoch 197 takes 0.379317998886 seconds\n",
      "epoch 198 takes 0.365881919861 seconds\n",
      "epoch 199 takes 0.389762878418 seconds\n",
      "Train phase finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Initial Params =======================#\n",
    "import PGBN_sampler \n",
    "from scipy.special import gamma\n",
    "Params = {}\n",
    "\n",
    "# 1-th layer\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1']         = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "# 2-th layer\n",
    "Params['Phi_2']  = 0.2 + 0.8*np.random.rand(Setting['K1'], Setting['K2'])\n",
    "Params['Phi_2']  = Params['Phi_2'] / np.sum(Params['Phi_2'], axis=0)\n",
    "Params['Theta_2']= np.random.rand(Setting['N_train'], Setting['K2'])\n",
    "Params['c3_n']   = 1 * np.ones([Setting['N_train']])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K2'], 1]) / Setting['K2']\n",
    "\n",
    "# Collection\n",
    "W_train_1 = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "W_train_2 = np.zeros([Setting['N_train'], Setting['K2']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "import time\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "\n",
    "#========================== Gibbs ==========================#\n",
    "for t in range(Setting['Iter']):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================#\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])     # Augmentation on D\n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1'])    # Augmentation on w\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32') \n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1  # padding \n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "    \n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # ä¸€èˆ¬æœ€å¤š512ä¸ªå¹¶è¡Œçº¿ç¨‹\n",
    "    \n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64') # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')  # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3),axis=2) # N*K1\n",
    "    \n",
    "    #========================== 2nd layer Augmentation ==========================#\n",
    "    M1_tmp = np.array(np.transpose(np.round(Params['W1_nk1_Aug_Pooling'])), dtype='float64', order='C')\n",
    "    Theta2_tmp = np.array(np.transpose(Params['Theta_2']), dtype='float64', order='C')\n",
    "    Xt_to_t1_2,WSZS_2 = PGBN_sampler.Crt_Multirnd_Matrix(M1_tmp, Params['Phi_2'], Theta2_tmp)\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update D,Phi\n",
    "    for k1 in range(Setting['K1']):\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] \n",
    "        X_k1_34_tmp = np.random.gamma(X_k1_34 + SuperParams['eta'])\n",
    "        D1_k1_s     = X_k1_34_tmp / np.sum(X_k1_34_tmp)\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "        \n",
    "    Phi_2_tmp       = np.random.gamma(WSZS_2 + SuperParams['eta'])\n",
    "    Params['Phi_2'] = Phi_2_tmp / np.sum(Phi_2_tmp, axis=0)\n",
    "    \n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_2'], Params['Theta_2'].T),0)) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    Params['c3_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c3_n']     = Params['c3_n'] / (SuperParams['f0'] + np.sum(Params['Theta_2'],axis=1))   \n",
    "\n",
    "    # Update w_j\n",
    "    W_k2_sn = np.random.gamma(Params['Gamma'] + Xt_to_t1_2) / (-np.log(1-Params['p2_n']) + Params['c3_n']) # V*N\n",
    "    Params['Theta_2'] = np.transpose(W_k2_sn)\n",
    "    \n",
    "    shape1 = np.dot(Params['Phi_2'], Params['Theta_2'].T) # V*N\n",
    "    W_k1_sn = np.random.gamma(shape1 + Params['W1_nk1_Aug_Pooling'].T ) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn) \n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    if t == 0:\n",
    "        Iter_time.append(end_time - start_time)\n",
    "    else:\n",
    "        Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "    #====================== Likelihood ======================#\n",
    "    if np.mod(t,50) == 0:\n",
    "\n",
    "        likelyhood = 0\n",
    "        start_time = time.time()\n",
    "        Orgin_X = np.zeros([Setting['N_train'], Setting['K1_V1'], Setting['K1_V2']])\n",
    "        Orgin_X[[batch_file_index, batch_rows, batch_cols+1]] = batch_value\n",
    "\n",
    "        for i in range(Setting['N_train']):\n",
    "\n",
    "            Phi_tmp = np.transpose(np.reshape(Params['D1_k1'],[Setting['K1'], Setting['K1_S3'], Setting['K1_S4'], 1]),[1,2,3,0])\n",
    "            Theta_tmp = np.transpose(Params['W1_nk1'][i:i+1,:,:,:], [0,2,3,1])\n",
    "            PhiTheta_1= sess.run(X_1, feed_dict={Phi_1:Phi_tmp.astype(np.float32), Theta_1:Theta_tmp.astype(np.float32)})\n",
    "            PhiTheta_1= PhiTheta_1.astype(np.float64)\n",
    "            likelyhood = likelyhood + np.sum(Orgin_X[i,:,:] * log_max(PhiTheta_1[0,:,:,0]) - PhiTheta_1[0,:,:,0] - log_max(gamma(Orgin_X[i,:,:] + 1)))  \n",
    "        \n",
    "        end_time = time.time()\n",
    "        print \"Likelihood \" + str(likelyhood / Setting['N_train']) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "        Iter_lh.append(likelyhood / Setting['N_train'])\n",
    "    \n",
    "print \"Train phase finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
