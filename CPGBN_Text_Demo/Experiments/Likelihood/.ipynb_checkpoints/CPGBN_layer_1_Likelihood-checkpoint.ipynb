{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "Preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Tensorflow initial finished\n",
      "CUDA initial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.RandomState(1)\n",
    "\n",
    "realmin = 2.2e-10\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "#====================== Load data ======================#\n",
    "import cPickle\n",
    "\n",
    "DATA = cPickle.load(open(\"./TREC_3k-12-6.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = DATA['Vocabulary']\n",
    "data_vab_count_list    = DATA['Vab_count']\n",
    "data_vab_length        = DATA['Vab_Size']\n",
    "data_label             = DATA['Label']\n",
    "data_train_list        = DATA['Train_Origin']\n",
    "data_train_label       = np.array(DATA['Train_Label'])\n",
    "data_train_split       = DATA['Train_Word_Split']\n",
    "data_train_list_index  = DATA['Train_Word2Index']\n",
    "data_test_list         = DATA['Test_Origin']\n",
    "data_test_label        = np.array(DATA['Test_Label'])\n",
    "data_test_split        = DATA['Test_Word_Split']\n",
    "data_test_list_index   = DATA['Test_Word2Index']\n",
    "data_value             = 10\n",
    "\n",
    "print 'Load data'\n",
    "\n",
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_train_list)):\n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)                                         \n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0) \n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "\n",
    "print 'Preprocess finished'\n",
    "\n",
    "batch_len_tr        = batch_len\n",
    "batch_rows_tr       = batch_rows\n",
    "batch_cols_tr       = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "#======================= Setting =======================#\n",
    "Setting = {}\n",
    "Setting['N_train']    = len(data_train_list) - delete_count \n",
    "Setting['K1']         = 32\n",
    "Setting['K1_V1']      = DATA['Vab_Size']\n",
    "Setting['K1_V2']      = np.max(batch_len) + 2  # padding             　\n",
    "Setting['K1_S3']      = DATA['Vab_Size']\n",
    "Setting['K1_S4']      = 3\n",
    "Setting['K1_S1']      = Setting['K1_V1'] + 1 - Setting['K1_S3']\n",
    "Setting['K1_S2']      = Setting['K1_V2'] + 1 - Setting['K1_S4']   \n",
    "\n",
    "Setting['Iter']       = 200\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "#======================= SuperParams =======================#\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.05 # Phi\n",
    "\n",
    "#======================= Tensorflow Initial =======================#\n",
    "# Initial Graph\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) #HWC\n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print 'Tensorflow initial finished'\n",
    "\n",
    "#====================== CUDA Initial ======================#\n",
    "# Note， do not add any cuda operation among CUDA initial such as Tensorflow!!!!!!!!!!!!!!!!!!\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"CUDA initial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 0.207113981247 seconds\n",
      "Likelihood -1267.714801307131 takes 27.1473531723 seconds\n",
      "epoch 1 takes 0.188264131546 seconds\n",
      "epoch 2 takes 0.183929204941 seconds\n",
      "epoch 3 takes 0.183134078979 seconds\n",
      "epoch 4 takes 0.188502073288 seconds\n",
      "epoch 5 takes 0.202018022537 seconds\n",
      "epoch 6 takes 0.19130396843 seconds\n",
      "epoch 7 takes 0.212840080261 seconds\n",
      "epoch 8 takes 0.187429904938 seconds\n",
      "epoch 9 takes 0.19215798378 seconds\n",
      "epoch 10 takes 0.18748497963 seconds\n",
      "epoch 11 takes 0.192721843719 seconds\n",
      "epoch 12 takes 0.206775903702 seconds\n",
      "epoch 13 takes 0.196552038193 seconds\n",
      "epoch 14 takes 0.203095912933 seconds\n",
      "epoch 15 takes 0.199639081955 seconds\n",
      "epoch 16 takes 0.207841873169 seconds\n",
      "epoch 17 takes 0.204576015472 seconds\n",
      "epoch 18 takes 0.201128959656 seconds\n",
      "epoch 19 takes 0.190614938736 seconds\n",
      "epoch 20 takes 0.19217300415 seconds\n",
      "epoch 21 takes 0.216048955917 seconds\n",
      "epoch 22 takes 0.203029870987 seconds\n",
      "epoch 23 takes 0.197026014328 seconds\n",
      "epoch 24 takes 0.184854984283 seconds\n",
      "epoch 25 takes 0.201342821121 seconds\n",
      "epoch 26 takes 0.187332868576 seconds\n",
      "epoch 27 takes 0.199831962585 seconds\n",
      "epoch 28 takes 0.198665142059 seconds\n",
      "epoch 29 takes 0.201065063477 seconds\n",
      "epoch 30 takes 0.186563968658 seconds\n",
      "epoch 31 takes 0.21245598793 seconds\n",
      "epoch 32 takes 0.186305046082 seconds\n",
      "epoch 33 takes 0.199579000473 seconds\n",
      "epoch 34 takes 0.188552856445 seconds\n",
      "epoch 35 takes 0.198606014252 seconds\n",
      "epoch 36 takes 0.182190895081 seconds\n",
      "epoch 37 takes 0.198749065399 seconds\n",
      "epoch 38 takes 0.189135074615 seconds\n",
      "epoch 39 takes 0.189501047134 seconds\n",
      "epoch 40 takes 0.20432305336 seconds\n",
      "epoch 41 takes 0.190552949905 seconds\n",
      "epoch 42 takes 0.202198028564 seconds\n",
      "epoch 43 takes 0.191321134567 seconds\n",
      "epoch 44 takes 0.18362903595 seconds\n",
      "epoch 45 takes 0.180441141129 seconds\n",
      "epoch 46 takes 0.182797908783 seconds\n",
      "epoch 47 takes 0.184331893921 seconds\n",
      "epoch 48 takes 0.194231987 seconds\n",
      "epoch 49 takes 0.185719966888 seconds\n",
      "epoch 50 takes 0.198813915253 seconds\n",
      "Likelihood -585.0247045055771 takes 24.8998332024 seconds\n",
      "epoch 51 takes 0.202349901199 seconds\n",
      "epoch 52 takes 0.193563938141 seconds\n",
      "epoch 53 takes 0.196943998337 seconds\n",
      "epoch 54 takes 0.197470188141 seconds\n",
      "epoch 55 takes 0.209341049194 seconds\n",
      "epoch 56 takes 0.20309305191 seconds\n",
      "epoch 57 takes 0.20139503479 seconds\n",
      "epoch 58 takes 0.207761049271 seconds\n",
      "epoch 59 takes 0.194292783737 seconds\n",
      "epoch 60 takes 0.192578792572 seconds\n",
      "epoch 61 takes 0.198334932327 seconds\n",
      "epoch 62 takes 0.199114084244 seconds\n",
      "epoch 63 takes 0.199196815491 seconds\n",
      "epoch 64 takes 0.198318958282 seconds\n",
      "epoch 65 takes 0.208312034607 seconds\n",
      "epoch 66 takes 0.209336042404 seconds\n",
      "epoch 67 takes 0.197627067566 seconds\n",
      "epoch 68 takes 0.213241100311 seconds\n",
      "epoch 69 takes 0.213160991669 seconds\n",
      "epoch 70 takes 0.199898004532 seconds\n",
      "epoch 71 takes 0.208475112915 seconds\n",
      "epoch 72 takes 0.206138134003 seconds\n",
      "epoch 73 takes 0.214753866196 seconds\n",
      "epoch 74 takes 0.198586940765 seconds\n",
      "epoch 75 takes 0.196001052856 seconds\n",
      "epoch 76 takes 0.196412086487 seconds\n",
      "epoch 77 takes 0.21674489975 seconds\n",
      "epoch 78 takes 0.198210000992 seconds\n",
      "epoch 79 takes 0.185207128525 seconds\n",
      "epoch 80 takes 0.202637910843 seconds\n",
      "epoch 81 takes 0.204237937927 seconds\n",
      "epoch 82 takes 0.192293167114 seconds\n",
      "epoch 83 takes 0.203083992004 seconds\n",
      "epoch 84 takes 0.188119888306 seconds\n",
      "epoch 85 takes 0.185400009155 seconds\n",
      "epoch 86 takes 0.186584949493 seconds\n",
      "epoch 87 takes 0.192233085632 seconds\n",
      "epoch 88 takes 0.197103023529 seconds\n",
      "epoch 89 takes 0.204027891159 seconds\n",
      "epoch 90 takes 0.221820116043 seconds\n",
      "epoch 91 takes 0.200870990753 seconds\n",
      "epoch 92 takes 0.194289922714 seconds\n",
      "epoch 93 takes 0.202631950378 seconds\n",
      "epoch 94 takes 0.191339015961 seconds\n",
      "epoch 95 takes 0.216153144836 seconds\n",
      "epoch 96 takes 0.190893888474 seconds\n",
      "epoch 97 takes 0.192759037018 seconds\n",
      "epoch 98 takes 0.202857971191 seconds\n",
      "epoch 99 takes 0.199268102646 seconds\n",
      "epoch 100 takes 0.192088127136 seconds\n",
      "Likelihood -583.8521050515749 takes 25.749309063 seconds\n",
      "epoch 101 takes 0.231406927109 seconds\n",
      "epoch 102 takes 0.205889940262 seconds\n",
      "epoch 103 takes 0.1922519207 seconds\n",
      "epoch 104 takes 0.202517032623 seconds\n",
      "epoch 105 takes 0.19551897049 seconds\n",
      "epoch 106 takes 0.197668075562 seconds\n",
      "epoch 107 takes 0.203804016113 seconds\n",
      "epoch 108 takes 0.210107088089 seconds\n",
      "epoch 109 takes 0.218262195587 seconds\n",
      "epoch 110 takes 0.218278884888 seconds\n",
      "epoch 111 takes 0.196736097336 seconds\n",
      "epoch 112 takes 0.211182117462 seconds\n",
      "epoch 113 takes 0.204830169678 seconds\n",
      "epoch 114 takes 0.195748806 seconds\n",
      "epoch 115 takes 0.209413051605 seconds\n",
      "epoch 116 takes 0.226572990417 seconds\n",
      "epoch 117 takes 0.199683904648 seconds\n",
      "epoch 118 takes 0.195657968521 seconds\n",
      "epoch 119 takes 0.195452928543 seconds\n",
      "epoch 120 takes 0.203619003296 seconds\n",
      "epoch 121 takes 0.204006910324 seconds\n",
      "epoch 122 takes 0.207903862 seconds\n",
      "epoch 123 takes 0.212430000305 seconds\n",
      "epoch 124 takes 0.1875 seconds\n",
      "epoch 125 takes 0.234649896622 seconds\n",
      "epoch 126 takes 0.200551986694 seconds\n",
      "epoch 127 takes 0.192492008209 seconds\n",
      "epoch 128 takes 0.190171003342 seconds\n",
      "epoch 129 takes 0.195773124695 seconds\n",
      "epoch 130 takes 0.209930181503 seconds\n",
      "epoch 131 takes 0.183351039886 seconds\n",
      "epoch 132 takes 0.199102878571 seconds\n",
      "epoch 133 takes 0.195754051208 seconds\n",
      "epoch 134 takes 0.203525066376 seconds\n",
      "epoch 135 takes 0.191838026047 seconds\n",
      "epoch 136 takes 0.207524061203 seconds\n",
      "epoch 137 takes 0.211158037186 seconds\n",
      "epoch 138 takes 0.215101957321 seconds\n",
      "epoch 139 takes 0.226022005081 seconds\n",
      "epoch 140 takes 0.202872991562 seconds\n",
      "epoch 141 takes 0.200037002563 seconds\n",
      "epoch 142 takes 0.196032047272 seconds\n",
      "epoch 143 takes 0.199300050735 seconds\n",
      "epoch 144 takes 0.203630924225 seconds\n",
      "epoch 145 takes 0.191933870316 seconds\n",
      "epoch 146 takes 0.20318198204 seconds\n",
      "epoch 147 takes 0.197158098221 seconds\n",
      "epoch 148 takes 0.225548028946 seconds\n",
      "epoch 149 takes 0.192801952362 seconds\n",
      "epoch 150 takes 0.196259975433 seconds\n",
      "Likelihood -583.5923402097976 takes 23.2305209637 seconds\n",
      "epoch 151 takes 0.188755989075 seconds\n",
      "epoch 152 takes 0.193951129913 seconds\n",
      "epoch 153 takes 0.207087993622 seconds\n",
      "epoch 154 takes 0.214359998703 seconds\n",
      "epoch 155 takes 0.195013046265 seconds\n",
      "epoch 156 takes 0.212328910828 seconds\n",
      "epoch 157 takes 0.187269926071 seconds\n",
      "epoch 158 takes 0.200766086578 seconds\n",
      "epoch 159 takes 0.199720144272 seconds\n",
      "epoch 160 takes 0.196012973785 seconds\n",
      "epoch 161 takes 0.194520950317 seconds\n",
      "epoch 162 takes 0.194509983063 seconds\n",
      "epoch 163 takes 0.198266983032 seconds\n",
      "epoch 164 takes 0.210708141327 seconds\n",
      "epoch 165 takes 0.19224190712 seconds\n",
      "epoch 166 takes 0.184335947037 seconds\n",
      "epoch 167 takes 0.186125040054 seconds\n",
      "epoch 168 takes 0.185760974884 seconds\n",
      "epoch 169 takes 0.182822942734 seconds\n",
      "epoch 170 takes 0.198426961899 seconds\n",
      "epoch 171 takes 0.191245079041 seconds\n",
      "epoch 172 takes 0.191634893417 seconds\n",
      "epoch 173 takes 0.178478956223 seconds\n",
      "epoch 174 takes 0.190098047256 seconds\n",
      "epoch 175 takes 0.190146207809 seconds\n",
      "epoch 176 takes 0.195490121841 seconds\n",
      "epoch 177 takes 0.193833112717 seconds\n",
      "epoch 178 takes 0.210331916809 seconds\n",
      "epoch 179 takes 0.186531066895 seconds\n",
      "epoch 180 takes 0.19718003273 seconds\n",
      "epoch 181 takes 0.192822933197 seconds\n",
      "epoch 182 takes 0.210050106049 seconds\n",
      "epoch 183 takes 0.182997226715 seconds\n",
      "epoch 184 takes 0.205415010452 seconds\n",
      "epoch 185 takes 0.186297893524 seconds\n",
      "epoch 186 takes 0.186408996582 seconds\n",
      "epoch 187 takes 0.183490991592 seconds\n",
      "epoch 188 takes 0.193037033081 seconds\n",
      "epoch 189 takes 0.189186096191 seconds\n",
      "epoch 190 takes 0.189795970917 seconds\n",
      "epoch 191 takes 0.203290939331 seconds\n",
      "epoch 192 takes 0.188097000122 seconds\n",
      "epoch 193 takes 0.19948887825 seconds\n",
      "epoch 194 takes 0.18923997879 seconds\n",
      "epoch 195 takes 0.203104019165 seconds\n",
      "epoch 196 takes 0.189391136169 seconds\n",
      "epoch 197 takes 0.194071054459 seconds\n",
      "epoch 198 takes 0.198112010956 seconds\n",
      "epoch 199 takes 0.205665826797 seconds\n",
      "Train phase finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Initial Params =======================#\n",
    "import PGBN_sampler \n",
    "from scipy.special import gamma\n",
    "Params = {}\n",
    "\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K1'], 1]) / Setting['K1']\n",
    "\n",
    "# Collection\n",
    "W_train = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "import time\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "\n",
    "#========================== Gibbs ==========================＃\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================＃\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])     # Augmentation on D\n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1'])    # Augmentation on w\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32')\n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1  # padding\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "\n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape, dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "\n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64') # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')  # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3), axis=2) # N*K1\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update D\n",
    "    for k1 in range(Setting['K1']):\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] \n",
    "        D1_k1_s = (X_k1_34 + SuperParams['eta']) / np.sum(X_k1_34 + SuperParams['eta'])\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "\n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    # Update w_j\n",
    "    W_k1_sn = np.random.gamma(Params['W1_nk1_Aug_Pooling'].T + Params['Gamma']) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn)  # N*K1\n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    " \n",
    "    end_time = time.time()\n",
    "\n",
    "    if t == 0:\n",
    "        Iter_time.append(end_time - start_time)\n",
    "    else:\n",
    "        Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "    #====================== Likelihood ======================#\n",
    "    if np.mod(t,50) == 0:\n",
    "        \n",
    "        likelyhood = 0\n",
    "        start_time = time.time()\n",
    "        Orgin_X = np.zeros([Setting['N_train'], Setting['K1_V1'], Setting['K1_V2']])\n",
    "        Orgin_X[[batch_file_index, batch_rows, batch_cols+1]] = batch_value\n",
    "\n",
    "        for i in range(Setting['N_train']):\n",
    "            \n",
    "            Phi_tmp = np.transpose(np.reshape(Params['D1_k1'],[Setting['K1'], Setting['K1_S3'], Setting['K1_S4'], 1]),[1,2,3,0])\n",
    "            Theta_tmp = np.transpose(Params['W1_nk1'][i:i+1,:,:,:], [0,2,3,1])\n",
    "            PhiTheta_1= sess.run(X_1, feed_dict={Phi_1:Phi_tmp.astype(np.float32), Theta_1:Theta_tmp.astype(np.float32)})\n",
    "\n",
    "            likelyhood = likelyhood + np.sum(Orgin_X[i,:,:] * log_max(PhiTheta_1[0,:,:,0]) - PhiTheta_1[0,:,:,0] - log_max(gamma(Orgin_X[i,:,:] + 1)))  \n",
    "        end_time = time.time()\n",
    "        print \"Likelihood \" + str(likelyhood / Setting['N_train']) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "        Iter_lh.append(likelyhood / Setting['N_train'])\n",
    "        \n",
    "print \"Train phase finished\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
