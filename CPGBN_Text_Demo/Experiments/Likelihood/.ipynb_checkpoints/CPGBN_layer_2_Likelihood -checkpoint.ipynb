{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "Preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Tensorflow initial finished\n",
      "CUDA initial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.RandomState(1)\n",
    "\n",
    "realmin = 2.2e-10\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "#====================== Load data ======================#\n",
    "import cPickle\n",
    "\n",
    "DATA = cPickle.load(open(\"./TREC_3k-12-6.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = DATA['Vocabulary']\n",
    "data_vab_count_list    = DATA['Vab_count']\n",
    "data_vab_length        = DATA['Vab_Size']\n",
    "data_label             = DATA['Label']\n",
    "data_train_list        = DATA['Train_Origin']\n",
    "data_train_label       = np.array(DATA['Train_Label'])\n",
    "data_train_split       = DATA['Train_Word_Split']\n",
    "data_train_list_index  = DATA['Train_Word2Index']\n",
    "data_test_list         = DATA['Test_Origin']\n",
    "data_test_label        = np.array(DATA['Test_Label'])\n",
    "data_test_split        = DATA['Test_Word_Split']\n",
    "data_test_list_index   = DATA['Test_Word2Index']\n",
    "data_value             = 10\n",
    "\n",
    "print 'Load data'\n",
    "\n",
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_train_list)): \n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0)\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "\n",
    "print 'Preprocess finished'\n",
    "\n",
    "batch_len_tr  = batch_len\n",
    "batch_rows_tr = batch_rows\n",
    "batch_cols_tr = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "#======================= Setting =======================#\n",
    "Setting = {}\n",
    "Setting['N_train']    = len(data_train_list) - delete_count\n",
    "Setting['N_test']     = len(data_test_list)\n",
    "# 1-th layer\n",
    "Setting['K1']         = 32\n",
    "Setting['K1_V1']      = DATA['Vab_Size']\n",
    "Setting['K1_V2']      = np.max(batch_len) + 2  # padding\n",
    "Setting['K1_S3']      = DATA['Vab_Size']\n",
    "Setting['K1_S4']      = 3\n",
    "Setting['K1_S1']      = Setting['K1_V1'] + 1 - Setting['K1_S3'] # 1\n",
    "Setting['K1_S2']      = Setting['K1_V2'] + 1 - Setting['K1_S4'] # 26\n",
    "# 2-th layer\n",
    "Setting['K2']         = 16\n",
    "\n",
    "Setting['Iter']       = 200\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "#======================= SuperParams =======================#\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.05 # Phi\n",
    "\n",
    "#======================= Tensorflow Initial =======================#\n",
    "# Initial Graph\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) #HWC\n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print 'Tensorflow initial finished'\n",
    "\n",
    "#====================== CUDA Initial ======================#\n",
    "# Note， do not add any cuda operation among CUDA initial such as Tensorflow!!!!!!!!!!!!!!!!!!\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"CUDA initial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 0.328140974045 seconds\n",
      "Likelihood -1268.4135686563031 takes 32.8852369785 seconds\n",
      "epoch 1 takes 0.2995698452 seconds\n",
      "epoch 2 takes 0.307909965515 seconds\n",
      "epoch 3 takes 0.312574863434 seconds\n",
      "epoch 4 takes 0.307695865631 seconds\n",
      "epoch 5 takes 0.313855171204 seconds\n",
      "epoch 6 takes 0.313759088516 seconds\n",
      "epoch 7 takes 0.299799919128 seconds\n",
      "epoch 8 takes 0.316197156906 seconds\n",
      "epoch 9 takes 0.314877986908 seconds\n",
      "epoch 10 takes 0.325203895569 seconds\n",
      "epoch 11 takes 0.30215716362 seconds\n",
      "epoch 12 takes 0.327917098999 seconds\n",
      "epoch 13 takes 0.32829284668 seconds\n",
      "epoch 14 takes 0.310379981995 seconds\n",
      "epoch 15 takes 0.312335014343 seconds\n",
      "epoch 16 takes 0.333402872086 seconds\n",
      "epoch 17 takes 0.324542045593 seconds\n",
      "epoch 18 takes 0.329352140427 seconds\n",
      "epoch 19 takes 0.306978940964 seconds\n",
      "epoch 20 takes 0.316106081009 seconds\n",
      "epoch 21 takes 0.314373016357 seconds\n",
      "epoch 22 takes 0.307020902634 seconds\n",
      "epoch 23 takes 0.315527915955 seconds\n",
      "epoch 24 takes 0.299167871475 seconds\n",
      "epoch 25 takes 0.320230007172 seconds\n",
      "epoch 26 takes 0.296269178391 seconds\n",
      "epoch 27 takes 0.312161922455 seconds\n",
      "epoch 28 takes 0.299497127533 seconds\n",
      "epoch 29 takes 0.308192014694 seconds\n",
      "epoch 30 takes 0.29708981514 seconds\n",
      "epoch 31 takes 0.297353029251 seconds\n",
      "epoch 32 takes 0.313412904739 seconds\n",
      "epoch 33 takes 0.295959949493 seconds\n",
      "epoch 34 takes 0.30605006218 seconds\n",
      "epoch 35 takes 0.303905010223 seconds\n",
      "epoch 36 takes 0.288184165955 seconds\n",
      "epoch 37 takes 0.300035953522 seconds\n",
      "epoch 38 takes 0.299371957779 seconds\n",
      "epoch 39 takes 0.303516149521 seconds\n",
      "epoch 40 takes 0.302267074585 seconds\n",
      "epoch 41 takes 0.30193901062 seconds\n",
      "epoch 42 takes 0.299347162247 seconds\n",
      "epoch 43 takes 0.294578790665 seconds\n",
      "epoch 44 takes 0.312580108643 seconds\n",
      "epoch 45 takes 0.311862945557 seconds\n",
      "epoch 46 takes 0.296965122223 seconds\n",
      "epoch 47 takes 0.307734966278 seconds\n",
      "epoch 48 takes 0.296884059906 seconds\n",
      "epoch 49 takes 0.297670125961 seconds\n",
      "epoch 50 takes 0.298938035965 seconds\n",
      "Likelihood -539.762078937774 takes 30.4671809673 seconds\n",
      "epoch 51 takes 0.306637048721 seconds\n",
      "epoch 52 takes 0.324005126953 seconds\n",
      "epoch 53 takes 0.310389995575 seconds\n",
      "epoch 54 takes 0.310812950134 seconds\n",
      "epoch 55 takes 0.303783178329 seconds\n",
      "epoch 56 takes 0.314634084702 seconds\n",
      "epoch 57 takes 0.295612812042 seconds\n",
      "epoch 58 takes 0.315417051315 seconds\n",
      "epoch 59 takes 0.308945894241 seconds\n",
      "epoch 60 takes 0.302887916565 seconds\n",
      "epoch 61 takes 0.288941144943 seconds\n",
      "epoch 62 takes 0.303424835205 seconds\n",
      "epoch 63 takes 0.292301893234 seconds\n",
      "epoch 64 takes 0.305738925934 seconds\n",
      "epoch 65 takes 0.312456130981 seconds\n",
      "epoch 66 takes 0.30265712738 seconds\n",
      "epoch 67 takes 0.31317615509 seconds\n",
      "epoch 68 takes 0.288906097412 seconds\n",
      "epoch 69 takes 0.296302080154 seconds\n",
      "epoch 70 takes 0.319108963013 seconds\n",
      "epoch 71 takes 0.305140018463 seconds\n",
      "epoch 72 takes 0.30034995079 seconds\n",
      "epoch 73 takes 0.300964832306 seconds\n",
      "epoch 74 takes 0.300733089447 seconds\n",
      "epoch 75 takes 0.300024032593 seconds\n",
      "epoch 76 takes 0.303679943085 seconds\n",
      "epoch 77 takes 0.283081054688 seconds\n",
      "epoch 78 takes 0.3006939888 seconds\n",
      "epoch 79 takes 0.302853107452 seconds\n",
      "epoch 80 takes 0.31841802597 seconds\n",
      "epoch 81 takes 0.296071052551 seconds\n",
      "epoch 82 takes 0.30100107193 seconds\n",
      "epoch 83 takes 0.301163911819 seconds\n",
      "epoch 84 takes 0.31473493576 seconds\n",
      "epoch 85 takes 0.308120012283 seconds\n",
      "epoch 86 takes 0.306082010269 seconds\n",
      "epoch 87 takes 0.308160066605 seconds\n",
      "epoch 88 takes 0.312223911285 seconds\n",
      "epoch 89 takes 0.309887886047 seconds\n",
      "epoch 90 takes 0.299076080322 seconds\n",
      "epoch 91 takes 0.301443099976 seconds\n",
      "epoch 92 takes 0.39058804512 seconds\n",
      "epoch 93 takes 0.390617132187 seconds\n",
      "epoch 94 takes 0.393210887909 seconds\n",
      "epoch 95 takes 0.392189025879 seconds\n",
      "epoch 96 takes 0.382565021515 seconds\n",
      "epoch 97 takes 0.405636787415 seconds\n",
      "epoch 98 takes 0.392305135727 seconds\n",
      "epoch 99 takes 0.383414030075 seconds\n",
      "epoch 100 takes 0.404098033905 seconds\n",
      "Likelihood -535.7026631008018 takes 30.407184124 seconds\n",
      "epoch 101 takes 0.319890022278 seconds\n",
      "epoch 102 takes 0.299134969711 seconds\n",
      "epoch 103 takes 0.295034885406 seconds\n",
      "epoch 104 takes 0.309975862503 seconds\n",
      "epoch 105 takes 0.304672956467 seconds\n",
      "epoch 106 takes 0.300536870956 seconds\n",
      "epoch 107 takes 0.309952020645 seconds\n",
      "epoch 108 takes 0.306534051895 seconds\n",
      "epoch 109 takes 0.326022863388 seconds\n",
      "epoch 110 takes 0.302394866943 seconds\n",
      "epoch 111 takes 0.291448116302 seconds\n",
      "epoch 112 takes 0.30358505249 seconds\n",
      "epoch 113 takes 0.29435300827 seconds\n",
      "epoch 114 takes 0.309852838516 seconds\n",
      "epoch 115 takes 0.30904006958 seconds\n",
      "epoch 116 takes 0.325543880463 seconds\n",
      "epoch 117 takes 0.308961868286 seconds\n",
      "epoch 118 takes 0.308334827423 seconds\n",
      "epoch 119 takes 0.299388885498 seconds\n",
      "epoch 120 takes 0.310017108917 seconds\n",
      "epoch 121 takes 0.322302103043 seconds\n",
      "epoch 122 takes 0.312034130096 seconds\n",
      "epoch 123 takes 0.313920021057 seconds\n",
      "epoch 124 takes 0.308250188828 seconds\n",
      "epoch 125 takes 0.306324005127 seconds\n",
      "epoch 126 takes 0.30855512619 seconds\n",
      "epoch 127 takes 0.322613954544 seconds\n",
      "epoch 128 takes 0.308985948563 seconds\n",
      "epoch 129 takes 0.309891939163 seconds\n",
      "epoch 130 takes 0.320071220398 seconds\n",
      "epoch 131 takes 0.310440063477 seconds\n",
      "epoch 132 takes 0.304814100266 seconds\n",
      "epoch 133 takes 0.305946826935 seconds\n",
      "epoch 134 takes 0.302282810211 seconds\n",
      "epoch 135 takes 0.318824052811 seconds\n",
      "epoch 136 takes 0.30966091156 seconds\n",
      "epoch 137 takes 0.312335014343 seconds\n",
      "epoch 138 takes 0.311357021332 seconds\n",
      "epoch 139 takes 0.293576955795 seconds\n",
      "epoch 140 takes 0.304351091385 seconds\n",
      "epoch 141 takes 0.322808980942 seconds\n",
      "epoch 142 takes 0.313714027405 seconds\n",
      "epoch 143 takes 0.305635929108 seconds\n",
      "epoch 144 takes 0.308924198151 seconds\n",
      "epoch 145 takes 0.307474136353 seconds\n",
      "epoch 146 takes 0.300570964813 seconds\n",
      "epoch 147 takes 0.333747148514 seconds\n",
      "epoch 148 takes 0.329045057297 seconds\n",
      "epoch 149 takes 0.397719144821 seconds\n",
      "epoch 150 takes 0.394124031067 seconds\n"
     ]
    }
   ],
   "source": [
    "#======================= Initial Params =======================#\n",
    "import PGBN_sampler \n",
    "from scipy.special import gamma\n",
    "Params = {}\n",
    "\n",
    "# 1-th layer\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1']         = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "# 2-th layer\n",
    "Params['Phi_2']  = 0.2 + 0.8*np.random.rand(Setting['K1'], Setting['K2'])\n",
    "Params['Phi_2']  = Params['Phi_2'] / np.sum(Params['Phi_2'], axis=0)\n",
    "Params['Theta_2']= np.random.rand(Setting['N_train'], Setting['K2'])\n",
    "Params['c3_n']   = 1 * np.ones([Setting['N_train']])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K2'], 1]) / Setting['K2']\n",
    "\n",
    "# Collection\n",
    "W_train_1 = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "W_train_2 = np.zeros([Setting['N_train'], Setting['K2']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "import time\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "\n",
    "#========================== Gibbs ==========================#\n",
    "for t in range(Setting['Iter']):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================#\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])     # Augmentation on D\n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1'])    # Augmentation on w\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32') \n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1  # padding \n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "    \n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "    \n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64') # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')  # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3),axis=2) # N*K1\n",
    "    \n",
    "    #========================== 2nd layer Augmentation ==========================#\n",
    "    M1_tmp = np.array(np.transpose(np.round(Params['W1_nk1_Aug_Pooling'])), dtype='float64', order='C')\n",
    "    Theta2_tmp = np.array(np.transpose(Params['Theta_2']), dtype='float64', order='C')\n",
    "    Xt_to_t1_2,WSZS_2 = PGBN_sampler.Crt_Multirnd_Matrix(M1_tmp, Params['Phi_2'], Theta2_tmp)\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update D,Phi\n",
    "    for k1 in range(Setting['K1']):\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] \n",
    "        X_k1_34_tmp = np.random.gamma(X_k1_34 + SuperParams['eta'])\n",
    "        D1_k1_s     = X_k1_34_tmp / np.sum(X_k1_34_tmp)\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "        \n",
    "    Phi_2_tmp       = np.random.gamma(WSZS_2 + SuperParams['eta'])\n",
    "    Params['Phi_2'] = Phi_2_tmp / np.sum(Phi_2_tmp, axis=0)\n",
    "    \n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_2'], Params['Theta_2'].T),0)) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    Params['c3_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c3_n']     = Params['c3_n'] / (SuperParams['f0'] + np.sum(Params['Theta_2'],axis=1))   \n",
    "\n",
    "    # Update w_j\n",
    "    W_k2_sn = np.random.gamma(Params['Gamma'] + Xt_to_t1_2) / (-np.log(1-Params['p2_n']) + Params['c3_n']) # V*N\n",
    "    Params['Theta_2'] = np.transpose(W_k2_sn)\n",
    "    \n",
    "    shape1 = np.dot(Params['Phi_2'], Params['Theta_2'].T) # V*N\n",
    "    W_k1_sn = np.random.gamma(shape1 + Params['W1_nk1_Aug_Pooling'].T ) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn) \n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    if t == 0:\n",
    "        Iter_time.append(end_time - start_time)\n",
    "    else:\n",
    "        Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "    #====================== Likelihood ======================#\n",
    "    if np.mod(t,50) == 0:\n",
    "\n",
    "        likelyhood = 0\n",
    "        start_time = time.time()\n",
    "        Orgin_X = np.zeros([Setting['N_train'], Setting['K1_V1'], Setting['K1_V2']])\n",
    "        Orgin_X[[batch_file_index, batch_rows, batch_cols+1]] = batch_value\n",
    "\n",
    "        for i in range(Setting['N_train']):\n",
    "\n",
    "            Phi_tmp = np.transpose(np.reshape(Params['D1_k1'],[Setting['K1'], Setting['K1_S3'], Setting['K1_S4'], 1]),[1,2,3,0])\n",
    "            Theta_tmp = np.transpose(Params['W1_nk1'][i:i+1,:,:,:], [0,2,3,1])\n",
    "            PhiTheta_1= sess.run(X_1, feed_dict={Phi_1:Phi_tmp.astype(np.float32), Theta_1:Theta_tmp.astype(np.float32)})\n",
    "            PhiTheta_1= PhiTheta_1.astype(np.float64)\n",
    "            likelyhood = likelyhood + np.sum(Orgin_X[i,:,:] * log_max(PhiTheta_1[0,:,:,0]) - PhiTheta_1[0,:,:,0] - log_max(gamma(Orgin_X[i,:,:] + 1)))  \n",
    "        \n",
    "        end_time = time.time()\n",
    "        print \"Likelihood \" + str(likelyhood / Setting['N_train']) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "        Iter_lh.append(likelyhood / Setting['N_train'])\n",
    "    \n",
    "print \"Train phase finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
