{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "Preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Tensorflow initial finished\n",
      "CUDA initial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.RandomState(1)\n",
    "\n",
    "realmin = 2.2e-10\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "#====================== Load data ======================#\n",
    "import cPickle\n",
    "\n",
    "DATA = cPickle.load(open(\"./TREC_3k-12-6.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = DATA['Vocabulary']\n",
    "data_vab_count_list    = DATA['Vab_count']\n",
    "data_vab_length        = DATA['Vab_Size']\n",
    "data_label             = DATA['Label']\n",
    "data_train_list        = DATA['Train_Origin']\n",
    "data_train_label       = np.array(DATA['Train_Label'])\n",
    "data_train_split       = DATA['Train_Word_Split']\n",
    "data_train_list_index  = DATA['Train_Word2Index']\n",
    "data_test_list         = DATA['Test_Origin']\n",
    "data_test_label        = np.array(DATA['Test_Label'])\n",
    "data_test_split        = DATA['Test_Word_Split']\n",
    "data_test_list_index   = DATA['Test_Word2Index']\n",
    "data_value             = 10\n",
    "\n",
    "print 'Load data'\n",
    "\n",
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_train_list)):\n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)                                         \n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0) \n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "\n",
    "print 'Preprocess finished'\n",
    "\n",
    "batch_len_tr        = batch_len\n",
    "batch_rows_tr       = batch_rows\n",
    "batch_cols_tr       = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "#======================= Setting =======================#\n",
    "Setting = {}\n",
    "Setting['N_train']    = len(data_train_list) - delete_count \n",
    "Setting['K1']         = 32\n",
    "Setting['K1_V1']      = DATA['Vab_Size']\n",
    "Setting['K1_V2']      = np.max(batch_len) + 2  # padding             　\n",
    "Setting['K1_S3']      = DATA['Vab_Size']\n",
    "Setting['K1_S4']      = 3\n",
    "Setting['K1_S1']      = Setting['K1_V1'] + 1 - Setting['K1_S3']\n",
    "Setting['K1_S2']      = Setting['K1_V2'] + 1 - Setting['K1_S4']   \n",
    "\n",
    "Setting['Iter']       = 200\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "#======================= SuperParams =======================#\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.05 # Phi\n",
    "\n",
    "#======================= Tensorflow Initial =======================#\n",
    "# Initial Graph\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) #HWC\n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print 'Tensorflow initial finished'\n",
    "\n",
    "#====================== CUDA Initial ======================#\n",
    "# Note， do not add any cuda operation among CUDA initial such as Tensorflow!!!!!!!!!!!!!!!!!!\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"CUDA initial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 0.214260101318 seconds\n",
      "Likelihood -518.5410512004162 takes 23.6906311512 seconds\n",
      "epoch 1 takes 0.184551954269 seconds\n",
      "epoch 2 takes 0.205935001373 seconds\n",
      "epoch 3 takes 0.188102960587 seconds\n",
      "epoch 4 takes 0.184844017029 seconds\n",
      "epoch 5 takes 0.192376852036 seconds\n",
      "epoch 6 takes 0.201794147491 seconds\n",
      "epoch 7 takes 0.195115089417 seconds\n",
      "epoch 8 takes 0.194356918335 seconds\n",
      "epoch 9 takes 0.194139003754 seconds\n",
      "epoch 10 takes 0.190070152283 seconds\n",
      "epoch 11 takes 0.215881824493 seconds\n",
      "epoch 12 takes 0.201840877533 seconds\n",
      "epoch 13 takes 0.201107978821 seconds\n",
      "epoch 14 takes 0.207763195038 seconds\n",
      "epoch 15 takes 0.204153060913 seconds\n",
      "epoch 16 takes 0.205536842346 seconds\n",
      "epoch 17 takes 0.210997104645 seconds\n",
      "epoch 18 takes 0.198394060135 seconds\n",
      "epoch 19 takes 0.21150302887 seconds\n",
      "epoch 20 takes 0.19393491745 seconds\n",
      "epoch 21 takes 0.211939811707 seconds\n",
      "epoch 22 takes 0.212212085724 seconds\n",
      "epoch 23 takes 0.2281229496 seconds\n",
      "epoch 24 takes 0.230623960495 seconds\n",
      "epoch 25 takes 0.23565196991 seconds\n",
      "epoch 26 takes 0.222026824951 seconds\n",
      "epoch 27 takes 0.234110116959 seconds\n",
      "epoch 28 takes 0.219620943069 seconds\n",
      "epoch 29 takes 0.233608007431 seconds\n",
      "epoch 30 takes 0.243293046951 seconds\n",
      "epoch 31 takes 0.223052024841 seconds\n",
      "epoch 32 takes 0.208885908127 seconds\n",
      "epoch 33 takes 0.208620786667 seconds\n",
      "epoch 34 takes 0.195674180984 seconds\n",
      "epoch 35 takes 0.192591905594 seconds\n",
      "epoch 36 takes 0.198843955994 seconds\n",
      "epoch 37 takes 0.20109796524 seconds\n",
      "epoch 38 takes 0.193540096283 seconds\n",
      "epoch 39 takes 0.191550970078 seconds\n",
      "epoch 40 takes 0.183109045029 seconds\n",
      "epoch 41 takes 0.18902182579 seconds\n",
      "epoch 42 takes 0.19175195694 seconds\n",
      "epoch 43 takes 0.195425033569 seconds\n",
      "epoch 44 takes 0.213719129562 seconds\n",
      "epoch 45 takes 0.190408945084 seconds\n",
      "epoch 46 takes 0.191353797913 seconds\n",
      "epoch 47 takes 0.202306985855 seconds\n",
      "epoch 48 takes 0.201679944992 seconds\n",
      "epoch 49 takes 0.201560020447 seconds\n",
      "epoch 50 takes 0.204454183578 seconds\n",
      "Likelihood -269.4857705278361 takes 27.1569950581 seconds\n",
      "epoch 51 takes 0.203210830688 seconds\n",
      "epoch 52 takes 0.211116790771 seconds\n",
      "epoch 53 takes 0.213969945908 seconds\n",
      "epoch 54 takes 0.202886104584 seconds\n",
      "epoch 55 takes 0.208149909973 seconds\n",
      "epoch 56 takes 0.205245018005 seconds\n",
      "epoch 57 takes 0.197838068008 seconds\n",
      "epoch 58 takes 0.212352991104 seconds\n",
      "epoch 59 takes 0.223446130753 seconds\n",
      "epoch 60 takes 0.200119018555 seconds\n",
      "epoch 61 takes 0.200132846832 seconds\n",
      "epoch 62 takes 0.197077035904 seconds\n",
      "epoch 63 takes 0.190662145615 seconds\n",
      "epoch 64 takes 0.229997158051 seconds\n",
      "epoch 65 takes 0.200724840164 seconds\n",
      "epoch 66 takes 0.200253009796 seconds\n",
      "epoch 67 takes 0.188343048096 seconds\n",
      "epoch 68 takes 0.20049405098 seconds\n",
      "epoch 69 takes 0.218113899231 seconds\n",
      "epoch 70 takes 0.2149310112 seconds\n",
      "epoch 71 takes 0.228947162628 seconds\n",
      "epoch 72 takes 0.208487033844 seconds\n",
      "epoch 73 takes 0.211568117142 seconds\n",
      "epoch 74 takes 0.185518980026 seconds\n",
      "epoch 75 takes 0.221217155457 seconds\n",
      "epoch 76 takes 0.212671041489 seconds\n",
      "epoch 77 takes 0.203881025314 seconds\n",
      "epoch 78 takes 0.197954893112 seconds\n",
      "epoch 79 takes 0.213083028793 seconds\n",
      "epoch 80 takes 0.218477964401 seconds\n",
      "epoch 81 takes 0.205463886261 seconds\n",
      "epoch 82 takes 0.209180831909 seconds\n",
      "epoch 83 takes 0.201775074005 seconds\n",
      "epoch 84 takes 0.205495119095 seconds\n",
      "epoch 85 takes 0.202913999557 seconds\n",
      "epoch 86 takes 0.212712049484 seconds\n",
      "epoch 87 takes 0.214090108871 seconds\n",
      "epoch 88 takes 0.194526910782 seconds\n",
      "epoch 89 takes 0.203407049179 seconds\n",
      "epoch 90 takes 0.189600944519 seconds\n",
      "epoch 91 takes 0.224148988724 seconds\n",
      "epoch 92 takes 0.194225072861 seconds\n",
      "epoch 93 takes 0.2138671875 seconds\n",
      "epoch 94 takes 0.209781885147 seconds\n",
      "epoch 95 takes 0.203298807144 seconds\n",
      "epoch 96 takes 0.203706979752 seconds\n",
      "epoch 97 takes 0.199136972427 seconds\n",
      "epoch 98 takes 0.221441984177 seconds\n",
      "epoch 99 takes 0.200486898422 seconds\n",
      "epoch 100 takes 0.210119009018 seconds\n",
      "Likelihood -269.0540910272793 takes 24.9252319336 seconds\n",
      "epoch 101 takes 0.212985038757 seconds\n",
      "epoch 102 takes 0.196170806885 seconds\n",
      "epoch 103 takes 0.195307016373 seconds\n",
      "epoch 104 takes 0.188679933548 seconds\n",
      "epoch 105 takes 0.207149982452 seconds\n",
      "epoch 106 takes 0.204801082611 seconds\n",
      "epoch 107 takes 0.198714971542 seconds\n",
      "epoch 108 takes 0.192554950714 seconds\n",
      "epoch 109 takes 0.194032907486 seconds\n",
      "epoch 110 takes 0.202790975571 seconds\n",
      "epoch 111 takes 0.186985015869 seconds\n",
      "epoch 112 takes 0.195659160614 seconds\n",
      "epoch 113 takes 0.198992013931 seconds\n",
      "epoch 114 takes 0.202011823654 seconds\n",
      "epoch 115 takes 0.206546068192 seconds\n",
      "epoch 116 takes 0.190566062927 seconds\n",
      "epoch 117 takes 0.199980974197 seconds\n",
      "epoch 118 takes 0.225955963135 seconds\n",
      "epoch 119 takes 0.188323020935 seconds\n",
      "epoch 120 takes 0.216730833054 seconds\n",
      "epoch 121 takes 0.216060876846 seconds\n",
      "epoch 122 takes 0.215615034103 seconds\n",
      "epoch 123 takes 0.218776941299 seconds\n",
      "epoch 124 takes 0.235002040863 seconds\n",
      "epoch 125 takes 0.210074901581 seconds\n",
      "epoch 126 takes 0.196860074997 seconds\n",
      "epoch 127 takes 0.203346014023 seconds\n",
      "epoch 128 takes 0.209791898727 seconds\n",
      "epoch 129 takes 0.199218988419 seconds\n",
      "epoch 130 takes 0.214814186096 seconds\n",
      "epoch 131 takes 0.204414129257 seconds\n",
      "epoch 132 takes 0.204205989838 seconds\n",
      "epoch 133 takes 0.235662937164 seconds\n",
      "epoch 134 takes 0.216023921967 seconds\n",
      "epoch 135 takes 0.223098993301 seconds\n",
      "epoch 136 takes 0.197104930878 seconds\n",
      "epoch 137 takes 0.216398954391 seconds\n",
      "epoch 138 takes 0.186765909195 seconds\n",
      "epoch 139 takes 0.228055000305 seconds\n",
      "epoch 140 takes 0.197304964066 seconds\n",
      "epoch 141 takes 0.198684215546 seconds\n",
      "epoch 142 takes 0.191332101822 seconds\n",
      "epoch 143 takes 0.18789100647 seconds\n",
      "epoch 144 takes 0.201110839844 seconds\n",
      "epoch 145 takes 0.192430973053 seconds\n",
      "epoch 146 takes 0.203958034515 seconds\n",
      "epoch 147 takes 0.190992832184 seconds\n",
      "epoch 148 takes 0.21222615242 seconds\n",
      "epoch 149 takes 0.19476890564 seconds\n",
      "epoch 150 takes 0.20392203331 seconds\n",
      "Likelihood -268.87135334302553 takes 25.071215868 seconds\n",
      "epoch 151 takes 0.189931869507 seconds\n",
      "epoch 152 takes 0.207515001297 seconds\n",
      "epoch 153 takes 0.224764108658 seconds\n",
      "epoch 154 takes 0.231050014496 seconds\n",
      "epoch 155 takes 0.205899000168 seconds\n",
      "epoch 156 takes 0.192745923996 seconds\n",
      "epoch 157 takes 0.195454835892 seconds\n",
      "epoch 158 takes 0.207230091095 seconds\n",
      "epoch 159 takes 0.196930885315 seconds\n",
      "epoch 160 takes 0.197667121887 seconds\n",
      "epoch 161 takes 0.181420087814 seconds\n",
      "epoch 162 takes 0.189275979996 seconds\n",
      "epoch 163 takes 0.19924902916 seconds\n",
      "epoch 164 takes 0.193773031235 seconds\n",
      "epoch 165 takes 0.191759109497 seconds\n",
      "epoch 166 takes 0.197016954422 seconds\n",
      "epoch 167 takes 0.203273057938 seconds\n",
      "epoch 168 takes 0.204864025116 seconds\n",
      "epoch 169 takes 0.206125974655 seconds\n",
      "epoch 170 takes 0.212924003601 seconds\n",
      "epoch 171 takes 0.209422111511 seconds\n",
      "epoch 172 takes 0.240700006485 seconds\n",
      "epoch 173 takes 0.225781917572 seconds\n",
      "epoch 174 takes 0.20606303215 seconds\n",
      "epoch 175 takes 0.200349092484 seconds\n",
      "epoch 176 takes 0.20862698555 seconds\n",
      "epoch 177 takes 0.215065002441 seconds\n",
      "epoch 178 takes 0.186352968216 seconds\n",
      "epoch 179 takes 0.19932103157 seconds\n",
      "epoch 180 takes 0.203765869141 seconds\n",
      "epoch 181 takes 0.21114897728 seconds\n",
      "epoch 182 takes 0.225880146027 seconds\n",
      "epoch 183 takes 0.188879966736 seconds\n",
      "epoch 184 takes 0.201760053635 seconds\n",
      "epoch 185 takes 0.201359987259 seconds\n",
      "epoch 186 takes 0.233012914658 seconds\n",
      "epoch 187 takes 0.20201587677 seconds\n",
      "epoch 188 takes 0.193706035614 seconds\n",
      "epoch 189 takes 0.200732946396 seconds\n",
      "epoch 190 takes 0.206905126572 seconds\n",
      "epoch 191 takes 0.197685956955 seconds\n",
      "epoch 192 takes 0.228157997131 seconds\n",
      "epoch 193 takes 0.21541595459 seconds\n",
      "epoch 194 takes 0.227710962296 seconds\n",
      "epoch 195 takes 0.22216796875 seconds\n",
      "epoch 196 takes 0.234226942062 seconds\n",
      "epoch 197 takes 0.226541996002 seconds\n",
      "epoch 198 takes 0.230264902115 seconds\n",
      "epoch 199 takes 0.215276002884 seconds\n",
      "Train phase finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Initial Params =======================#\n",
    "import PGBN_sampler \n",
    "from scipy.special import gamma\n",
    "Params = {}\n",
    "\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K1'], 1]) / Setting['K1']\n",
    "\n",
    "# Collection\n",
    "W_train = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "import time\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "\n",
    "#========================== Gibbs ==========================＃\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================＃\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])     # Augmentation on D\n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1'])    # Augmentation on w\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32')\n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1  # padding\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "\n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape, dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "\n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64') # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')  # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3), axis=2) # N*K1\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update D\n",
    "    for k1 in range(Setting['K1']):\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] \n",
    "        D1_k1_s = (X_k1_34 + SuperParams['eta']) / np.sum(X_k1_34 + SuperParams['eta'])\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "\n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    # Update w_j\n",
    "    W_k1_sn = np.random.gamma(Params['W1_nk1_Aug_Pooling'].T + Params['Gamma']) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn)  # N*K1\n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    " \n",
    "    end_time = time.time()\n",
    "\n",
    "    if t == 0:\n",
    "        Iter_time.append(end_time - start_time)\n",
    "    else:\n",
    "        Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "    #====================== Likelihood ======================#\n",
    "    if np.mod(t,50) == 0:\n",
    "        \n",
    "        likelyhood = 0\n",
    "        start_time = time.time()\n",
    "        Orgin_X = np.zeros([Setting['N_train'], Setting['K1_V1'], Setting['K1_V2']])\n",
    "        Orgin_X[[batch_file_index, batch_rows, batch_cols+1]] = batch_value\n",
    "\n",
    "        for i in range(Setting['N_train']):\n",
    "            \n",
    "            Phi_tmp = np.transpose(np.reshape(Params['D1_k1'],[Setting['K1'], Setting['K1_S3'], Setting['K1_S4'], 1]),[1,2,3,0])\n",
    "            Theta_tmp = np.transpose(Params['W1_nk1'][i:i+1,:,:,:], [0,2,3,1])\n",
    "            PhiTheta_1= sess.run(X_1, feed_dict={Phi_1:Phi_tmp.astype(np.float32), Theta_1:Theta_tmp.astype(np.float32)})\n",
    "\n",
    "            likelyhood = likelyhood + np.sum(Orgin_X[i,:,:] * log_max(PhiTheta_1[0,:,:,0]) - PhiTheta_1[0,:,:,0] - log_max(gamma(Orgin_X[i,:,:] + 1)))  \n",
    "        end_time = time.time()\n",
    "        print \"Likelihood \" + str(likelyhood / Setting['N_train']) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "        Iter_lh.append(likelyhood / Setting['N_train'])\n",
    "        \n",
    "print \"Train phase finished\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
