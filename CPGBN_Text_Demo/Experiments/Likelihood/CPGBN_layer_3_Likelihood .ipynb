{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "Preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Tensorflow initial finished\n",
      "CUDA initial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.RandomState(1)\n",
    "\n",
    "realmin = 2.2e-10\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "#====================== Load data ======================#\n",
    "import cPickle\n",
    "\n",
    "DATA = cPickle.load(open(\"./TREC_3k-12-6.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = DATA['Vocabulary']\n",
    "data_vab_count_list    = DATA['Vab_count']\n",
    "data_vab_length        = DATA['Vab_Size']\n",
    "data_label             = DATA['Label']\n",
    "data_train_list        = DATA['Train_Origin']\n",
    "data_train_label       = np.array(DATA['Train_Label'])\n",
    "data_train_split       = DATA['Train_Word_Split']\n",
    "data_train_list_index  = DATA['Train_Word2Index']\n",
    "data_test_list         = DATA['Test_Origin']\n",
    "data_test_label        = np.array(DATA['Test_Label'])\n",
    "data_test_split        = DATA['Test_Word_Split']\n",
    "data_test_list_index   = DATA['Test_Word2Index']\n",
    "data_value             = 10\n",
    "\n",
    "print 'Load data'\n",
    "\n",
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_train_list)): \n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0)\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "\n",
    "print 'Preprocess finished'\n",
    "\n",
    "batch_len_tr        = batch_len\n",
    "batch_rows_tr       = batch_rows\n",
    "batch_cols_tr       = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "\n",
    "#======================= Setting =======================#\n",
    "Setting = {}\n",
    "Setting['N_train'] = len(data_train_list) - delete_count \n",
    "Setting['N_test']  = len(data_test_list)\n",
    "# 1-th layer\n",
    "Setting['K1']      = 32\n",
    "Setting['K1_V1']   = DATA['Vab_Size']\n",
    "Setting['K1_V2']   = np.max(batch_len) + 2  # padding\n",
    "Setting['K1_S3']   = DATA['Vab_Size']\n",
    "Setting['K1_S4']   = 3\n",
    "Setting['K1_S1']   = Setting['K1_V1'] + 1 - Setting['K1_S3']\n",
    "Setting['K1_S2']   = Setting['K1_V2'] + 1 - Setting['K1_S4']\n",
    "# 2-th layer\n",
    "Setting['K2']      = 16\n",
    "Setting['K2_V1']   = Setting['K1_S1'] \n",
    "Setting['K2_V2']   = Setting['K1_S2']  + 2  \n",
    "Setting['K2_S3']   = 1\n",
    "Setting['K2_S4']   = 3\n",
    "Setting['K2_S1']   = Setting['K2_V1'] + 1 - Setting['K2_S3']\n",
    "Setting['K2_S2']   = Setting['K2_V2'] + 1 - Setting['K2_S4']\n",
    "# 3-th layer\n",
    "Setting['K3']      = 8\n",
    "Setting['K3_V1']   = Setting['K2_S1']  \n",
    "Setting['K3_V2']   = Setting['K2_S2']  + 2  \n",
    "Setting['K3_S3']   = 1\n",
    "Setting['K3_S4']   = 3\n",
    "Setting['K3_S1']   = Setting['K3_V1'] + 1 - Setting['K3_S3']\n",
    "Setting['K3_S2']   = Setting['K3_V2'] + 1 - Setting['K3_S4']\n",
    "\n",
    "Setting['Iter']       = 200\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "#======================= SuperParams =======================#\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.05 # Phi\n",
    "\n",
    "#======================= Tensorflow Initial =======================#\n",
    "# Initial Graph\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) #HWC\n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print 'Tensorflow initial finished'\n",
    "\n",
    "#====================== CUDA Initial ======================#\n",
    "# Note， do not add any cuda operation among CUDA initial such as Tensorflow!!!!!!!!!!!!!!!!!!\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"CUDA initial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 0.307423830032 seconds\n",
      "Likelihood -518.8165414634121 takes 31.4894177914 seconds\n",
      "epoch 1 takes 0.323964834213 seconds\n",
      "epoch 2 takes 0.313561916351 seconds\n",
      "epoch 3 takes 0.303629159927 seconds\n",
      "epoch 4 takes 0.310297966003 seconds\n",
      "epoch 5 takes 0.302727937698 seconds\n",
      "epoch 6 takes 0.305879116058 seconds\n",
      "epoch 7 takes 0.301468133926 seconds\n",
      "epoch 8 takes 0.311198949814 seconds\n",
      "epoch 9 takes 0.29713177681 seconds\n",
      "epoch 10 takes 0.304172992706 seconds\n",
      "epoch 11 takes 0.338248968124 seconds\n",
      "epoch 12 takes 0.303249835968 seconds\n",
      "epoch 13 takes 0.309933900833 seconds\n",
      "epoch 14 takes 0.338131189346 seconds\n",
      "epoch 15 takes 0.317309856415 seconds\n",
      "epoch 16 takes 0.315564155579 seconds\n",
      "epoch 17 takes 0.313064098358 seconds\n",
      "epoch 18 takes 0.324512004852 seconds\n",
      "epoch 19 takes 0.299966096878 seconds\n",
      "epoch 20 takes 0.312839984894 seconds\n",
      "epoch 21 takes 0.301939964294 seconds\n",
      "epoch 22 takes 0.305242776871 seconds\n",
      "epoch 23 takes 0.299085140228 seconds\n",
      "epoch 24 takes 0.313480854034 seconds\n",
      "epoch 25 takes 0.299092054367 seconds\n",
      "epoch 26 takes 0.313207864761 seconds\n",
      "epoch 27 takes 0.319657087326 seconds\n",
      "epoch 28 takes 0.318965911865 seconds\n",
      "epoch 29 takes 0.3177921772 seconds\n",
      "epoch 30 takes 0.319245100021 seconds\n",
      "epoch 31 takes 0.325155973434 seconds\n",
      "epoch 32 takes 0.334738969803 seconds\n",
      "epoch 33 takes 0.32359790802 seconds\n",
      "epoch 34 takes 0.306304931641 seconds\n",
      "epoch 35 takes 0.307504892349 seconds\n",
      "epoch 36 takes 0.294635057449 seconds\n",
      "epoch 37 takes 0.297502994537 seconds\n",
      "epoch 38 takes 0.315189838409 seconds\n",
      "epoch 39 takes 0.316304922104 seconds\n",
      "epoch 40 takes 0.319672107697 seconds\n",
      "epoch 41 takes 0.333779096603 seconds\n",
      "epoch 42 takes 0.304086208344 seconds\n",
      "epoch 43 takes 0.29899597168 seconds\n",
      "epoch 44 takes 0.29173707962 seconds\n",
      "epoch 45 takes 0.301502943039 seconds\n",
      "epoch 46 takes 0.390897035599 seconds\n",
      "epoch 47 takes 0.393732070923 seconds\n",
      "epoch 48 takes 0.405393838882 seconds\n",
      "epoch 49 takes 0.418869972229 seconds\n",
      "epoch 50 takes 0.396447896957 seconds\n",
      "Likelihood -228.427369542407 takes 32.3357310295 seconds\n",
      "epoch 51 takes 0.324661970139 seconds\n",
      "epoch 52 takes 0.31578207016 seconds\n",
      "epoch 53 takes 0.321697950363 seconds\n",
      "epoch 54 takes 0.321415185928 seconds\n",
      "epoch 55 takes 0.321826934814 seconds\n",
      "epoch 56 takes 0.306510925293 seconds\n",
      "epoch 57 takes 0.318255901337 seconds\n",
      "epoch 58 takes 0.301898956299 seconds\n",
      "epoch 59 takes 0.314348936081 seconds\n",
      "epoch 60 takes 0.405278921127 seconds\n",
      "epoch 61 takes 0.397588968277 seconds\n",
      "epoch 62 takes 0.392413854599 seconds\n",
      "epoch 63 takes 0.400828123093 seconds\n",
      "epoch 64 takes 0.401319026947 seconds\n",
      "epoch 65 takes 0.394613027573 seconds\n",
      "epoch 66 takes 0.315387964249 seconds\n",
      "epoch 67 takes 0.316514968872 seconds\n",
      "epoch 68 takes 0.30281496048 seconds\n",
      "epoch 69 takes 0.32692694664 seconds\n",
      "epoch 70 takes 0.335520029068 seconds\n",
      "epoch 71 takes 0.349376916885 seconds\n",
      "epoch 72 takes 0.352060079575 seconds\n",
      "epoch 73 takes 0.322486877441 seconds\n",
      "epoch 74 takes 0.34610915184 seconds\n",
      "epoch 75 takes 0.340859889984 seconds\n",
      "epoch 76 takes 0.320708990097 seconds\n",
      "epoch 77 takes 0.324414968491 seconds\n",
      "epoch 78 takes 0.329084873199 seconds\n",
      "epoch 79 takes 0.321145772934 seconds\n",
      "epoch 80 takes 0.318719863892 seconds\n",
      "epoch 81 takes 0.323633909225 seconds\n",
      "epoch 82 takes 0.313858985901 seconds\n",
      "epoch 83 takes 0.323750972748 seconds\n",
      "epoch 84 takes 0.331233978271 seconds\n",
      "epoch 85 takes 0.303542137146 seconds\n",
      "epoch 86 takes 0.317308187485 seconds\n",
      "epoch 87 takes 0.324387073517 seconds\n",
      "epoch 88 takes 0.322220087051 seconds\n",
      "epoch 89 takes 0.30703496933 seconds\n",
      "epoch 90 takes 0.297607183456 seconds\n",
      "epoch 91 takes 0.332627058029 seconds\n",
      "epoch 92 takes 0.313725948334 seconds\n",
      "epoch 93 takes 0.335396051407 seconds\n",
      "epoch 94 takes 0.310364961624 seconds\n",
      "epoch 95 takes 0.302199840546 seconds\n",
      "epoch 96 takes 0.304378032684 seconds\n",
      "epoch 97 takes 0.316111087799 seconds\n",
      "epoch 98 takes 0.316522836685 seconds\n",
      "epoch 99 takes 0.311686038971 seconds\n",
      "epoch 100 takes 0.315570116043 seconds\n",
      "Likelihood -227.46521617607453 takes 31.495857954 seconds\n",
      "epoch 101 takes 0.412167072296 seconds\n",
      "epoch 102 takes 0.383992910385 seconds\n",
      "epoch 103 takes 0.397356033325 seconds\n",
      "epoch 104 takes 0.388015985489 seconds\n",
      "epoch 105 takes 0.40402507782 seconds\n",
      "epoch 106 takes 0.402770996094 seconds\n",
      "epoch 107 takes 0.403896093369 seconds\n",
      "epoch 108 takes 0.396836042404 seconds\n",
      "epoch 109 takes 0.415093183517 seconds\n",
      "epoch 110 takes 0.400346040726 seconds\n",
      "epoch 111 takes 0.323586940765 seconds\n",
      "epoch 112 takes 0.317099809647 seconds\n",
      "epoch 113 takes 0.337287902832 seconds\n",
      "epoch 114 takes 0.322423934937 seconds\n",
      "epoch 115 takes 0.333873033524 seconds\n",
      "epoch 116 takes 0.321801185608 seconds\n",
      "epoch 117 takes 0.342509031296 seconds\n",
      "epoch 118 takes 0.317921876907 seconds\n",
      "epoch 119 takes 0.32600903511 seconds\n",
      "epoch 120 takes 0.313694953918 seconds\n",
      "epoch 121 takes 0.347980976105 seconds\n",
      "epoch 122 takes 0.315626859665 seconds\n",
      "epoch 123 takes 0.312537908554 seconds\n",
      "epoch 124 takes 0.33979511261 seconds\n",
      "epoch 125 takes 0.329597949982 seconds\n",
      "epoch 126 takes 0.31985616684 seconds\n",
      "epoch 127 takes 0.325623035431 seconds\n",
      "epoch 128 takes 0.333275079727 seconds\n",
      "epoch 129 takes 0.312165975571 seconds\n",
      "epoch 130 takes 0.305500984192 seconds\n",
      "epoch 131 takes 0.315237998962 seconds\n",
      "epoch 132 takes 0.304397106171 seconds\n",
      "epoch 133 takes 0.317184925079 seconds\n",
      "epoch 134 takes 0.325370073318 seconds\n",
      "epoch 135 takes 0.327043056488 seconds\n",
      "epoch 136 takes 0.34358382225 seconds\n",
      "epoch 137 takes 0.317068099976 seconds\n",
      "epoch 138 takes 0.33346581459 seconds\n",
      "epoch 139 takes 0.313906908035 seconds\n",
      "epoch 140 takes 0.306535959244 seconds\n",
      "epoch 141 takes 0.328295946121 seconds\n",
      "epoch 142 takes 0.319725990295 seconds\n",
      "epoch 143 takes 0.306056022644 seconds\n",
      "epoch 144 takes 0.323228120804 seconds\n",
      "epoch 145 takes 0.318971157074 seconds\n",
      "epoch 146 takes 0.379692077637 seconds\n",
      "epoch 147 takes 0.396524906158 seconds\n",
      "epoch 148 takes 0.386138916016 seconds\n",
      "epoch 149 takes 0.406622886658 seconds\n",
      "epoch 150 takes 0.397085189819 seconds\n",
      "Likelihood -227.10003150944465 takes 35.5845148563 seconds\n",
      "epoch 151 takes 0.323898077011 seconds\n",
      "epoch 152 takes 0.320561170578 seconds\n",
      "epoch 153 takes 0.319041013718 seconds\n",
      "epoch 154 takes 0.335500001907 seconds\n",
      "epoch 155 takes 0.305978059769 seconds\n",
      "epoch 156 takes 0.32435798645 seconds\n",
      "epoch 157 takes 0.323171138763 seconds\n",
      "epoch 158 takes 0.323787927628 seconds\n",
      "epoch 159 takes 0.341885089874 seconds\n",
      "epoch 160 takes 0.323542118073 seconds\n",
      "epoch 161 takes 0.327884912491 seconds\n",
      "epoch 162 takes 0.383734941483 seconds\n",
      "epoch 163 takes 0.42128610611 seconds\n",
      "epoch 164 takes 0.397919893265 seconds\n",
      "epoch 165 takes 0.397100925446 seconds\n",
      "epoch 166 takes 0.317126989365 seconds\n",
      "epoch 167 takes 0.325097084045 seconds\n",
      "epoch 168 takes 0.31214594841 seconds\n",
      "epoch 169 takes 0.351287841797 seconds\n",
      "epoch 170 takes 0.318047046661 seconds\n",
      "epoch 171 takes 0.307008981705 seconds\n",
      "epoch 172 takes 0.321333885193 seconds\n",
      "epoch 173 takes 0.323140859604 seconds\n",
      "epoch 174 takes 0.306231975555 seconds\n",
      "epoch 175 takes 0.315507888794 seconds\n",
      "epoch 176 takes 0.322360992432 seconds\n",
      "epoch 177 takes 0.318212032318 seconds\n",
      "epoch 178 takes 0.369884967804 seconds\n",
      "epoch 179 takes 0.374503135681 seconds\n",
      "epoch 180 takes 0.388401985168 seconds\n",
      "epoch 181 takes 0.32711315155 seconds\n",
      "epoch 182 takes 0.323825120926 seconds\n",
      "epoch 183 takes 0.32689499855 seconds\n",
      "epoch 184 takes 0.328236818314 seconds\n",
      "epoch 185 takes 0.298188924789 seconds\n",
      "epoch 186 takes 0.319311141968 seconds\n",
      "epoch 187 takes 0.319546937943 seconds\n",
      "epoch 188 takes 0.319130897522 seconds\n",
      "epoch 189 takes 0.327263116837 seconds\n",
      "epoch 190 takes 0.322607040405 seconds\n",
      "epoch 191 takes 0.323143959045 seconds\n",
      "epoch 192 takes 0.318094015121 seconds\n",
      "epoch 193 takes 0.29402589798 seconds\n",
      "epoch 194 takes 0.384582996368 seconds\n",
      "epoch 195 takes 0.380021095276 seconds\n",
      "epoch 196 takes 0.399047851562 seconds\n",
      "epoch 197 takes 0.39321398735 seconds\n",
      "epoch 198 takes 0.418023824692 seconds\n",
      "epoch 199 takes 0.38297700882 seconds\n",
      "Train phase finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Initial Params =======================#\n",
    "import PGBN_sampler \n",
    "from scipy.special import gamma\n",
    "Params = {}\n",
    "\n",
    "# 1-th layer\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "# 2-th layer\n",
    "Params['Phi_2']  = 0.2 + 0.8*np.random.rand(Setting['K1'], Setting['K2'])\n",
    "Params['Phi_2']  = Params['Phi_2'] / np.sum(Params['Phi_2'], axis=0)\n",
    "Params['Theta_2']= np.random.rand(Setting['N_train'], Setting['K2'])\n",
    "\n",
    "Params['c3_n']   = 1 * np.ones([Setting['N_train']])\n",
    "tmp = -log_max(1 - Params['p2_n'])\n",
    "Params['p3_n']   = (tmp / (tmp + Params['c3_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "# 3-th layer\n",
    "Params['Phi_3']  = 0.2 + 0.8*np.random.rand(Setting['K2'], Setting['K3'])\n",
    "Params['Phi_3']  = Params['Phi_3'] / np.sum(Params['Phi_3'], axis=0)\n",
    "Params['Theta_3']= np.random.rand(Setting['N_train'], Setting['K3'])\n",
    "\n",
    "Params['c4_n']   = 1 * np.ones([Setting['N_train']])\n",
    "tmp = -log_max(1 - Params['p3_n'])\n",
    "Params['p4_n']   = (tmp / (tmp + Params['c4_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K3'], 1]) / Setting['K3']\n",
    "\n",
    "# Collection\n",
    "W_train_1 = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "W_train_2 = np.zeros([Setting['N_train'], Setting['K2']])\n",
    "W_train_3 = np.zeros([Setting['N_train'], Setting['K3']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "import time\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "\n",
    "#========================== Gibbs ==========================#\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================#\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])     # Augmentation on D\n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1'])    # Augmentation on w\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32') \n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1  # padding\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "    \n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "    \n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64') # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')  # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3),axis=2) # N*K1\n",
    "    \n",
    "    #========================== 2nd layer Augmentation ==========================#\n",
    "    M1_tmp = np.array(np.transpose(np.round(Params['W1_nk1_Aug_Pooling'])), dtype='float64', order='C')\n",
    "    Theta2_tmp = np.array(np.transpose(Params['Theta_2']), dtype='float64', order='C')\n",
    "    Xt_to_t1_2,WSZS_2 = PGBN_sampler.Crt_Multirnd_Matrix(M1_tmp, Params['Phi_2'], Theta2_tmp)\n",
    "    \n",
    "    #========================== 3rd layer Augmentation ==========================#\n",
    "    M2_tmp = np.array(np.round(Xt_to_t1_2), dtype='float64', order='C')\n",
    "    Theta3_tmp = np.array(np.transpose(Params['Theta_3']), dtype='float64', order='C')\n",
    "    Xt_to_t1_3,WSZS_3 = PGBN_sampler.Crt_Multirnd_Matrix(M2_tmp, Params['Phi_3'], Theta3_tmp)\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update D,Phi\n",
    "    for k1 in range(Setting['K1']):\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] # 按三四维增广的矩阵\n",
    "        X_k1_34_tmp = np.random.gamma(X_k1_34 + SuperParams['eta'])\n",
    "        D1_k1_s     = X_k1_34_tmp / np.sum(X_k1_34_tmp)\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "        \n",
    "    Phi_2_tmp       = np.random.gamma(WSZS_2 + SuperParams['eta'])\n",
    "    Params['Phi_2'] = Phi_2_tmp / np.sum(Phi_2_tmp, axis=0)\n",
    "    \n",
    "    Phi_3_tmp       = np.random.gamma(WSZS_3 + SuperParams['eta'])\n",
    "    Params['Phi_3'] = Phi_3_tmp / np.sum(Phi_3_tmp, axis=0)\n",
    "    \n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_2'], Params['Theta_2'].T),0)) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    Params['c3_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_3'], Params['Theta_3'].T),0)) \n",
    "    Params['c3_n']     = Params['c3_n'] / (SuperParams['f0'] + np.sum(Params['Theta_2'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p2_n'])\n",
    "    Params['p3_n']     = tmp / (Params['c3_n'] + tmp)\n",
    "    \n",
    "    Params['c4_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c4_n']     = Params['c4_n'] / (SuperParams['f0'] + np.sum(Params['Theta_3'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p3_n'])\n",
    "    Params['p4_n']     = tmp / (Params['c4_n'] + tmp)\n",
    "    \n",
    "    # Update w_j\n",
    "    W_k3_sn = np.random.gamma(Params['Gamma'] + Xt_to_t1_3) / (-np.log(1-Params['p3_n']) + Params['c4_n']) # V*N\n",
    "    Params['Theta_3'] = np.transpose(W_k3_sn)\n",
    "    \n",
    "    shape2 = np.dot(Params['Phi_3'], Params['Theta_3'].T)\n",
    "    W_k2_sn = np.random.gamma(shape2 + Xt_to_t1_2) / (-np.log(1-Params['p2_n']) + Params['c3_n']) # V*N\n",
    "    Params['Theta_2'] = np.transpose(W_k2_sn)\n",
    "    \n",
    "    shape1 = np.dot(Params['Phi_2'], Params['Theta_2'].T) # V*N\n",
    "    W_k1_sn = np.random.gamma(shape1 + Params['W1_nk1_Aug_Pooling'].T ) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn) \n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if t == 0:\n",
    "        Iter_time.append(end_time - start_time)\n",
    "    else:\n",
    "        Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    # Likelyhood\n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "    #====================== Likelihood ======================# \n",
    "    if np.mod(t,50) == 0:\n",
    "        \n",
    "        likelyhood = 0\n",
    "        start_time = time.time()\n",
    "        Orgin_X = np.zeros([Setting['N_train'], Setting['K1_V1'], Setting['K1_V2']])\n",
    "        Orgin_X[[batch_file_index, batch_rows, batch_cols+1]] = batch_value\n",
    "\n",
    "        for i in range(Setting['N_train']):\n",
    "            \n",
    "            Phi_tmp = np.transpose(np.reshape(Params['D1_k1'],[Setting['K1'], Setting['K1_S3'], Setting['K1_S4'], 1]),[1,2,3,0])\n",
    "            Theta_tmp = np.transpose(Params['W1_nk1'][i:i+1,:,:,:], [0,2,3,1])\n",
    "            PhiTheta_1= sess.run(X_1, feed_dict={Phi_1:Phi_tmp.astype(np.float32), Theta_1:Theta_tmp.astype(np.float32)})\n",
    "            PhiTheta_1= PhiTheta_1.astype(np.float64)\n",
    "            likelyhood = likelyhood + np.sum(Orgin_X[i,:,:] * log_max(PhiTheta_1[0,:,:,0]) - PhiTheta_1[0,:,:,0] - log_max(gamma(Orgin_X[i,:,:] + 1)))  \n",
    "        end_time = time.time()\n",
    "        print \"Likelihood \" + str(likelyhood / Setting['N_train']) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "        Iter_lh.append(likelyhood / Setting['N_train'])\n",
    "    \n",
    "print \"Train phase finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
