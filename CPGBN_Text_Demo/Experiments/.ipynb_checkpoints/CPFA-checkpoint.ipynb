{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "kernel intial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy.special import gamma\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.RandomState(1)\n",
    "realmin = 2.2e-10\n",
    "\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "def MultRnd(value, MultRate):\n",
    "    MultRate = MultRate / np.sum(MultRate)\n",
    "    MultRate_Sum = np.reshape(MultRate, [-1])\n",
    "    N = len(MultRate_Sum)\n",
    "    Amnk = np.zeros([N])\n",
    "\n",
    "    if N == 1:\n",
    "        Amnk = value\n",
    "    else:\n",
    "        for i in range(1, N, 1):  \n",
    "            MultRate_Sum[i] = MultRate_Sum[i] + MultRate_Sum[i - 1]\n",
    "\n",
    "        Uni_Rnd = np.random.rand(np.int64(value))    # 0 - 1\n",
    "        flag_new = Uni_Rnd <= MultRate_Sum[0]\n",
    "        Amnk[0] = np.sum(flag_new)\n",
    "        \n",
    "        for i in range(1, N, 1):  \n",
    "            flag_old = flag_new\n",
    "            flag_new = Uni_Rnd <= MultRate_Sum[i]\n",
    "            Amnk[i] = np.sum(~flag_old & flag_new)\n",
    "\n",
    "        Amnk = np.reshape(Amnk, MultRate.shape)\n",
    "        \n",
    "    return Amnk\n",
    "\n",
    "def Dis_Dic(D):\n",
    "    [K, K1, K2] = D.shape\n",
    "    w_n = np.ceil(np.sqrt(K))\n",
    "    h_n = np.ceil(K / w_n)\n",
    "    weight = w_n * K2\n",
    "    height = h_n * K1\n",
    "    Dic = np.zeros([np.int32(weight), np.int32(height)])\n",
    "    count = 0\n",
    "    for k1 in range(np.int32(w_n)):\n",
    "        for k2 in range(np.int32(h_n)):\n",
    "            Dic[k1 * K1: (k1 + 1) * K1, k2 * K2: (k2 + 1) * K2] = D[count, :, :]\n",
    "            count += 1\n",
    "            if count == K:\n",
    "                break\n",
    "        if count == K:\n",
    "            break\n",
    "    plt.figure\n",
    "    plt.imshow(Dic)\n",
    "    plt.show()\n",
    "\n",
    "def Conv_Aug(Kernel, Score_Shape):\n",
    "    [K1, K2] = Score_Shape\n",
    "    [K3, K4] = Kernel.shape\n",
    "    V1 = K1 + K3 - 1\n",
    "    V2 = K2 + K4 - 1\n",
    "\n",
    "    # Padding\n",
    "    Kernel_Pad = np.zeros([2 * V1 - K3, 2 * V2 - K4])  ## Pad [V1 - K3, V2 - K4]\n",
    "    Kernel_Pad[V1 - K3: V1, V2 - K4: V2] = Kernel\n",
    "    Kernel_Pad = Kernel_Pad.T\n",
    "    M, N = Kernel_Pad.shape\n",
    "    # Parameters\n",
    "    col_extent = N - K1 + 1\n",
    "    row_extent = M - K2 + 1\n",
    "\n",
    "    # Get Starting block indices\n",
    "    start_idx = np.arange(K2)[:, None] * N + np.arange(K1)\n",
    "    # Get offsetted indices across the height and width of input array\n",
    "    offset_idx = np.arange(row_extent)[:, None] * N + np.arange(col_extent)\n",
    "    # Get all actual indices & index into input array for final output\n",
    "    out = np.take(Kernel_Pad, start_idx.ravel()[:, None] + offset_idx.ravel())\n",
    "\n",
    "    return np.flip(out.T, axis=1)\n",
    "\n",
    "#=======================Load data=======================#\n",
    "import numpy as np\n",
    "import cPickle\n",
    "\n",
    "TREC = cPickle.load(open(\"./TREC.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = TREC['Vocabulary']\n",
    "data_vab_count_list    = TREC['Vab_count']\n",
    "data_vab_length        = TREC['Vab_Size']\n",
    "data_label             = TREC['Label']\n",
    "data_train_list        = TREC['Train_Origin']\n",
    "data_train_label       = np.array(TREC['Train_Label'])\n",
    "data_train_split       = TREC['Train_Word_Split']\n",
    "data_train_list_index  = TREC['Train_Word2Index']\n",
    "data_test_list         = TREC['Test_Origin']\n",
    "data_test_label        = np.array(TREC['Test_Label'])\n",
    "data_test_split        = TREC['Test_Word_Split']\n",
    "data_test_list_index   = TREC['Test_Word2Index']\n",
    "\n",
    "print 'load data'\n",
    "\n",
    "#=============preprocess==============#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_train_list)): \n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)                                        \n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * 10\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0) \n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * 10), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "\n",
    "print 'preprocess finished'\n",
    "\n",
    "batch_len_tr  = batch_len\n",
    "batch_rows_tr = batch_rows\n",
    "batch_cols_tr = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "# Setting\n",
    "Setting = {}\n",
    "Setting['N_train']    = len(data_train_list) - delete_count \n",
    "Setting['K1']         = 32\n",
    "Setting['K1_V1']      = TREC['Vab_Size']\n",
    "Setting['K1_V2']      = np.max(batch_len) + 2                     \n",
    "Setting['K1_S3']      = TREC['Vab_Size']\n",
    "Setting['K1_S4']      = 3\n",
    "Setting['K1_S1']      = Setting['K1_V1'] + 1 - Setting['K1_S3']\n",
    "Setting['K1_S2']      = Setting['K1_V2'] + 1 - Setting['K1_S4'] \n",
    "Setting['Iter']       = 200\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "# SuperParamsSetting\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.05 # Phi\n",
    "\n",
    "# Initial Graph\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) \n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# cuda par\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"kernel intial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 0.949940919876 seconds\n",
      "epoch 1 takes 0.844228029251 seconds\n",
      "epoch 2 takes 0.7796189785 seconds\n",
      "epoch 3 takes 0.829735994339 seconds\n",
      "epoch 4 takes 0.845260858536 seconds\n",
      "epoch 5 takes 0.81299996376 seconds\n",
      "epoch 6 takes 0.84748506546 seconds\n",
      "epoch 7 takes 0.898957014084 seconds\n",
      "epoch 8 takes 0.804496049881 seconds\n",
      "epoch 9 takes 0.863608121872 seconds\n",
      "epoch 10 takes 0.804375171661 seconds\n",
      "epoch 11 takes 0.944366931915 seconds\n",
      "epoch 12 takes 0.928349018097 seconds\n",
      "epoch 13 takes 0.829618930817 seconds\n",
      "epoch 14 takes 0.818796873093 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e6ce951f787a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'D1_k1_Aug'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD1_k1_Aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m                         \u001b[0;31m# K1*S3*S4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1_nk1_Aug_Pooling'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1_nk1_Aug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# N*K1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m#==========================Sampler==========================＃\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m     return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 1882\u001b[0;31m                          out=out, **kwargs)\n\u001b[0m\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initial Params\n",
    "Params = {}\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K1'], 1]) / Setting['K1']\n",
    "\n",
    "W_train = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "# Gibbs\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #==========================Augmentation==========================＃\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1']) \n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1']) \n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32')\n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1  \n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "\n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape, dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "\n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64')                        # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')                         # K1*S3*S4\n",
    "    \n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3), axis=2) # N*K1\n",
    "    \n",
    "    #==========================Sampler==========================＃\n",
    "    for k1 in range(Setting['K1']):\n",
    "        # update 1th D\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] \n",
    "        D1_k1_s = (X_k1_34 + SuperParams['eta']) / np.sum(X_k1_34 + SuperParams['eta'])\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    W_k1_sn = np.random.gamma(Params['W1_nk1_Aug_Pooling'].T + Params['Gamma']) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn)                                                   # N*K1\n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "    \n",
    "    if t >=Setting['Burinin']:\n",
    "        W_train = W_train + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])        \n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if t == 0:\n",
    "        Iter_time.append(end_time - start_time)\n",
    "    else:\n",
    "        Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "\n",
    "    #==========================Likelyhood==========================＃\n",
    "#     if np.mod(t,100) == 0:\n",
    "        \n",
    "#         likelyhood = 0\n",
    "#         start_time = time.time()\n",
    "#         Orgin_X = np.zeros([Setting['N_train'], Setting['K1_V1'], Setting['K1_V2']])\n",
    "#         Orgin_X[[batch_file_index, batch_rows, batch_cols+1]] = batch_value\n",
    "\n",
    "#         for i in range(Setting['N_train']):\n",
    "            \n",
    "#             Phi_tmp = np.transpose(np.reshape(Params['D1_k1'],[Setting['K1'], Setting['K1_S3'], Setting['K1_S4'], 1]),[1,2,3,0])\n",
    "#             Theta_tmp = np.transpose(Params['W1_nk1'][i:i+1,:,:,:], [0,2,3,1])\n",
    "#             PhiTheta_1= sess.run(X_1, feed_dict={Phi_1:Phi_tmp.astype(np.float32), Theta_1:Theta_tmp.astype(np.float32)})\n",
    "\n",
    "#             likelyhood = likelyhood + np.sum(Orgin_X[i,:,:] * log_max(PhiTheta_1[0,:,:,0]) - PhiTheta_1[0,:,:,0] - log_max(gamma(Orgin_X[i,:,:] + 1)))  \n",
    "#         end_time = time.time()\n",
    "#         print \"Likelihood \" + str(likelyhood / Setting['N_train']) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "#         Iter_lh.append(likelyhood / Setting['N_train'])\n",
    "\n",
    "            \n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(Iter_time ,Iter_lh)\n",
    "# plt.show()       \n",
    "\n",
    "# import numpy as np\n",
    "# import scipy.io as sio    \n",
    "        \n",
    "# sio.savemat('TREC_Layer1.mat',{'Time_Layer1':Iter_time, 'LH_Layer1':Iter_lh})\n",
    "# print 'save mat finished'\n",
    "        \n",
    "print \"train phase finished\" \n",
    "W_train =  W_train / Setting['Collection'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess finished\n",
      "epoch 0 takes 0.0199570655823 seconds\n",
      "epoch 1 takes 0.0185301303864 seconds\n",
      "epoch 2 takes 0.0200290679932 seconds\n",
      "epoch 3 takes 0.0185430049896 seconds\n",
      "epoch 4 takes 0.0183799266815 seconds\n",
      "epoch 5 takes 0.0176138877869 seconds\n",
      "epoch 6 takes 0.0188391208649 seconds\n",
      "epoch 7 takes 0.0179731845856 seconds\n",
      "epoch 8 takes 0.0178079605103 seconds\n",
      "epoch 9 takes 0.0171101093292 seconds\n",
      "epoch 10 takes 0.0185408592224 seconds\n",
      "epoch 11 takes 0.0193610191345 seconds\n",
      "epoch 12 takes 0.020397901535 seconds\n",
      "epoch 13 takes 0.0195310115814 seconds\n",
      "epoch 14 takes 0.0181279182434 seconds\n",
      "epoch 15 takes 0.0191988945007 seconds\n",
      "epoch 16 takes 0.0190598964691 seconds\n",
      "epoch 17 takes 0.0189878940582 seconds\n",
      "epoch 18 takes 0.0192351341248 seconds\n",
      "epoch 19 takes 0.0196549892426 seconds\n",
      "epoch 20 takes 0.0190598964691 seconds\n",
      "epoch 21 takes 0.0195970535278 seconds\n",
      "epoch 22 takes 0.0196220874786 seconds\n",
      "epoch 23 takes 0.0193839073181 seconds\n",
      "epoch 24 takes 0.0196149349213 seconds\n",
      "epoch 25 takes 0.0190179347992 seconds\n",
      "epoch 26 takes 0.017560005188 seconds\n",
      "epoch 27 takes 0.0175151824951 seconds\n",
      "epoch 28 takes 0.0173561573029 seconds\n",
      "epoch 29 takes 0.0175051689148 seconds\n",
      "epoch 30 takes 0.0174360275269 seconds\n",
      "epoch 31 takes 0.0176739692688 seconds\n",
      "epoch 32 takes 0.0193059444427 seconds\n",
      "epoch 33 takes 0.0177462100983 seconds\n",
      "epoch 34 takes 0.021143913269 seconds\n",
      "epoch 35 takes 0.0175149440765 seconds\n",
      "epoch 36 takes 0.0173349380493 seconds\n",
      "epoch 37 takes 0.0193979740143 seconds\n",
      "epoch 38 takes 0.0193719863892 seconds\n",
      "epoch 39 takes 0.0197360515594 seconds\n",
      "epoch 40 takes 0.0195000171661 seconds\n",
      "epoch 41 takes 0.0192649364471 seconds\n",
      "epoch 42 takes 0.0194940567017 seconds\n",
      "epoch 43 takes 0.0202670097351 seconds\n",
      "epoch 44 takes 0.0211780071259 seconds\n",
      "epoch 45 takes 0.0198879241943 seconds\n",
      "epoch 46 takes 0.0189828872681 seconds\n",
      "epoch 47 takes 0.02135014534 seconds\n",
      "epoch 48 takes 0.020336151123 seconds\n",
      "epoch 49 takes 0.0205969810486 seconds\n",
      "epoch 50 takes 0.0181460380554 seconds\n",
      "epoch 51 takes 0.0182030200958 seconds\n",
      "epoch 52 takes 0.0212688446045 seconds\n",
      "epoch 53 takes 0.0226030349731 seconds\n",
      "epoch 54 takes 0.018944978714 seconds\n",
      "epoch 55 takes 0.0207312107086 seconds\n",
      "epoch 56 takes 0.0203659534454 seconds\n",
      "epoch 57 takes 0.0189990997314 seconds\n",
      "epoch 58 takes 0.0227098464966 seconds\n",
      "epoch 59 takes 0.0199139118195 seconds\n",
      "epoch 60 takes 0.0196030139923 seconds\n",
      "epoch 61 takes 0.0181720256805 seconds\n",
      "epoch 62 takes 0.0198349952698 seconds\n",
      "epoch 63 takes 0.0206108093262 seconds\n",
      "epoch 64 takes 0.0190651416779 seconds\n",
      "epoch 65 takes 0.0197839736938 seconds\n",
      "epoch 66 takes 0.0195579528809 seconds\n",
      "epoch 67 takes 0.0192511081696 seconds\n",
      "epoch 68 takes 0.0229868888855 seconds\n",
      "epoch 69 takes 0.0207500457764 seconds\n",
      "epoch 70 takes 0.020054101944 seconds\n",
      "epoch 71 takes 0.0203490257263 seconds\n",
      "epoch 72 takes 0.021910905838 seconds\n",
      "epoch 73 takes 0.0208458900452 seconds\n",
      "epoch 74 takes 0.0188889503479 seconds\n",
      "epoch 75 takes 0.0192978382111 seconds\n",
      "epoch 76 takes 0.0204811096191 seconds\n",
      "epoch 77 takes 0.0221049785614 seconds\n",
      "epoch 78 takes 0.0179500579834 seconds\n",
      "epoch 79 takes 0.0177240371704 seconds\n",
      "epoch 80 takes 0.0193419456482 seconds\n",
      "epoch 81 takes 0.0199661254883 seconds\n",
      "epoch 82 takes 0.0193328857422 seconds\n",
      "epoch 83 takes 0.0180881023407 seconds\n",
      "epoch 84 takes 0.0189590454102 seconds\n",
      "epoch 85 takes 0.0199551582336 seconds\n",
      "epoch 86 takes 0.0194029808044 seconds\n",
      "epoch 87 takes 0.0194339752197 seconds\n",
      "epoch 88 takes 0.0196850299835 seconds\n",
      "epoch 89 takes 0.0212750434875 seconds\n",
      "epoch 90 takes 0.020430803299 seconds\n",
      "epoch 91 takes 0.0185918807983 seconds\n",
      "epoch 92 takes 0.0186281204224 seconds\n",
      "epoch 93 takes 0.0182898044586 seconds\n",
      "epoch 94 takes 0.016970872879 seconds\n",
      "epoch 95 takes 0.0176498889923 seconds\n",
      "epoch 96 takes 0.0206530094147 seconds\n",
      "epoch 97 takes 0.0189828872681 seconds\n",
      "epoch 98 takes 0.0197179317474 seconds\n",
      "epoch 99 takes 0.0195128917694 seconds\n",
      "epoch 100 takes 0.0197470188141 seconds\n",
      "epoch 101 takes 0.0194530487061 seconds\n",
      "epoch 102 takes 0.0194709300995 seconds\n",
      "epoch 103 takes 0.0195169448853 seconds\n",
      "epoch 104 takes 0.0222067832947 seconds\n",
      "epoch 105 takes 0.0188541412354 seconds\n",
      "epoch 106 takes 0.0190269947052 seconds\n",
      "epoch 107 takes 0.0196738243103 seconds\n",
      "epoch 108 takes 0.0178050994873 seconds\n",
      "epoch 109 takes 0.0168569087982 seconds\n",
      "epoch 110 takes 0.0208339691162 seconds\n",
      "epoch 111 takes 0.0198857784271 seconds\n",
      "epoch 112 takes 0.0188679695129 seconds\n",
      "epoch 113 takes 0.0188338756561 seconds\n",
      "epoch 114 takes 0.0195758342743 seconds\n",
      "epoch 115 takes 0.0209519863129 seconds\n",
      "epoch 116 takes 0.0196199417114 seconds\n",
      "epoch 117 takes 0.0198149681091 seconds\n",
      "epoch 118 takes 0.0199761390686 seconds\n",
      "epoch 119 takes 0.021332025528 seconds\n",
      "epoch 120 takes 0.0188300609589 seconds\n",
      "epoch 121 takes 0.0198910236359 seconds\n",
      "epoch 122 takes 0.0192070007324 seconds\n",
      "epoch 123 takes 0.0180599689484 seconds\n",
      "epoch 124 takes 0.0177810192108 seconds\n",
      "epoch 125 takes 0.0209248065948 seconds\n",
      "epoch 126 takes 0.0205790996552 seconds\n",
      "epoch 127 takes 0.0196599960327 seconds\n",
      "epoch 128 takes 0.0202507972717 seconds\n",
      "epoch 129 takes 0.0201048851013 seconds\n",
      "epoch 130 takes 0.0196149349213 seconds\n",
      "epoch 131 takes 0.0211548805237 seconds\n",
      "epoch 132 takes 0.0187199115753 seconds\n",
      "epoch 133 takes 0.0195138454437 seconds\n",
      "epoch 134 takes 0.0195820331573 seconds\n",
      "epoch 135 takes 0.0183689594269 seconds\n",
      "epoch 136 takes 0.0174551010132 seconds\n",
      "epoch 137 takes 0.0168609619141 seconds\n",
      "epoch 138 takes 0.017578125 seconds\n",
      "epoch 139 takes 0.0203139781952 seconds\n",
      "epoch 140 takes 0.0185000896454 seconds\n",
      "epoch 141 takes 0.0217010974884 seconds\n",
      "epoch 142 takes 0.0216429233551 seconds\n",
      "epoch 143 takes 0.0189681053162 seconds\n",
      "epoch 144 takes 0.0190172195435 seconds\n",
      "epoch 145 takes 0.0183160305023 seconds\n",
      "epoch 146 takes 0.0205888748169 seconds\n",
      "epoch 147 takes 0.0202960968018 seconds\n",
      "epoch 148 takes 0.021008014679 seconds\n",
      "epoch 149 takes 0.0201041698456 seconds\n",
      "epoch 150 takes 0.0195770263672 seconds\n",
      "epoch 151 takes 0.019996881485 seconds\n",
      "epoch 152 takes 0.0169808864594 seconds\n",
      "epoch 153 takes 0.0190169811249 seconds\n",
      "epoch 154 takes 0.0197517871857 seconds\n",
      "epoch 155 takes 0.0194139480591 seconds\n",
      "epoch 156 takes 0.0188989639282 seconds\n",
      "epoch 157 takes 0.0212061405182 seconds\n",
      "epoch 158 takes 0.0220508575439 seconds\n",
      "epoch 159 takes 0.0196900367737 seconds\n",
      "epoch 160 takes 0.0190110206604 seconds\n",
      "epoch 161 takes 0.0194718837738 seconds\n",
      "epoch 162 takes 0.0191059112549 seconds\n",
      "epoch 163 takes 0.0191650390625 seconds\n",
      "epoch 164 takes 0.0176651477814 seconds\n",
      "epoch 165 takes 0.0174770355225 seconds\n",
      "epoch 166 takes 0.0178489685059 seconds\n",
      "epoch 167 takes 0.0197539329529 seconds\n",
      "epoch 168 takes 0.0200710296631 seconds\n",
      "epoch 169 takes 0.0198140144348 seconds\n",
      "epoch 170 takes 0.0186238288879 seconds\n",
      "epoch 171 takes 0.0196170806885 seconds\n",
      "epoch 172 takes 0.0191719532013 seconds\n",
      "epoch 173 takes 0.0215609073639 seconds\n",
      "epoch 174 takes 0.0194401741028 seconds\n",
      "epoch 175 takes 0.0198588371277 seconds\n",
      "epoch 176 takes 0.017520904541 seconds\n",
      "epoch 177 takes 0.0194270610809 seconds\n",
      "epoch 178 takes 0.0200381278992 seconds\n",
      "epoch 179 takes 0.0198559761047 seconds\n",
      "epoch 180 takes 0.0204238891602 seconds\n",
      "epoch 181 takes 0.0185949802399 seconds\n",
      "epoch 182 takes 0.0191838741302 seconds\n",
      "epoch 183 takes 0.0193049907684 seconds\n",
      "epoch 184 takes 0.0192577838898 seconds\n",
      "epoch 185 takes 0.0195190906525 seconds\n",
      "epoch 186 takes 0.019544839859 seconds\n",
      "epoch 187 takes 0.0176630020142 seconds\n",
      "epoch 188 takes 0.0206110477448 seconds\n",
      "epoch 189 takes 0.0202589035034 seconds\n",
      "epoch 190 takes 0.0213048458099 seconds\n",
      "epoch 191 takes 0.0211160182953 seconds\n",
      "epoch 192 takes 0.019159078598 seconds\n",
      "epoch 193 takes 0.0201940536499 seconds\n",
      "epoch 194 takes 0.0199868679047 seconds\n",
      "epoch 195 takes 0.0194711685181 seconds\n",
      "epoch 196 takes 0.020024061203 seconds\n",
      "epoch 197 takes 0.0191268920898 seconds\n",
      "epoch 198 takes 0.0203900337219 seconds\n",
      "epoch 199 takes 0.0201680660248 seconds\n",
      "test phase finished\n"
     ]
    }
   ],
   "source": [
    "#=============preprocess==============#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_test_list)): \n",
    "    \n",
    "    x_single = np.reshape(data_test_list_index[i], [len(data_test_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * 10\n",
    "        batch_label      = np.array([data_test_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0)\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * 10), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_test_label[i]])),axis=0)\n",
    "\n",
    "batch_len_te        = batch_len\n",
    "batch_rows_te       = batch_rows\n",
    "batch_cols_te       = batch_cols\n",
    "batch_file_index_te = batch_file_index\n",
    "batch_value_te      = batch_value\n",
    "batch_label_te      = batch_label\n",
    "\n",
    "print 'preprocess finished'\n",
    "\n",
    "Setting['N_test']   = len(batch_len)        \n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_test'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_test']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "W_test = np.zeros([Setting['N_test'], Setting['K1']])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K1'], 1]) / Setting['K1']\n",
    "\n",
    "# Gibbs\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #==========================Augmentation==========================＃\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])  \n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1']) \n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32')\n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "\n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1        = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1         = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug    = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug     = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "\n",
    "    # 第一层增广的结果\n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64')                        # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')                         # K1*S3*S4\n",
    "    \n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3), axis=2) # N*K1\n",
    "    \n",
    "    #==========================Sampler==========================＃\n",
    "    \n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    W_k1_sn = np.random.gamma(Params['W1_nk1_Aug_Pooling'].T + Params['Gamma']) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn)  # N*K1\n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        \n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "\n",
    "    if t >=Setting['Burinin']:\n",
    "        \n",
    "        W_test = W_test + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Likelyhood\n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "        \n",
    "#     if np.mod(t+1,100) == 0:\n",
    "        \n",
    "#         likelyhood = 0\n",
    "#         start_time = time.time()\n",
    "#         Orgin_X = np.zeros([Setting['N_test'], Setting['K1_V1'], Setting['K1_V2']])\n",
    "#         Orgin_X[[batch_file_index_te, batch_rows_te, batch_cols_te+1]] = batch_value_te\n",
    "\n",
    "#         for i in range(Setting['N_test']):\n",
    "            \n",
    "#             Phi_tmp = np.transpose(np.reshape(Params['D1_k1'],[Setting['K1'], Setting['K1_S3'], Setting['K1_S4'], 1]),[1,2,3,0])\n",
    "#             Theta_tmp = np.transpose(Params['W1_nk1'][i:i+1,:,:,:], [0,2,3,1])\n",
    "#             PhiTheta_1= sess.run(X_1, feed_dict={Phi_1:Phi_tmp.astype(np.float64), Theta_1:Theta_tmp.astype(np.float64)})\n",
    "\n",
    "#             likelyhood = likelyhood + np.sum(Orgin_X[i,:,:] * log_max(PhiTheta_1[0,:,:,0]) - PhiTheta_1[0,:,:,0] - log_max(gamma(Orgin_X[i,:,:] + 1)))  \n",
    "#         end_time = time.time()\n",
    "#         print \"Likelihood \" + str(likelyhood / Setting['N_test']) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "\n",
    "    \n",
    "print \"test phase finished\"\n",
    "W_test =  W_test / Setting['Collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.566030814380044\n",
      "0.642\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# W_train = (W_train - np.reshape(np.mean(W_train,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_train, axis=0),[1,Setting['K1']])\n",
    "# W_test = (W_test - np.reshape(np.mean(W_test,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_test, axis=0),[1,Setting['K1']])\n",
    "\n",
    "\n",
    "clf = svm.SVC()                             # class\n",
    "clf.fit(W_train, batch_label_tr)            # training the svc model \n",
    "\n",
    "print clf.score(W_train, batch_label_tr)    # training the svc model \n",
    "print clf.score(W_test,  batch_label_te)    # training the svc model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
