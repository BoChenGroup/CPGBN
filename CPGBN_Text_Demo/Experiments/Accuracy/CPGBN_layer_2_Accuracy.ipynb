{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "Preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Tensorflow initial finished\n",
      "CUDA initial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.RandomState(1)\n",
    "\n",
    "realmin = 2.2e-10\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "#=============load data==============#\n",
    "import cPickle\n",
    "\n",
    "DATA = cPickle.load(open(\"./TREC.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = DATA['Vocabulary']\n",
    "data_vab_count_list    = DATA['Vab_count']\n",
    "data_vab_length        = DATA['Vab_Size']\n",
    "data_label             = DATA['Label']\n",
    "data_train_list        = DATA['Train_Origin']\n",
    "data_train_label       = np.array(DATA['Train_Label'])\n",
    "data_train_split       = DATA['Train_Word_Split']\n",
    "data_train_list_index  = DATA['Train_Word2Index']\n",
    "data_test_list         = DATA['Test_Origin']\n",
    "data_test_label        = np.array(DATA['Test_Label'])\n",
    "data_test_split        = DATA['Test_Word_Split']\n",
    "data_test_list_index   = DATA['Test_Word2Index']\n",
    "data_value             = 25\n",
    "\n",
    "print 'Load data'\n",
    "\n",
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_train_list)): \n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)                                        \n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0) \n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "\n",
    "print 'Preprocess finished'\n",
    "\n",
    "batch_len_tr        = batch_len\n",
    "batch_rows_tr       = batch_rows\n",
    "batch_cols_tr       = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "#======================= Setting =======================#\n",
    "Setting = {}\n",
    "Setting['N_train'] = len(data_train_list) - delete_count\n",
    "Setting['N_test']  = len(data_test_list)\n",
    "# 1-th layer\n",
    "Setting['K1']      = 200\n",
    "Setting['K1_V1']   = DATA['Vab_Size']\n",
    "Setting['K1_V2']   = np.max(batch_len) + 2 # padding\n",
    "Setting['K1_S3']   = DATA['Vab_Size']\n",
    "Setting['K1_S4']   = 3\n",
    "Setting['K1_S1']   = Setting['K1_V1'] + 1 - Setting['K1_S3'] \n",
    "Setting['K1_S2']   = Setting['K1_V2'] + 1 - Setting['K1_S4'] \n",
    "# 2-th layer\n",
    "Setting['K2']         = 100\n",
    "Setting['Iter']       = 200\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "#======================= SuperParams =======================#\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.05 # Phi\n",
    "\n",
    "#======================= Tensorflow Initial =======================#\n",
    "# Initial Graph\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) #HWC\n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print 'Tensorflow initial finished'\n",
    "\n",
    "#====================== CUDA Initial ======================#\n",
    "# Note， do not add any cuda operation among CUDA initial such as Tensorflow!!!!!!!!!!!!!!!!!!\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"CUDA initial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 1.7770409584 seconds\n",
      "epoch 1 takes 1.53521490097 seconds\n",
      "epoch 2 takes 1.53734111786 seconds\n",
      "epoch 3 takes 1.54374313354 seconds\n",
      "epoch 4 takes 1.57491111755 seconds\n",
      "epoch 5 takes 1.54111599922 seconds\n",
      "epoch 6 takes 1.51093482971 seconds\n",
      "epoch 7 takes 1.55684018135 seconds\n",
      "epoch 8 takes 1.61343002319 seconds\n",
      "epoch 9 takes 1.51858615875 seconds\n",
      "epoch 10 takes 1.53158211708 seconds\n",
      "epoch 11 takes 1.48957300186 seconds\n",
      "epoch 12 takes 1.52285003662 seconds\n",
      "epoch 13 takes 1.5439658165 seconds\n",
      "epoch 14 takes 1.49136519432 seconds\n",
      "epoch 15 takes 1.5142519474 seconds\n",
      "epoch 16 takes 1.50900006294 seconds\n",
      "epoch 17 takes 1.53925204277 seconds\n",
      "epoch 18 takes 1.50195717812 seconds\n",
      "epoch 19 takes 1.55368900299 seconds\n",
      "epoch 20 takes 1.5256831646 seconds\n",
      "epoch 21 takes 1.51545000076 seconds\n",
      "epoch 22 takes 1.51876592636 seconds\n",
      "epoch 23 takes 1.52349996567 seconds\n",
      "epoch 24 takes 1.50203800201 seconds\n",
      "epoch 25 takes 1.53326797485 seconds\n",
      "epoch 26 takes 1.51614785194 seconds\n",
      "epoch 27 takes 1.49795794487 seconds\n",
      "epoch 28 takes 1.59873604774 seconds\n",
      "epoch 29 takes 1.66645002365 seconds\n",
      "epoch 30 takes 1.54045081139 seconds\n",
      "epoch 31 takes 1.56190490723 seconds\n",
      "epoch 32 takes 1.54174304008 seconds\n",
      "epoch 33 takes 1.61097502708 seconds\n",
      "epoch 34 takes 1.60385704041 seconds\n",
      "epoch 35 takes 1.65159988403 seconds\n",
      "epoch 36 takes 1.63818597794 seconds\n",
      "epoch 37 takes 1.56953310966 seconds\n",
      "epoch 38 takes 1.52087497711 seconds\n",
      "epoch 39 takes 1.51663589478 seconds\n",
      "epoch 40 takes 1.55143094063 seconds\n",
      "epoch 41 takes 1.54216885567 seconds\n",
      "epoch 42 takes 1.50917196274 seconds\n",
      "epoch 43 takes 1.50865983963 seconds\n",
      "epoch 44 takes 1.52321910858 seconds\n",
      "epoch 45 takes 1.53525114059 seconds\n",
      "epoch 46 takes 1.53291082382 seconds\n",
      "epoch 47 takes 1.70606303215 seconds\n",
      "epoch 48 takes 2.1720252037 seconds\n",
      "epoch 49 takes 2.12672281265 seconds\n",
      "epoch 50 takes 1.77604484558 seconds\n",
      "epoch 51 takes 1.55142593384 seconds\n",
      "epoch 52 takes 1.50907301903 seconds\n",
      "epoch 53 takes 1.64429998398 seconds\n",
      "epoch 54 takes 1.91711592674 seconds\n",
      "epoch 55 takes 1.529047966 seconds\n",
      "epoch 56 takes 1.55514717102 seconds\n",
      "epoch 57 takes 1.88294577599 seconds\n",
      "epoch 58 takes 1.53352499008 seconds\n",
      "epoch 59 takes 1.49965190887 seconds\n",
      "epoch 60 takes 1.84504890442 seconds\n",
      "epoch 61 takes 1.88222002983 seconds\n",
      "epoch 62 takes 1.51464700699 seconds\n",
      "epoch 63 takes 1.5461230278 seconds\n",
      "epoch 64 takes 1.55911922455 seconds\n",
      "epoch 65 takes 1.49730992317 seconds\n",
      "epoch 66 takes 1.52059412003 seconds\n",
      "epoch 67 takes 1.537099123 seconds\n",
      "epoch 68 takes 1.52400398254 seconds\n",
      "epoch 69 takes 1.49688100815 seconds\n",
      "epoch 70 takes 1.57788085938 seconds\n",
      "epoch 71 takes 1.50412583351 seconds\n",
      "epoch 72 takes 1.51601600647 seconds\n",
      "epoch 73 takes 1.52712392807 seconds\n",
      "epoch 74 takes 1.5349919796 seconds\n",
      "epoch 75 takes 1.77648997307 seconds\n",
      "epoch 76 takes 2.07539200783 seconds\n",
      "epoch 77 takes 2.10364508629 seconds\n",
      "epoch 78 takes 1.82559204102 seconds\n",
      "epoch 79 takes 1.48428201675 seconds\n",
      "epoch 80 takes 1.51862692833 seconds\n",
      "epoch 81 takes 1.52298688889 seconds\n",
      "epoch 82 takes 1.54981398582 seconds\n",
      "epoch 83 takes 1.53482699394 seconds\n",
      "epoch 84 takes 1.49548816681 seconds\n",
      "epoch 85 takes 1.73207402229 seconds\n",
      "epoch 86 takes 1.97863197327 seconds\n",
      "epoch 87 takes 1.50927090645 seconds\n",
      "epoch 88 takes 1.51564598083 seconds\n",
      "epoch 89 takes 1.56106996536 seconds\n",
      "epoch 90 takes 1.50601601601 seconds\n",
      "epoch 91 takes 1.52945208549 seconds\n",
      "epoch 92 takes 1.54022097588 seconds\n",
      "epoch 93 takes 1.50469493866 seconds\n",
      "epoch 94 takes 1.5092291832 seconds\n",
      "epoch 95 takes 1.5521531105 seconds\n",
      "epoch 96 takes 1.76048898697 seconds\n",
      "epoch 97 takes 2.14620900154 seconds\n",
      "epoch 98 takes 2.21968483925 seconds\n",
      "epoch 99 takes 1.77293395996 seconds\n",
      "epoch 100 takes 1.54072189331 seconds\n",
      "epoch 101 takes 1.52532505989 seconds\n",
      "epoch 102 takes 1.50364613533 seconds\n",
      "epoch 103 takes 1.54089212418 seconds\n",
      "epoch 104 takes 1.52760004997 seconds\n",
      "epoch 105 takes 1.53461098671 seconds\n",
      "epoch 106 takes 1.54527807236 seconds\n",
      "epoch 107 takes 1.49940609932 seconds\n",
      "epoch 108 takes 1.55798602104 seconds\n",
      "epoch 109 takes 1.5203371048 seconds\n",
      "epoch 110 takes 1.76637196541 seconds\n",
      "epoch 111 takes 2.22122192383 seconds\n",
      "epoch 112 takes 2.2155649662 seconds\n",
      "epoch 113 takes 2.21410489082 seconds\n",
      "epoch 114 takes 2.18725204468 seconds\n",
      "epoch 115 takes 2.19649505615 seconds\n",
      "epoch 116 takes 1.8238260746 seconds\n",
      "epoch 117 takes 1.50704908371 seconds\n",
      "epoch 118 takes 1.53394794464 seconds\n",
      "epoch 119 takes 1.52860593796 seconds\n",
      "epoch 120 takes 1.51337099075 seconds\n",
      "epoch 121 takes 1.49873304367 seconds\n",
      "epoch 122 takes 1.61858892441 seconds\n",
      "epoch 123 takes 1.71131491661 seconds\n",
      "epoch 124 takes 2.18547701836 seconds\n",
      "epoch 125 takes 2.14789104462 seconds\n",
      "epoch 126 takes 2.13156914711 seconds\n",
      "epoch 127 takes 2.15736818314 seconds\n",
      "epoch 128 takes 2.02504301071 seconds\n",
      "epoch 129 takes 1.73768210411 seconds\n",
      "epoch 130 takes 2.12327814102 seconds\n",
      "epoch 131 takes 1.83350396156 seconds\n",
      "epoch 132 takes 1.53336286545 seconds\n",
      "epoch 133 takes 1.52567005157 seconds\n",
      "epoch 134 takes 1.53215289116 seconds\n",
      "epoch 135 takes 1.53305006027 seconds\n",
      "epoch 136 takes 1.55086898804 seconds\n",
      "epoch 137 takes 1.54419207573 seconds\n",
      "epoch 138 takes 1.80663704872 seconds\n",
      "epoch 139 takes 2.15632295609 seconds\n",
      "epoch 140 takes 2.18777108192 seconds\n",
      "epoch 141 takes 2.23056101799 seconds\n",
      "epoch 142 takes 2.17476010323 seconds\n",
      "epoch 143 takes 2.14829802513 seconds\n",
      "epoch 144 takes 1.69266605377 seconds\n",
      "epoch 145 takes 1.51417899132 seconds\n",
      "epoch 146 takes 1.50328993797 seconds\n",
      "epoch 147 takes 1.5500228405 seconds\n",
      "epoch 148 takes 1.49747014046 seconds\n",
      "epoch 149 takes 1.53773283958 seconds\n",
      "epoch 150 takes 1.57315993309 seconds\n",
      "epoch 151 takes 1.56201100349 seconds\n",
      "epoch 152 takes 1.56649279594 seconds\n",
      "epoch 153 takes 1.5524828434 seconds\n",
      "epoch 154 takes 1.56259083748 seconds\n",
      "epoch 155 takes 1.71834111214 seconds\n",
      "epoch 156 takes 1.56413388252 seconds\n",
      "epoch 157 takes 1.52809000015 seconds\n",
      "epoch 158 takes 1.55565595627 seconds\n",
      "epoch 159 takes 1.5643351078 seconds\n",
      "epoch 160 takes 1.57274198532 seconds\n",
      "epoch 161 takes 1.86208105087 seconds\n",
      "epoch 162 takes 2.22048807144 seconds\n",
      "epoch 163 takes 2.23056793213 seconds\n",
      "epoch 164 takes 2.27901291847 seconds\n",
      "epoch 165 takes 2.21722888947 seconds\n",
      "epoch 166 takes 2.2166030407 seconds\n",
      "epoch 167 takes 2.21343898773 seconds\n",
      "epoch 168 takes 2.26990413666 seconds\n",
      "epoch 169 takes 2.21503090858 seconds\n",
      "epoch 170 takes 1.75957393646 seconds\n",
      "epoch 171 takes 1.54746699333 seconds\n",
      "epoch 172 takes 1.58620095253 seconds\n",
      "epoch 173 takes 1.55560898781 seconds\n",
      "epoch 174 takes 1.57424902916 seconds\n",
      "epoch 175 takes 1.54933714867 seconds\n",
      "epoch 176 takes 1.58936595917 seconds\n",
      "epoch 177 takes 1.54501700401 seconds\n",
      "epoch 178 takes 1.53653407097 seconds\n",
      "epoch 179 takes 1.5527009964 seconds\n",
      "epoch 180 takes 1.55480289459 seconds\n",
      "epoch 181 takes 1.93858408928 seconds\n",
      "epoch 182 takes 2.2080950737 seconds\n",
      "epoch 183 takes 2.24874401093 seconds\n",
      "epoch 184 takes 2.26058411598 seconds\n",
      "epoch 185 takes 2.21964597702 seconds\n",
      "epoch 186 takes 2.2572388649 seconds\n",
      "epoch 187 takes 2.23015117645 seconds\n",
      "epoch 188 takes 2.22553610802 seconds\n",
      "epoch 189 takes 2.03096389771 seconds\n",
      "epoch 190 takes 1.549284935 seconds\n",
      "epoch 191 takes 1.55154299736 seconds\n",
      "epoch 192 takes 1.54164290428 seconds\n",
      "epoch 193 takes 1.58562207222 seconds\n",
      "epoch 194 takes 1.55923891068 seconds\n",
      "epoch 195 takes 1.56323504448 seconds\n",
      "epoch 196 takes 1.57190704346 seconds\n",
      "epoch 197 takes 1.56015396118 seconds\n",
      "epoch 198 takes 1.54022717476 seconds\n",
      "epoch 199 takes 1.55438280106 seconds\n",
      "Train phase finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Initial Params =======================#\n",
    "import PGBN_sampler \n",
    "from scipy.special import gamma\n",
    "Params = {}\n",
    "\n",
    "# 1-th layer\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1']         = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "# 2-th layer\n",
    "Params['Phi_2']  = 0.2 + 0.8*np.random.rand(Setting['K1'], Setting['K2'])\n",
    "Params['Phi_2']  = Params['Phi_2'] / np.sum(Params['Phi_2'], axis=0)\n",
    "Params['Theta_2']= np.random.rand(Setting['N_train'], Setting['K2'])\n",
    "Params['c3_n']   = 1 * np.ones([Setting['N_train']])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K2'], 1]) / Setting['K2']\n",
    "\n",
    "# Collection\n",
    "W_train_1 = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "W_train_2 = np.zeros([Setting['N_train'], Setting['K2']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "import time\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "\n",
    "#========================== Gibbs ==========================#\n",
    "for t in range(Setting['Iter']):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================#\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])     # Augmentation on D \n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1'])    # Augmentation on w\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32') \n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1  # padding \n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "    \n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "    \n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64')                       # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')                        # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3),axis=2) # N*K1\n",
    "    \n",
    "    #========================== 2nd layer Augmentation ==========================#\n",
    "    M1_tmp = np.array(np.transpose(np.round(Params['W1_nk1_Aug_Pooling'])), dtype='float64', order='C')\n",
    "    Theta2_tmp = np.array(np.transpose(Params['Theta_2']), dtype='float64', order='C')\n",
    "    Xt_to_t1_2,WSZS_2 = PGBN_sampler.Crt_Multirnd_Matrix(M1_tmp, Params['Phi_2'], Theta2_tmp)\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update D,Phi\n",
    "    for k1 in range(Setting['K1']):\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] \n",
    "        X_k1_34_tmp = np.random.gamma(X_k1_34 + SuperParams['eta'])\n",
    "        D1_k1_s     = X_k1_34_tmp / np.sum(X_k1_34_tmp)\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "        \n",
    "    Phi_2_tmp       = np.random.gamma(WSZS_2 + SuperParams['eta'])\n",
    "    Params['Phi_2'] = Phi_2_tmp / np.sum(Phi_2_tmp, axis=0)\n",
    "\n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_2'], Params['Theta_2'].T),0)) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    Params['c3_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c3_n']     = Params['c3_n'] / (SuperParams['f0'] + np.sum(Params['Theta_2'],axis=1))   \n",
    "    \n",
    "    # Update w_j\n",
    "    W_k2_sn = np.random.gamma(Params['Gamma'] + Xt_to_t1_2) / (-np.log(1-Params['p2_n']) + Params['c3_n']) # V*N\n",
    "    Params['Theta_2'] = np.transpose(W_k2_sn)\n",
    "    \n",
    "    shape1 = np.dot(Params['Phi_2'], Params['Theta_2'].T) # V*N\n",
    "    W_k1_sn = np.random.gamma(shape1 + Params['W1_nk1_Aug_Pooling'].T ) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn) \n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "    \n",
    "    if t >=Setting['Burinin']:\n",
    "        W_train_1 = W_train_1 + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])\n",
    "        W_train_2 = W_train_2 + Params['Theta_2']\n",
    "            \n",
    "    end_time = time.time()\n",
    "    \n",
    "    if t == 0:\n",
    "        Iter_time.append(end_time - start_time)\n",
    "    else:\n",
    "        Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "print \"Train phase finished\"\n",
    "W_train_1 =  W_train_1 / Setting['Collection']  \n",
    "W_train_2 =  W_train_2 / Setting['Collection']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess finished\n",
      "epoch 0 takes 0.139043807983 seconds\n",
      "epoch 1 takes 0.135283946991 seconds\n",
      "epoch 2 takes 0.160463094711 seconds\n",
      "epoch 3 takes 0.143355131149 seconds\n",
      "epoch 4 takes 0.131431102753 seconds\n",
      "epoch 5 takes 0.144780158997 seconds\n",
      "epoch 6 takes 0.133867025375 seconds\n",
      "epoch 7 takes 0.136509895325 seconds\n",
      "epoch 8 takes 0.134027957916 seconds\n",
      "epoch 9 takes 0.139613866806 seconds\n",
      "epoch 10 takes 0.132142066956 seconds\n",
      "epoch 11 takes 0.137145042419 seconds\n",
      "epoch 12 takes 0.134495019913 seconds\n",
      "epoch 13 takes 0.139231920242 seconds\n",
      "epoch 14 takes 0.132050037384 seconds\n",
      "epoch 15 takes 0.135944843292 seconds\n",
      "epoch 16 takes 0.137104988098 seconds\n",
      "epoch 17 takes 0.144098997116 seconds\n",
      "epoch 18 takes 0.13571190834 seconds\n",
      "epoch 19 takes 0.141473054886 seconds\n",
      "epoch 20 takes 0.140522003174 seconds\n",
      "epoch 21 takes 0.148288965225 seconds\n",
      "epoch 22 takes 0.149328947067 seconds\n",
      "epoch 23 takes 0.150618076324 seconds\n",
      "epoch 24 takes 0.151479005814 seconds\n",
      "epoch 25 takes 0.134768009186 seconds\n",
      "epoch 26 takes 0.146973848343 seconds\n",
      "epoch 27 takes 0.136189937592 seconds\n",
      "epoch 28 takes 0.136675834656 seconds\n",
      "epoch 29 takes 0.142297029495 seconds\n",
      "epoch 30 takes 0.145925998688 seconds\n",
      "epoch 31 takes 0.146642923355 seconds\n",
      "epoch 32 takes 0.136882066727 seconds\n",
      "epoch 33 takes 0.148061990738 seconds\n",
      "epoch 34 takes 0.147093057632 seconds\n",
      "epoch 35 takes 0.15031504631 seconds\n",
      "epoch 36 takes 0.143349885941 seconds\n",
      "epoch 37 takes 0.139646053314 seconds\n",
      "epoch 38 takes 0.135886907578 seconds\n",
      "epoch 39 takes 0.147449016571 seconds\n",
      "epoch 40 takes 0.135389089584 seconds\n",
      "epoch 41 takes 0.138039112091 seconds\n",
      "epoch 42 takes 0.133702993393 seconds\n",
      "epoch 43 takes 0.139530897141 seconds\n",
      "epoch 44 takes 0.136174917221 seconds\n",
      "epoch 45 takes 0.137012004852 seconds\n",
      "epoch 46 takes 0.132440090179 seconds\n",
      "epoch 47 takes 0.141442060471 seconds\n",
      "epoch 48 takes 0.135301113129 seconds\n",
      "epoch 49 takes 0.138565063477 seconds\n",
      "epoch 50 takes 0.136450052261 seconds\n",
      "epoch 51 takes 0.136213064194 seconds\n",
      "epoch 52 takes 0.131958007812 seconds\n",
      "epoch 53 takes 0.137207984924 seconds\n",
      "epoch 54 takes 0.137565135956 seconds\n",
      "epoch 55 takes 0.141397953033 seconds\n",
      "epoch 56 takes 0.144571065903 seconds\n",
      "epoch 57 takes 0.14647102356 seconds\n",
      "epoch 58 takes 0.143787145615 seconds\n",
      "epoch 59 takes 0.146031141281 seconds\n",
      "epoch 60 takes 0.133702039719 seconds\n",
      "epoch 61 takes 0.139033079147 seconds\n",
      "epoch 62 takes 0.130595207214 seconds\n",
      "epoch 63 takes 0.14682507515 seconds\n",
      "epoch 64 takes 0.132265090942 seconds\n",
      "epoch 65 takes 0.140590906143 seconds\n",
      "epoch 66 takes 0.132859945297 seconds\n",
      "epoch 67 takes 0.14201593399 seconds\n",
      "epoch 68 takes 0.137528896332 seconds\n",
      "epoch 69 takes 0.141391992569 seconds\n",
      "epoch 70 takes 0.133002996445 seconds\n",
      "epoch 71 takes 0.141458034515 seconds\n",
      "epoch 72 takes 0.132252931595 seconds\n",
      "epoch 73 takes 0.136908054352 seconds\n",
      "epoch 74 takes 0.131149053574 seconds\n",
      "epoch 75 takes 0.143634080887 seconds\n",
      "epoch 76 takes 0.131310939789 seconds\n",
      "epoch 77 takes 0.135867118835 seconds\n",
      "epoch 78 takes 0.135329961777 seconds\n",
      "epoch 79 takes 0.14653301239 seconds\n",
      "epoch 80 takes 0.13418006897 seconds\n",
      "epoch 81 takes 0.14055109024 seconds\n",
      "epoch 82 takes 0.129723787308 seconds\n",
      "epoch 83 takes 0.144516944885 seconds\n",
      "epoch 84 takes 0.131049156189 seconds\n",
      "epoch 85 takes 0.142143964767 seconds\n",
      "epoch 86 takes 0.131862163544 seconds\n",
      "epoch 87 takes 0.141961812973 seconds\n",
      "epoch 88 takes 0.130947113037 seconds\n",
      "epoch 89 takes 0.140715122223 seconds\n",
      "epoch 90 takes 0.131490945816 seconds\n",
      "epoch 91 takes 0.144293069839 seconds\n",
      "epoch 92 takes 0.131196975708 seconds\n",
      "epoch 93 takes 0.143873929977 seconds\n",
      "epoch 94 takes 0.13197016716 seconds\n",
      "epoch 95 takes 0.141831874847 seconds\n",
      "epoch 96 takes 0.131540060043 seconds\n",
      "epoch 97 takes 0.139806032181 seconds\n",
      "epoch 98 takes 0.134747982025 seconds\n",
      "epoch 99 takes 0.145556926727 seconds\n",
      "epoch 100 takes 0.135106086731 seconds\n",
      "epoch 101 takes 0.140763998032 seconds\n",
      "epoch 102 takes 0.130344152451 seconds\n",
      "epoch 103 takes 0.137856960297 seconds\n",
      "epoch 104 takes 0.132558822632 seconds\n",
      "epoch 105 takes 0.137398958206 seconds\n",
      "epoch 106 takes 0.135971069336 seconds\n",
      "epoch 107 takes 0.13850903511 seconds\n",
      "epoch 108 takes 0.130258083344 seconds\n",
      "epoch 109 takes 0.136512041092 seconds\n",
      "epoch 110 takes 0.133593082428 seconds\n",
      "epoch 111 takes 0.137414932251 seconds\n",
      "epoch 112 takes 0.153875827789 seconds\n",
      "epoch 113 takes 0.135776996613 seconds\n",
      "epoch 114 takes 0.131370067596 seconds\n",
      "epoch 115 takes 0.149009943008 seconds\n",
      "epoch 116 takes 0.132905960083 seconds\n",
      "epoch 117 takes 0.139302015305 seconds\n",
      "epoch 118 takes 0.132460832596 seconds\n",
      "epoch 119 takes 0.134162902832 seconds\n",
      "epoch 120 takes 0.131748914719 seconds\n",
      "epoch 121 takes 0.144958019257 seconds\n",
      "epoch 122 takes 0.135898113251 seconds\n",
      "epoch 123 takes 0.144812107086 seconds\n",
      "epoch 124 takes 0.132761001587 seconds\n",
      "epoch 125 takes 0.138765096664 seconds\n",
      "epoch 126 takes 0.131361961365 seconds\n",
      "epoch 127 takes 0.146051168442 seconds\n",
      "epoch 128 takes 0.133313894272 seconds\n",
      "epoch 129 takes 0.141202926636 seconds\n",
      "epoch 130 takes 0.131860971451 seconds\n",
      "epoch 131 takes 0.140584945679 seconds\n",
      "epoch 132 takes 0.131139039993 seconds\n",
      "epoch 133 takes 0.136084794998 seconds\n",
      "epoch 134 takes 0.133524894714 seconds\n",
      "epoch 135 takes 0.13987493515 seconds\n",
      "epoch 136 takes 0.131350040436 seconds\n",
      "epoch 137 takes 0.144384145737 seconds\n",
      "epoch 138 takes 0.133544921875 seconds\n",
      "epoch 139 takes 0.142482995987 seconds\n",
      "epoch 140 takes 0.132289171219 seconds\n",
      "epoch 141 takes 0.143425941467 seconds\n",
      "epoch 142 takes 0.131763935089 seconds\n",
      "epoch 143 takes 0.137049913406 seconds\n",
      "epoch 144 takes 0.130973100662 seconds\n",
      "epoch 145 takes 0.138505220413 seconds\n",
      "epoch 146 takes 0.131704092026 seconds\n",
      "epoch 147 takes 0.13772892952 seconds\n",
      "epoch 148 takes 0.134388923645 seconds\n",
      "epoch 149 takes 0.13907790184 seconds\n",
      "epoch 150 takes 0.136064052582 seconds\n",
      "epoch 151 takes 0.146951913834 seconds\n",
      "epoch 152 takes 0.133787870407 seconds\n",
      "epoch 153 takes 0.142813920975 seconds\n",
      "epoch 154 takes 0.137758016586 seconds\n",
      "epoch 155 takes 0.142212867737 seconds\n",
      "epoch 156 takes 0.133510828018 seconds\n",
      "epoch 157 takes 0.14693903923 seconds\n",
      "epoch 158 takes 0.136358976364 seconds\n",
      "epoch 159 takes 0.144171953201 seconds\n",
      "epoch 160 takes 0.133843898773 seconds\n",
      "epoch 161 takes 0.142533063889 seconds\n",
      "epoch 162 takes 0.138159990311 seconds\n",
      "epoch 163 takes 0.146258115768 seconds\n",
      "epoch 164 takes 0.140636920929 seconds\n",
      "epoch 165 takes 0.142678022385 seconds\n",
      "epoch 166 takes 0.140552043915 seconds\n",
      "epoch 167 takes 0.151883840561 seconds\n",
      "epoch 168 takes 0.149765014648 seconds\n",
      "epoch 169 takes 0.14439201355 seconds\n",
      "epoch 170 takes 0.140654087067 seconds\n",
      "epoch 171 takes 0.146402835846 seconds\n",
      "epoch 172 takes 0.142028093338 seconds\n",
      "epoch 173 takes 0.144551038742 seconds\n",
      "epoch 174 takes 0.140862941742 seconds\n",
      "epoch 175 takes 0.143321037292 seconds\n",
      "epoch 176 takes 0.134559869766 seconds\n",
      "epoch 177 takes 0.145805120468 seconds\n",
      "epoch 178 takes 0.139965057373 seconds\n",
      "epoch 179 takes 0.140084028244 seconds\n",
      "epoch 180 takes 0.133557081223 seconds\n",
      "epoch 181 takes 0.146138191223 seconds\n",
      "epoch 182 takes 0.13347196579 seconds\n",
      "epoch 183 takes 0.146368026733 seconds\n",
      "epoch 184 takes 0.132933139801 seconds\n",
      "epoch 185 takes 0.140006065369 seconds\n",
      "epoch 186 takes 0.134989976883 seconds\n",
      "epoch 187 takes 0.139715909958 seconds\n",
      "epoch 188 takes 0.13602399826 seconds\n",
      "epoch 189 takes 0.143750905991 seconds\n",
      "epoch 190 takes 0.136066913605 seconds\n",
      "epoch 191 takes 0.145287036896 seconds\n",
      "epoch 192 takes 0.131816864014 seconds\n",
      "epoch 193 takes 0.139245986938 seconds\n",
      "epoch 194 takes 0.135711193085 seconds\n",
      "epoch 195 takes 0.148350000381 seconds\n",
      "epoch 196 takes 0.134077072144 seconds\n",
      "epoch 197 takes 0.147802114487 seconds\n",
      "epoch 198 takes 0.136252880096 seconds\n",
      "epoch 199 takes 0.144145965576 seconds\n",
      "Test phase finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_test_list)): \n",
    "    \n",
    "    x_single = np.reshape(data_test_list_index[i], [len(data_test_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_test_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0)\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_test_label[i]])),axis=0)\n",
    "\n",
    "print 'Preprocess finished'\n",
    "\n",
    "batch_len_te        = batch_len\n",
    "batch_rows_te       = batch_rows\n",
    "batch_cols_te       = batch_cols\n",
    "batch_file_index_te = batch_file_index\n",
    "batch_value_te      = batch_value\n",
    "batch_label_te      = batch_label\n",
    "\n",
    "#======================= Initial =======================#\n",
    "Setting['N_test']   = len(batch_len)       \n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_test'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_test']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "Params['Theta_2']= np.random.rand(Setting['N_test'], Setting['K2'])\n",
    "Params['c3_n']   = 1 * np.ones([Setting['N_test']])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K2'], 1]) / Setting['K2']\n",
    "\n",
    "# Collection\n",
    "W_test_1 = np.zeros([Setting['N_test'], Setting['K1']])\n",
    "W_test_2 = np.zeros([Setting['N_test'], Setting['K2']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "#======================= Gibbs =======================#\n",
    "for t in range(Setting['Iter']):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================#\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])  \n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1']) \n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32') \n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "    \n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "    \n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64')                       # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')                        # K1*S3*S4\n",
    "    \n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3),axis=2) # N*K1\n",
    "    \n",
    "    #========================== 2nd layer Augmentation ==========================#\n",
    "    M1_tmp = np.array(np.transpose(np.round(Params['W1_nk1_Aug_Pooling'])), dtype='float64', order='C')\n",
    "    Theta2_tmp = np.array(np.transpose(Params['Theta_2']), dtype='float64', order='C')\n",
    "    Xt_to_t1_2,WSZS_2 = PGBN_sampler.Crt_Multirnd_Matrix(M1_tmp, Params['Phi_2'], Theta2_tmp)\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_2'], Params['Theta_2'].T),0)) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    Params['c3_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c3_n']     = Params['c3_n'] / (SuperParams['f0'] + np.sum(Params['Theta_2'],axis=1))   \n",
    "    \n",
    "    # Update w_j\n",
    "    W_k2_sn = np.random.gamma(Params['Gamma'] + Xt_to_t1_2) / (-np.log(1-Params['p2_n']) + Params['c3_n']) # V*N\n",
    "    Params['Theta_2'] = np.transpose(W_k2_sn)\n",
    "    \n",
    "    shape1 = np.dot(Params['Phi_2'], Params['Theta_2'].T) # V*N\n",
    "    W_k1_sn = np.random.gamma(Params['W1_nk1_Aug_Pooling'].T + shape1) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn) \n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "    \n",
    "    if t >=Setting['Burinin']:\n",
    "        W_test_1 = W_test_1 + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])\n",
    "        W_test_2 = W_test_2 + Params['Theta_2']\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "\n",
    "print \"Test phase finished\"\n",
    "W_test_1 =  W_test_1 / Setting['Collection']   \n",
    "W_test_2 =  W_test_2 / Setting['Collection']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7522010271460015\n",
      "0.752\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#========================== Normlization ==========================#\n",
    "# W_train = (W_train - np.reshape(np.mean(W_train,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_train, axis=0),[1,Setting['K1']])\n",
    "# W_test = (W_test - np.reshape(np.mean(W_test,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_test, axis=0),[1,Setting['K1']])\n",
    "\n",
    "clf = svm.SVC()                    # class\n",
    "clf.fit(W_train_1, batch_label_tr)            # training the svc model \n",
    "\n",
    "print clf.score(W_train_1, batch_label_tr)    # training the svc model \n",
    "print clf.score(W_test_1,  batch_label_te)    # training the svc model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
