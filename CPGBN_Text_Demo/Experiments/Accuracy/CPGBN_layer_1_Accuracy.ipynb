{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "Preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Tensorflow initial finished\n",
      "CUDA initial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.RandomState(1)\n",
    "\n",
    "realmin = 2.2e-10\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "#=============load data==============#\n",
    "import cPickle\n",
    "\n",
    "DATA = cPickle.load(open(\"./TREC.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = DATA['Vocabulary']\n",
    "data_vab_count_list    = DATA['Vab_count']\n",
    "data_vab_length        = DATA['Vab_Size']\n",
    "data_label             = DATA['Label']\n",
    "data_train_list        = DATA['Train_Origin']\n",
    "data_train_label       = np.array(DATA['Train_Label'])\n",
    "data_train_split       = DATA['Train_Word_Split']\n",
    "data_train_list_index  = DATA['Train_Word2Index']\n",
    "data_test_list         = DATA['Test_Origin']\n",
    "data_test_label        = np.array(DATA['Test_Label'])\n",
    "data_test_split        = DATA['Test_Word_Split']\n",
    "data_test_list_index   = DATA['Test_Word2Index']\n",
    "data_value             = 25\n",
    "\n",
    "print 'Load data'\n",
    "\n",
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_train_list)): \n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)                                        \n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0) \n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "\n",
    "print 'Preprocess finished'\n",
    "\n",
    "batch_len_tr        = batch_len\n",
    "batch_rows_tr       = batch_rows\n",
    "batch_cols_tr       = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "#======================= Setting =======================#\n",
    "Setting = {}\n",
    "Setting['N_train']    = len(data_train_list) - delete_count \n",
    "Setting['K1']         = 200\n",
    "Setting['K1_V1']      = DATA['Vab_Size']\n",
    "Setting['K1_V2']      = np.max(batch_len) + 2         # padding             \n",
    "Setting['K1_S3']      = DATA['Vab_Size']\n",
    "Setting['K1_S4']      = 3\n",
    "Setting['K1_S1']      = Setting['K1_V1'] + 1 - Setting['K1_S3']\n",
    "Setting['K1_S2']      = Setting['K1_V2'] + 1 - Setting['K1_S4'] \n",
    "Setting['Iter']       = 200\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "#======================= SuperParams =======================#\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.05 # Phi\n",
    "\n",
    "#======================= Tensorflow Initial =======================#\n",
    "# Initial Graph\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) #HWC\n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print 'Tensorflow initial finished'\n",
    "\n",
    "#====================== CUDA Initial ======================#\n",
    "# Noteï¼Œ do not add any cuda operation among CUDA initial such as Tensorflow!!!!!!!!!!!!!!!!!!\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"CUDA initial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 0.931187152863 seconds\n",
      "epoch 1 takes 0.793798923492 seconds\n",
      "epoch 2 takes 0.75062417984 seconds\n",
      "epoch 3 takes 0.716100931168 seconds\n",
      "epoch 4 takes 0.727140903473 seconds\n",
      "epoch 5 takes 0.755187034607 seconds\n",
      "epoch 6 takes 0.844794034958 seconds\n",
      "epoch 7 takes 0.708340883255 seconds\n",
      "epoch 8 takes 0.754155158997 seconds\n",
      "epoch 9 takes 0.735522985458 seconds\n",
      "epoch 10 takes 0.736206054688 seconds\n",
      "epoch 11 takes 0.718746900558 seconds\n",
      "epoch 12 takes 0.73920583725 seconds\n",
      "epoch 13 takes 0.740624904633 seconds\n",
      "epoch 14 takes 0.758556842804 seconds\n",
      "epoch 15 takes 0.738423109055 seconds\n",
      "epoch 16 takes 0.714504957199 seconds\n",
      "epoch 17 takes 0.746778964996 seconds\n",
      "epoch 18 takes 0.755737066269 seconds\n",
      "epoch 19 takes 0.729557991028 seconds\n",
      "epoch 20 takes 0.750688076019 seconds\n",
      "epoch 21 takes 0.757378101349 seconds\n",
      "epoch 22 takes 0.71556186676 seconds\n",
      "epoch 23 takes 0.73250412941 seconds\n",
      "epoch 24 takes 0.754863023758 seconds\n",
      "epoch 25 takes 0.747597932816 seconds\n",
      "epoch 26 takes 0.729775190353 seconds\n",
      "epoch 27 takes 0.729033946991 seconds\n",
      "epoch 28 takes 0.721119880676 seconds\n",
      "epoch 29 takes 0.742336988449 seconds\n",
      "epoch 30 takes 0.716873168945 seconds\n",
      "epoch 31 takes 0.714313983917 seconds\n",
      "epoch 32 takes 0.766886949539 seconds\n",
      "epoch 33 takes 0.746111154556 seconds\n",
      "epoch 34 takes 0.728813886642 seconds\n",
      "epoch 35 takes 0.702767133713 seconds\n",
      "epoch 36 takes 0.718864917755 seconds\n",
      "epoch 37 takes 0.728249073029 seconds\n",
      "epoch 38 takes 0.769627094269 seconds\n",
      "epoch 39 takes 0.743962049484 seconds\n",
      "epoch 40 takes 0.731578111649 seconds\n",
      "epoch 41 takes 0.721381902695 seconds\n",
      "epoch 42 takes 0.721824884415 seconds\n",
      "epoch 43 takes 0.726619958878 seconds\n",
      "epoch 44 takes 0.72806596756 seconds\n",
      "epoch 45 takes 0.721936941147 seconds\n",
      "epoch 46 takes 0.729778051376 seconds\n",
      "epoch 47 takes 0.735534906387 seconds\n",
      "epoch 48 takes 0.730834007263 seconds\n",
      "epoch 49 takes 0.737654924393 seconds\n",
      "epoch 50 takes 0.717891216278 seconds\n",
      "epoch 51 takes 0.726283073425 seconds\n",
      "epoch 52 takes 0.734914064407 seconds\n",
      "epoch 53 takes 0.716248989105 seconds\n",
      "epoch 54 takes 0.75096988678 seconds\n",
      "epoch 55 takes 0.721987009048 seconds\n",
      "epoch 56 takes 0.744702100754 seconds\n",
      "epoch 57 takes 0.741307973862 seconds\n",
      "epoch 58 takes 0.733655929565 seconds\n",
      "epoch 59 takes 0.717622041702 seconds\n",
      "epoch 60 takes 0.728516101837 seconds\n",
      "epoch 61 takes 0.737613201141 seconds\n",
      "epoch 62 takes 0.725696086884 seconds\n",
      "epoch 63 takes 0.744879007339 seconds\n",
      "epoch 64 takes 0.737946033478 seconds\n",
      "epoch 65 takes 0.741250991821 seconds\n",
      "epoch 66 takes 0.727794885635 seconds\n",
      "epoch 67 takes 0.719316959381 seconds\n",
      "epoch 68 takes 0.730599880219 seconds\n",
      "epoch 69 takes 0.73699092865 seconds\n",
      "epoch 70 takes 0.736116886139 seconds\n",
      "epoch 71 takes 0.729367017746 seconds\n",
      "epoch 72 takes 0.742730855942 seconds\n",
      "epoch 73 takes 0.731320142746 seconds\n",
      "epoch 74 takes 0.758944034576 seconds\n",
      "epoch 75 takes 0.72872209549 seconds\n",
      "epoch 76 takes 0.74440908432 seconds\n",
      "epoch 77 takes 0.713909864426 seconds\n",
      "epoch 78 takes 0.745337963104 seconds\n",
      "epoch 79 takes 0.728981018066 seconds\n",
      "epoch 80 takes 0.747098922729 seconds\n",
      "epoch 81 takes 0.738092184067 seconds\n",
      "epoch 82 takes 0.741465091705 seconds\n",
      "epoch 83 takes 0.741716861725 seconds\n",
      "epoch 84 takes 0.761364936829 seconds\n",
      "epoch 85 takes 0.714740037918 seconds\n",
      "epoch 86 takes 0.735754966736 seconds\n",
      "epoch 87 takes 0.738794088364 seconds\n",
      "epoch 88 takes 0.719830989838 seconds\n",
      "epoch 89 takes 0.739362001419 seconds\n",
      "epoch 90 takes 0.75164604187 seconds\n",
      "epoch 91 takes 0.805095911026 seconds\n",
      "epoch 92 takes 0.726367950439 seconds\n",
      "epoch 93 takes 0.737163066864 seconds\n",
      "epoch 94 takes 0.733975172043 seconds\n",
      "epoch 95 takes 0.727303028107 seconds\n",
      "epoch 96 takes 0.704108953476 seconds\n",
      "epoch 97 takes 0.739732027054 seconds\n",
      "epoch 98 takes 0.726655960083 seconds\n",
      "epoch 99 takes 0.740191221237 seconds\n",
      "epoch 100 takes 0.729490041733 seconds\n",
      "epoch 101 takes 0.743235111237 seconds\n",
      "epoch 102 takes 0.744735002518 seconds\n",
      "epoch 103 takes 0.735100030899 seconds\n",
      "epoch 104 takes 0.739044904709 seconds\n",
      "epoch 105 takes 0.732585906982 seconds\n",
      "epoch 106 takes 0.743319988251 seconds\n",
      "epoch 107 takes 0.733445882797 seconds\n",
      "epoch 108 takes 0.751161813736 seconds\n",
      "epoch 109 takes 0.734335184097 seconds\n",
      "epoch 110 takes 0.72895693779 seconds\n",
      "epoch 111 takes 0.729624986649 seconds\n",
      "epoch 112 takes 0.732241868973 seconds\n",
      "epoch 113 takes 0.745244026184 seconds\n",
      "epoch 114 takes 0.730419874191 seconds\n",
      "epoch 115 takes 0.725598812103 seconds\n",
      "epoch 116 takes 0.722923994064 seconds\n",
      "epoch 117 takes 0.722471952438 seconds\n",
      "epoch 118 takes 0.745646953583 seconds\n",
      "epoch 119 takes 0.728801965714 seconds\n",
      "epoch 120 takes 0.748394966125 seconds\n",
      "epoch 121 takes 0.720803022385 seconds\n",
      "epoch 122 takes 0.73091506958 seconds\n",
      "epoch 123 takes 0.734257221222 seconds\n",
      "epoch 124 takes 0.734919786453 seconds\n",
      "epoch 125 takes 0.734068870544 seconds\n",
      "epoch 126 takes 0.729089021683 seconds\n",
      "epoch 127 takes 0.741271972656 seconds\n",
      "epoch 128 takes 0.744792938232 seconds\n",
      "epoch 129 takes 0.735155105591 seconds\n",
      "epoch 130 takes 0.727530002594 seconds\n",
      "epoch 131 takes 0.779942989349 seconds\n",
      "epoch 132 takes 0.712013959885 seconds\n",
      "epoch 133 takes 0.73847413063 seconds\n",
      "epoch 134 takes 0.752392053604 seconds\n",
      "epoch 135 takes 0.734249830246 seconds\n",
      "epoch 136 takes 0.748323917389 seconds\n",
      "epoch 137 takes 0.734657049179 seconds\n",
      "epoch 138 takes 0.715080976486 seconds\n",
      "epoch 139 takes 0.755755901337 seconds\n",
      "epoch 140 takes 0.742707967758 seconds\n",
      "epoch 141 takes 0.730659008026 seconds\n",
      "epoch 142 takes 0.727514982224 seconds\n",
      "epoch 143 takes 0.769633054733 seconds\n",
      "epoch 144 takes 0.737670898438 seconds\n",
      "epoch 145 takes 0.734051942825 seconds\n",
      "epoch 146 takes 0.731400966644 seconds\n",
      "epoch 147 takes 0.737275123596 seconds\n",
      "epoch 148 takes 0.729595899582 seconds\n",
      "epoch 149 takes 0.720591068268 seconds\n",
      "epoch 150 takes 0.731266021729 seconds\n",
      "epoch 151 takes 0.769771099091 seconds\n",
      "epoch 152 takes 0.771455049515 seconds\n",
      "epoch 153 takes 0.768734931946 seconds\n",
      "epoch 154 takes 0.767693996429 seconds\n",
      "epoch 155 takes 0.759958982468 seconds\n",
      "epoch 156 takes 0.745533943176 seconds\n",
      "epoch 157 takes 0.767201900482 seconds\n",
      "epoch 158 takes 0.746932983398 seconds\n",
      "epoch 159 takes 0.763321876526 seconds\n",
      "epoch 160 takes 0.769653081894 seconds\n",
      "epoch 161 takes 0.765587091446 seconds\n",
      "epoch 162 takes 0.778543949127 seconds\n",
      "epoch 163 takes 0.76748418808 seconds\n",
      "epoch 164 takes 0.766057014465 seconds\n",
      "epoch 165 takes 0.754557132721 seconds\n",
      "epoch 166 takes 0.760416030884 seconds\n",
      "epoch 167 takes 0.770045042038 seconds\n",
      "epoch 168 takes 0.756891012192 seconds\n",
      "epoch 169 takes 0.762638092041 seconds\n",
      "epoch 170 takes 0.771320819855 seconds\n",
      "epoch 171 takes 0.752369880676 seconds\n",
      "epoch 172 takes 0.801008939743 seconds\n",
      "epoch 173 takes 0.762953996658 seconds\n",
      "epoch 174 takes 0.761839866638 seconds\n",
      "epoch 175 takes 0.783905029297 seconds\n",
      "epoch 176 takes 0.781357049942 seconds\n",
      "epoch 177 takes 0.770159959793 seconds\n",
      "epoch 178 takes 0.798629999161 seconds\n",
      "epoch 179 takes 0.766230106354 seconds\n",
      "epoch 180 takes 0.751111030579 seconds\n",
      "epoch 181 takes 0.777757883072 seconds\n",
      "epoch 182 takes 0.774387836456 seconds\n",
      "epoch 183 takes 0.769501209259 seconds\n",
      "epoch 184 takes 0.779748916626 seconds\n",
      "epoch 185 takes 0.7748939991 seconds\n",
      "epoch 186 takes 0.772572994232 seconds\n",
      "epoch 187 takes 0.773334026337 seconds\n",
      "epoch 188 takes 0.783207893372 seconds\n",
      "epoch 189 takes 0.769309997559 seconds\n",
      "epoch 190 takes 0.766860961914 seconds\n",
      "epoch 191 takes 0.767884016037 seconds\n",
      "epoch 192 takes 0.75421500206 seconds\n",
      "epoch 193 takes 0.741412878036 seconds\n",
      "epoch 194 takes 0.751097917557 seconds\n",
      "epoch 195 takes 0.766618013382 seconds\n",
      "epoch 196 takes 0.738148927689 seconds\n",
      "epoch 197 takes 0.778587818146 seconds\n",
      "epoch 198 takes 0.836474180222 seconds\n",
      "epoch 199 takes 0.774911880493 seconds\n",
      "Train phase finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Initial Params =======================#\n",
    "import PGBN_sampler \n",
    "from scipy.special import gamma\n",
    "Params = {}\n",
    "\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K1'], 1]) / Setting['K1']\n",
    "\n",
    "# Collection\n",
    "W_train = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "import time\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "\n",
    "#========================== Gibbs ==========================#\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================#\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])     # Augmentation on D\n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1'])    # Augmentation on w\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32')\n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1  # padding\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "\n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape, dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # ä¸€èˆ¬æœ€å¤š512ä¸ªå¹¶è¡Œçº¿ç¨‹\n",
    "\n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64')                        # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')                         # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3), axis=2) # N*K1\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update D\n",
    "    for k1 in range(Setting['K1']):\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] \n",
    "        D1_k1_s = (X_k1_34 + SuperParams['eta']) / np.sum(X_k1_34 + SuperParams['eta'])\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "\n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    # Update w_j\n",
    "    W_k1_sn = np.random.gamma(Params['W1_nk1_Aug_Pooling'].T + Params['Gamma']) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn)                                                   # N*K1\n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "    \n",
    "    #====================== Burin + Collection ======================#\n",
    "    if t >=Setting['Burinin']:\n",
    "        W_train = W_train + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])        \n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if t == 0:\n",
    "        Iter_time.append(end_time - start_time)\n",
    "    else:\n",
    "        Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "        \n",
    "print \"Train phase finished\"\n",
    "W_train =  W_train / Setting['Collection'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess finished\n",
      "epoch 0 takes 0.0759859085083 seconds\n",
      "epoch 1 takes 0.0830039978027 seconds\n",
      "epoch 2 takes 0.0847899913788 seconds\n",
      "epoch 3 takes 0.086168050766 seconds\n",
      "epoch 4 takes 0.0706140995026 seconds\n",
      "epoch 5 takes 0.0723879337311 seconds\n",
      "epoch 6 takes 0.077898979187 seconds\n",
      "epoch 7 takes 0.0833988189697 seconds\n",
      "epoch 8 takes 0.0847768783569 seconds\n",
      "epoch 9 takes 0.0692529678345 seconds\n",
      "epoch 10 takes 0.0660288333893 seconds\n",
      "epoch 11 takes 0.0681929588318 seconds\n",
      "epoch 12 takes 0.0725648403168 seconds\n",
      "epoch 13 takes 0.0687310695648 seconds\n",
      "epoch 14 takes 0.0720999240875 seconds\n",
      "epoch 15 takes 0.0713980197906 seconds\n",
      "epoch 16 takes 0.0706660747528 seconds\n",
      "epoch 17 takes 0.0830090045929 seconds\n",
      "epoch 18 takes 0.0799069404602 seconds\n",
      "epoch 19 takes 0.0840330123901 seconds\n",
      "epoch 20 takes 0.0733959674835 seconds\n",
      "epoch 21 takes 0.0784289836884 seconds\n",
      "epoch 22 takes 0.0838840007782 seconds\n",
      "epoch 23 takes 0.0839700698853 seconds\n",
      "epoch 24 takes 0.0691959857941 seconds\n",
      "epoch 25 takes 0.0646471977234 seconds\n",
      "epoch 26 takes 0.063826084137 seconds\n",
      "epoch 27 takes 0.0731410980225 seconds\n",
      "epoch 28 takes 0.0694878101349 seconds\n",
      "epoch 29 takes 0.0680019855499 seconds\n",
      "epoch 30 takes 0.070228099823 seconds\n",
      "epoch 31 takes 0.0717260837555 seconds\n",
      "epoch 32 takes 0.0680129528046 seconds\n",
      "epoch 33 takes 0.0690059661865 seconds\n",
      "epoch 34 takes 0.0720491409302 seconds\n",
      "epoch 35 takes 0.0690989494324 seconds\n",
      "epoch 36 takes 0.0710470676422 seconds\n",
      "epoch 37 takes 0.0729970932007 seconds\n",
      "epoch 38 takes 0.0685820579529 seconds\n",
      "epoch 39 takes 0.0692269802094 seconds\n",
      "epoch 40 takes 0.0828611850739 seconds\n",
      "epoch 41 takes 0.0929698944092 seconds\n",
      "epoch 42 takes 0.086893081665 seconds\n",
      "epoch 43 takes 0.0802171230316 seconds\n",
      "epoch 44 takes 0.0832531452179 seconds\n",
      "epoch 45 takes 0.0836069583893 seconds\n",
      "epoch 46 takes 0.0884389877319 seconds\n",
      "epoch 47 takes 0.0905840396881 seconds\n",
      "epoch 48 takes 0.0822038650513 seconds\n",
      "epoch 49 takes 0.0747921466827 seconds\n",
      "epoch 50 takes 0.0673279762268 seconds\n",
      "epoch 51 takes 0.0734751224518 seconds\n",
      "epoch 52 takes 0.0753538608551 seconds\n",
      "epoch 53 takes 0.0722529888153 seconds\n",
      "epoch 54 takes 0.0687408447266 seconds\n",
      "epoch 55 takes 0.0696978569031 seconds\n",
      "epoch 56 takes 0.069855928421 seconds\n",
      "epoch 57 takes 0.0817449092865 seconds\n",
      "epoch 58 takes 0.0837950706482 seconds\n",
      "epoch 59 takes 0.0826449394226 seconds\n",
      "epoch 60 takes 0.0831208229065 seconds\n",
      "epoch 61 takes 0.0765600204468 seconds\n",
      "epoch 62 takes 0.0703392028809 seconds\n",
      "epoch 63 takes 0.0695099830627 seconds\n",
      "epoch 64 takes 0.0731151103973 seconds\n",
      "epoch 65 takes 0.0686900615692 seconds\n",
      "epoch 66 takes 0.0701010227203 seconds\n",
      "epoch 67 takes 0.0747790336609 seconds\n",
      "epoch 68 takes 0.0671629905701 seconds\n",
      "epoch 69 takes 0.0674369335175 seconds\n",
      "epoch 70 takes 0.0711331367493 seconds\n",
      "epoch 71 takes 0.064327955246 seconds\n",
      "epoch 72 takes 0.0674979686737 seconds\n",
      "epoch 73 takes 0.0720238685608 seconds\n",
      "epoch 74 takes 0.0656938552856 seconds\n",
      "epoch 75 takes 0.0668959617615 seconds\n",
      "epoch 76 takes 0.0726780891418 seconds\n",
      "epoch 77 takes 0.0643670558929 seconds\n",
      "epoch 78 takes 0.0670239925385 seconds\n",
      "epoch 79 takes 0.0727770328522 seconds\n",
      "epoch 80 takes 0.0641860961914 seconds\n",
      "epoch 81 takes 0.0664539337158 seconds\n",
      "epoch 82 takes 0.0741231441498 seconds\n",
      "epoch 83 takes 0.0690379142761 seconds\n",
      "epoch 84 takes 0.0683190822601 seconds\n",
      "epoch 85 takes 0.0713238716125 seconds\n",
      "epoch 86 takes 0.0645608901978 seconds\n",
      "epoch 87 takes 0.0660018920898 seconds\n",
      "epoch 88 takes 0.0710120201111 seconds\n",
      "epoch 89 takes 0.0643351078033 seconds\n",
      "epoch 90 takes 0.0655870437622 seconds\n",
      "epoch 91 takes 0.0704600811005 seconds\n",
      "epoch 92 takes 0.0660321712494 seconds\n",
      "epoch 93 takes 0.0664529800415 seconds\n",
      "epoch 94 takes 0.071249961853 seconds\n",
      "epoch 95 takes 0.0640399456024 seconds\n",
      "epoch 96 takes 0.0753388404846 seconds\n",
      "epoch 97 takes 0.0703978538513 seconds\n",
      "epoch 98 takes 0.0655219554901 seconds\n",
      "epoch 99 takes 0.0664758682251 seconds\n",
      "epoch 100 takes 0.072830915451 seconds\n",
      "epoch 101 takes 0.0670700073242 seconds\n",
      "epoch 102 takes 0.0685238838196 seconds\n",
      "epoch 103 takes 0.0697658061981 seconds\n",
      "epoch 104 takes 0.0676980018616 seconds\n",
      "epoch 105 takes 0.0696818828583 seconds\n",
      "epoch 106 takes 0.0712020397186 seconds\n",
      "epoch 107 takes 0.0645048618317 seconds\n",
      "epoch 108 takes 0.0683410167694 seconds\n",
      "epoch 109 takes 0.0734779834747 seconds\n",
      "epoch 110 takes 0.0657591819763 seconds\n",
      "epoch 111 takes 0.0671370029449 seconds\n",
      "epoch 112 takes 0.0759479999542 seconds\n",
      "epoch 113 takes 0.0667588710785 seconds\n",
      "epoch 114 takes 0.0667629241943 seconds\n",
      "epoch 115 takes 0.0707278251648 seconds\n",
      "epoch 116 takes 0.064267873764 seconds\n",
      "epoch 117 takes 0.0643720626831 seconds\n",
      "epoch 118 takes 0.0685160160065 seconds\n",
      "epoch 119 takes 0.0692760944366 seconds\n",
      "epoch 120 takes 0.0704329013824 seconds\n",
      "epoch 121 takes 0.0720250606537 seconds\n",
      "epoch 122 takes 0.0783269405365 seconds\n",
      "epoch 123 takes 0.0726008415222 seconds\n",
      "epoch 124 takes 0.0698449611664 seconds\n",
      "epoch 125 takes 0.0680191516876 seconds\n",
      "epoch 126 takes 0.0702269077301 seconds\n",
      "epoch 127 takes 0.0720221996307 seconds\n",
      "epoch 128 takes 0.0675010681152 seconds\n",
      "epoch 129 takes 0.0699470043182 seconds\n",
      "epoch 130 takes 0.070858001709 seconds\n",
      "epoch 131 takes 0.0700109004974 seconds\n",
      "epoch 132 takes 0.0838329792023 seconds\n",
      "epoch 133 takes 0.0723059177399 seconds\n",
      "epoch 134 takes 0.0695469379425 seconds\n",
      "epoch 135 takes 0.0710129737854 seconds\n",
      "epoch 136 takes 0.070408821106 seconds\n",
      "epoch 137 takes 0.0676519870758 seconds\n",
      "epoch 138 takes 0.0741560459137 seconds\n",
      "epoch 139 takes 0.0698699951172 seconds\n",
      "epoch 140 takes 0.0645151138306 seconds\n",
      "epoch 141 takes 0.0711758136749 seconds\n",
      "epoch 142 takes 0.0830080509186 seconds\n",
      "epoch 143 takes 0.0827918052673 seconds\n",
      "epoch 144 takes 0.0747129917145 seconds\n",
      "epoch 145 takes 0.0691349506378 seconds\n",
      "epoch 146 takes 0.0688180923462 seconds\n",
      "epoch 147 takes 0.0697338581085 seconds\n",
      "epoch 148 takes 0.0729460716248 seconds\n",
      "epoch 149 takes 0.0678441524506 seconds\n",
      "epoch 150 takes 0.0712728500366 seconds\n",
      "epoch 151 takes 0.076339006424 seconds\n",
      "epoch 152 takes 0.0729100704193 seconds\n",
      "epoch 153 takes 0.0728688240051 seconds\n",
      "epoch 154 takes 0.069925069809 seconds\n",
      "epoch 155 takes 0.067195892334 seconds\n",
      "epoch 156 takes 0.0689918994904 seconds\n",
      "epoch 157 takes 0.0747208595276 seconds\n",
      "epoch 158 takes 0.0674850940704 seconds\n",
      "epoch 159 takes 0.0703210830688 seconds\n",
      "epoch 160 takes 0.0743188858032 seconds\n",
      "epoch 161 takes 0.0671720504761 seconds\n",
      "epoch 162 takes 0.0712449550629 seconds\n",
      "epoch 163 takes 0.0740301609039 seconds\n",
      "epoch 164 takes 0.0680451393127 seconds\n",
      "epoch 165 takes 0.0720160007477 seconds\n",
      "epoch 166 takes 0.073490858078 seconds\n",
      "epoch 167 takes 0.0685949325562 seconds\n",
      "epoch 168 takes 0.0709540843964 seconds\n",
      "epoch 169 takes 0.0775990486145 seconds\n",
      "epoch 170 takes 0.0716338157654 seconds\n",
      "epoch 171 takes 0.072273015976 seconds\n",
      "epoch 172 takes 0.074401140213 seconds\n",
      "epoch 173 takes 0.0714890956879 seconds\n",
      "epoch 174 takes 0.0713551044464 seconds\n",
      "epoch 175 takes 0.0748000144958 seconds\n",
      "epoch 176 takes 0.070063829422 seconds\n",
      "epoch 177 takes 0.0718970298767 seconds\n",
      "epoch 178 takes 0.0742249488831 seconds\n",
      "epoch 179 takes 0.0702469348907 seconds\n",
      "epoch 180 takes 0.0691220760345 seconds\n",
      "epoch 181 takes 0.0726900100708 seconds\n",
      "epoch 182 takes 0.0674328804016 seconds\n",
      "epoch 183 takes 0.0709388256073 seconds\n",
      "epoch 184 takes 0.0738139152527 seconds\n",
      "epoch 185 takes 0.0671792030334 seconds\n",
      "epoch 186 takes 0.0699219703674 seconds\n",
      "epoch 187 takes 0.0733749866486 seconds\n",
      "epoch 188 takes 0.067754983902 seconds\n",
      "epoch 189 takes 0.0728781223297 seconds\n",
      "epoch 190 takes 0.0762159824371 seconds\n",
      "epoch 191 takes 0.0667028427124 seconds\n",
      "epoch 192 takes 0.0715410709381 seconds\n",
      "epoch 193 takes 0.0734899044037 seconds\n",
      "epoch 194 takes 0.0665040016174 seconds\n",
      "epoch 195 takes 0.068902015686 seconds\n",
      "epoch 196 takes 0.0734560489655 seconds\n",
      "epoch 197 takes 0.0671398639679 seconds\n",
      "epoch 198 takes 0.0695581436157 seconds\n",
      "epoch 199 takes 0.0737960338593 seconds\n",
      "Test phase finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_test_list)): \n",
    "    \n",
    "    x_single = np.reshape(data_test_list_index[i], [len(data_test_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value \n",
    "        batch_label      = np.array([data_test_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0)\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value ), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_test_label[i]])),axis=0)\n",
    "\n",
    "batch_len_te        = batch_len\n",
    "batch_rows_te       = batch_rows\n",
    "batch_cols_te       = batch_cols\n",
    "batch_file_index_te = batch_file_index\n",
    "batch_value_te      = batch_value\n",
    "batch_label_te      = batch_label\n",
    "\n",
    "print 'Preprocess finished'\n",
    "\n",
    "#======================= Initial =======================#\n",
    "Setting['N_test']        = len(batch_len)        \n",
    "Params['W1_nk1']         = np.random.rand(Setting['N_test'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_test']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K1'], 1]) / Setting['K1']\n",
    "\n",
    "# Collection\n",
    "W_test = np.zeros([Setting['N_test'], Setting['K1']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "#======================= Gibbs =======================#\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================#\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])  \n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1']) \n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32')\n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "\n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1        = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1         = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug    = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug     = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # ä¸€èˆ¬æœ€å¤š512ä¸ªå¹¶è¡Œçº¿ç¨‹\n",
    "\n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64')                        # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')                         # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3), axis=2) # N*K1\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    # Update w_j\n",
    "    W_k1_sn = np.random.gamma(Params['W1_nk1_Aug_Pooling'].T + Params['Gamma']) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn)  # N*K1\n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "\n",
    "    if t >=Setting['Burinin']:\n",
    "        W_test = W_test + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "print \"Test phase finished\"\n",
    "W_test =  W_test / Setting['Collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7107483492296405\n",
      "0.678\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#========================== Normlization ==========================#\n",
    "# W_train = (W_train - np.reshape(np.mean(W_train,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_train, axis=0),[1,Setting['K1']])\n",
    "# W_test = (W_test - np.reshape(np.mean(W_test,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_test, axis=0),[1,Setting['K1']])\n",
    "\n",
    "\n",
    "clf = svm.SVC()                             # class\n",
    "clf.fit(W_train, batch_label_tr)            # training the svc model \n",
    "\n",
    "print clf.score(W_train, batch_label_tr)    # training the svc model \n",
    "print clf.score(W_test,  batch_label_te)    # training the svc model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
