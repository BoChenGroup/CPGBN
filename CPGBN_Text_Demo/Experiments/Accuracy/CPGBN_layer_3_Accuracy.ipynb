{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "Preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Tensorflow initial finished\n",
      "CUDA initial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.RandomState(1)\n",
    "\n",
    "realmin = 2.2e-10\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "#=============load data==============#\n",
    "import cPickle\n",
    "\n",
    "DATA = cPickle.load(open(\"./TREC.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = DATA['Vocabulary']\n",
    "data_vab_count_list    = DATA['Vab_count']\n",
    "data_vab_length        = DATA['Vab_Size']\n",
    "data_label             = DATA['Label']\n",
    "data_train_list        = DATA['Train_Origin']\n",
    "data_train_label       = np.array(DATA['Train_Label'])\n",
    "data_train_split       = DATA['Train_Word_Split']\n",
    "data_train_list_index  = DATA['Train_Word2Index']\n",
    "data_test_list         = DATA['Test_Origin']\n",
    "data_test_label        = np.array(DATA['Test_Label'])\n",
    "data_test_split        = DATA['Test_Word_Split']\n",
    "data_test_list_index   = DATA['Test_Word2Index']\n",
    "data_value             = 25\n",
    "\n",
    "print 'Load data'\n",
    "\n",
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_train_list)): \n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)                                        \n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0) \n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "\n",
    "print 'Preprocess finished'\n",
    "\n",
    "batch_len_tr        = batch_len\n",
    "batch_rows_tr       = batch_rows\n",
    "batch_cols_tr       = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "#======================= Setting =======================#\n",
    "Setting = {}\n",
    "Setting['N_train'] = len(data_train_list) - delete_count \n",
    "Setting['N_test']  = len(data_test_list)\n",
    "# 1-th layer\n",
    "Setting['K1']      = 200\n",
    "Setting['K1_V1']   = DATA['Vab_Size']\n",
    "Setting['K1_V2']   = np.max(batch_len) + 2  # padding\n",
    "Setting['K1_S3']   = DATA['Vab_Size']\n",
    "Setting['K1_S4']   = 3\n",
    "Setting['K1_S1']   = Setting['K1_V1'] + 1 - Setting['K1_S3']\n",
    "Setting['K1_S2']   = Setting['K1_V2'] + 1 - Setting['K1_S4']\n",
    "# 2-th layer\n",
    "Setting['K2']      = 100\n",
    "Setting['K2_V1']   = Setting['K1_S1']  \n",
    "Setting['K2_V2']   = Setting['K1_S2']  + 2  # padding\n",
    "Setting['K2_S3']   = 1\n",
    "Setting['K2_S4']   = 3\n",
    "Setting['K2_S1']   = Setting['K2_V1'] + 1 - Setting['K2_S3']\n",
    "Setting['K2_S2']   = Setting['K2_V2'] + 1 - Setting['K2_S4']\n",
    "# 3-th layer\n",
    "Setting['K3']      = 50\n",
    "Setting['K3_V1']   = Setting['K2_S1']  \n",
    "Setting['K3_V2']   = Setting['K2_S2']  + 2  # padding\n",
    "Setting['K3_S3']   = 1\n",
    "Setting['K3_S4']   = 3\n",
    "Setting['K3_S1']   = Setting['K3_V1'] + 1 - Setting['K3_S3']\n",
    "Setting['K3_S2']   = Setting['K3_V2'] + 1 - Setting['K3_S4']\n",
    "\n",
    "Setting['Iter']       = 200\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "#======================= SuperParams =======================#\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.05 # Phi\n",
    "\n",
    "#======================= Tensorflow Initial =======================#\n",
    "# Initial Graph\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) #HWC\n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print 'Tensorflow initial finished'\n",
    "\n",
    "#====================== CUDA Initial ======================#\n",
    "# Note， do not add any cuda operation among CUDA initial such as Tensorflow!!!!!!!!!!!!!!!!!!\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"CUDA initial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 1.95489811897 seconds\n",
      "epoch 1 takes 1.80502700806 seconds\n",
      "epoch 2 takes 1.82047605515 seconds\n",
      "epoch 3 takes 1.90739488602 seconds\n",
      "epoch 4 takes 2.15453886986 seconds\n",
      "epoch 5 takes 2.84570717812 seconds\n",
      "epoch 6 takes 2.21640110016 seconds\n",
      "epoch 7 takes 2.14647102356 seconds\n",
      "epoch 8 takes 2.19170999527 seconds\n",
      "epoch 9 takes 2.12259912491 seconds\n",
      "epoch 10 takes 2.37197899818 seconds\n",
      "epoch 11 takes 2.62688994408 seconds\n",
      "epoch 12 takes 2.6535820961 seconds\n",
      "epoch 13 takes 2.1285700798 seconds\n",
      "epoch 14 takes 2.10328292847 seconds\n",
      "epoch 15 takes 1.93394589424 seconds\n",
      "epoch 16 takes 2.00827097893 seconds\n",
      "epoch 17 takes 2.05371999741 seconds\n",
      "epoch 18 takes 2.96312785149 seconds\n",
      "epoch 19 takes 2.74654197693 seconds\n",
      "epoch 20 takes 2.25894784927 seconds\n",
      "epoch 21 takes 1.87960600853 seconds\n",
      "epoch 22 takes 1.91547298431 seconds\n",
      "epoch 23 takes 1.94008779526 seconds\n",
      "epoch 24 takes 1.88085103035 seconds\n",
      "epoch 25 takes 1.91137099266 seconds\n",
      "epoch 26 takes 2.00837993622 seconds\n",
      "epoch 27 takes 2.0022611618 seconds\n",
      "epoch 28 takes 2.44189095497 seconds\n",
      "epoch 29 takes 2.63152122498 seconds\n",
      "epoch 30 takes 2.04924988747 seconds\n",
      "epoch 31 takes 1.82733988762 seconds\n",
      "epoch 32 takes 1.83445596695 seconds\n",
      "epoch 33 takes 1.87221598625 seconds\n",
      "epoch 34 takes 1.83807706833 seconds\n",
      "epoch 35 takes 1.87514400482 seconds\n",
      "epoch 36 takes 1.97117781639 seconds\n",
      "epoch 37 takes 2.62039589882 seconds\n",
      "epoch 38 takes 2.16026306152 seconds\n",
      "epoch 39 takes 1.83807611465 seconds\n",
      "epoch 40 takes 1.77276301384 seconds\n",
      "epoch 41 takes 1.78736019135 seconds\n",
      "epoch 42 takes 1.90083909035 seconds\n",
      "epoch 43 takes 1.80980205536 seconds\n",
      "epoch 44 takes 2.41725492477 seconds\n",
      "epoch 45 takes 2.48070502281 seconds\n",
      "epoch 46 takes 2.0178771019 seconds\n",
      "epoch 47 takes 1.76703095436 seconds\n",
      "epoch 48 takes 1.82303214073 seconds\n",
      "epoch 49 takes 1.77006602287 seconds\n",
      "epoch 50 takes 1.78382301331 seconds\n",
      "epoch 51 takes 1.75486016273 seconds\n",
      "epoch 52 takes 2.42118310928 seconds\n",
      "epoch 53 takes 2.57885789871 seconds\n",
      "epoch 54 takes 2.10664892197 seconds\n",
      "epoch 55 takes 1.91417694092 seconds\n",
      "epoch 56 takes 1.96998214722 seconds\n",
      "epoch 57 takes 1.90863800049 seconds\n",
      "epoch 58 takes 2.00414395332 seconds\n",
      "epoch 59 takes 2.01776790619 seconds\n",
      "epoch 60 takes 2.05932307243 seconds\n",
      "epoch 61 takes 1.92997384071 seconds\n",
      "epoch 62 takes 2.01866793633 seconds\n",
      "epoch 63 takes 2.62242412567 seconds\n",
      "epoch 64 takes 2.59955811501 seconds\n",
      "epoch 65 takes 2.59789013863 seconds\n",
      "epoch 66 takes 2.61335682869 seconds\n",
      "epoch 67 takes 2.16909503937 seconds\n",
      "epoch 68 takes 2.05560588837 seconds\n",
      "epoch 69 takes 2.19517493248 seconds\n",
      "epoch 70 takes 2.03028678894 seconds\n",
      "epoch 71 takes 1.93272399902 seconds\n",
      "epoch 72 takes 2.26224088669 seconds\n",
      "epoch 73 takes 2.01432108879 seconds\n",
      "epoch 74 takes 2.11521792412 seconds\n",
      "epoch 75 takes 1.97964596748 seconds\n",
      "epoch 76 takes 1.95862197876 seconds\n",
      "epoch 77 takes 2.17468905449 seconds\n",
      "epoch 78 takes 1.95859909058 seconds\n",
      "epoch 79 takes 2.20237612724 seconds\n",
      "epoch 80 takes 2.60307693481 seconds\n",
      "epoch 81 takes 2.56676888466 seconds\n",
      "epoch 82 takes 2.62477898598 seconds\n",
      "epoch 83 takes 2.58957695961 seconds\n",
      "epoch 84 takes 2.60123896599 seconds\n",
      "epoch 85 takes 2.61499404907 seconds\n",
      "epoch 86 takes 2.57699608803 seconds\n",
      "epoch 87 takes 2.58520293236 seconds\n",
      "epoch 88 takes 2.56066298485 seconds\n",
      "epoch 89 takes 2.6690568924 seconds\n",
      "epoch 90 takes 2.63947701454 seconds\n",
      "epoch 91 takes 2.593542099 seconds\n",
      "epoch 92 takes 1.94140505791 seconds\n",
      "epoch 93 takes 1.98233699799 seconds\n",
      "epoch 94 takes 1.91521716118 seconds\n",
      "epoch 95 takes 2.13333296776 seconds\n",
      "epoch 96 takes 2.01513290405 seconds\n",
      "epoch 97 takes 1.96637582779 seconds\n",
      "epoch 98 takes 1.9398920536 seconds\n",
      "epoch 99 takes 2.56825900078 seconds\n",
      "epoch 100 takes 2.59343194962 seconds\n",
      "epoch 101 takes 2.3049621582 seconds\n",
      "epoch 102 takes 1.96033096313 seconds\n",
      "epoch 103 takes 2.09776902199 seconds\n",
      "epoch 104 takes 1.94789004326 seconds\n",
      "epoch 105 takes 2.067953825 seconds\n",
      "epoch 106 takes 1.93520092964 seconds\n",
      "epoch 107 takes 1.98866009712 seconds\n",
      "epoch 108 takes 2.09178781509 seconds\n",
      "epoch 109 takes 2.78783893585 seconds\n",
      "epoch 110 takes 2.55679702759 seconds\n",
      "epoch 111 takes 2.75711297989 seconds\n",
      "epoch 112 takes 2.58268618584 seconds\n",
      "epoch 113 takes 2.80466413498 seconds\n",
      "epoch 114 takes 2.54836082458 seconds\n",
      "epoch 115 takes 1.9383020401 seconds\n",
      "epoch 116 takes 1.95439100266 seconds\n",
      "epoch 117 takes 1.99552989006 seconds\n",
      "epoch 118 takes 2.01678085327 seconds\n",
      "epoch 119 takes 2.05180907249 seconds\n",
      "epoch 120 takes 2.63597583771 seconds\n",
      "epoch 121 takes 2.60856699944 seconds\n",
      "epoch 122 takes 2.50635695457 seconds\n",
      "epoch 123 takes 2.51785993576 seconds\n",
      "epoch 124 takes 2.23332500458 seconds\n",
      "epoch 125 takes 1.78672385216 seconds\n",
      "epoch 126 takes 1.79228305817 seconds\n",
      "epoch 127 takes 1.81199383736 seconds\n",
      "epoch 128 takes 1.78650307655 seconds\n",
      "epoch 129 takes 1.78321504593 seconds\n",
      "epoch 130 takes 1.80484700203 seconds\n",
      "epoch 131 takes 1.84648990631 seconds\n",
      "epoch 132 takes 1.97243499756 seconds\n",
      "epoch 133 takes 2.50419020653 seconds\n",
      "epoch 134 takes 2.58966588974 seconds\n",
      "epoch 135 takes 2.54536008835 seconds\n",
      "epoch 136 takes 2.56044602394 seconds\n",
      "epoch 137 takes 1.80422711372 seconds\n",
      "epoch 138 takes 1.78374004364 seconds\n",
      "epoch 139 takes 1.81638288498 seconds\n",
      "epoch 140 takes 1.81875085831 seconds\n",
      "epoch 141 takes 1.78879213333 seconds\n",
      "epoch 142 takes 1.91087198257 seconds\n",
      "epoch 143 takes 2.53369402885 seconds\n",
      "epoch 144 takes 2.5140478611 seconds\n",
      "epoch 145 takes 2.48590803146 seconds\n",
      "epoch 146 takes 2.57193779945 seconds\n",
      "epoch 147 takes 2.50091099739 seconds\n",
      "epoch 148 takes 2.5287668705 seconds\n",
      "epoch 149 takes 2.54107689857 seconds\n",
      "epoch 150 takes 2.54941916466 seconds\n",
      "epoch 151 takes 2.53914380074 seconds\n",
      "epoch 152 takes 1.83919906616 seconds\n",
      "epoch 153 takes 1.80506682396 seconds\n",
      "epoch 154 takes 1.82302498817 seconds\n",
      "epoch 155 takes 1.82202911377 seconds\n",
      "epoch 156 takes 1.77631020546 seconds\n",
      "epoch 157 takes 1.83798098564 seconds\n",
      "epoch 158 takes 1.85724711418 seconds\n",
      "epoch 159 takes 2.01732707024 seconds\n",
      "epoch 160 takes 2.53680109978 seconds\n",
      "epoch 161 takes 2.57710599899 seconds\n",
      "epoch 162 takes 2.53925490379 seconds\n",
      "epoch 163 takes 2.55119895935 seconds\n",
      "epoch 164 takes 1.82136893272 seconds\n",
      "epoch 165 takes 1.81168603897 seconds\n",
      "epoch 166 takes 1.84738612175 seconds\n",
      "epoch 167 takes 1.83548903465 seconds\n",
      "epoch 168 takes 1.83065986633 seconds\n",
      "epoch 169 takes 2.05103206635 seconds\n",
      "epoch 170 takes 2.57911896706 seconds\n",
      "epoch 171 takes 2.57691884041 seconds\n",
      "epoch 172 takes 2.54141306877 seconds\n",
      "epoch 173 takes 2.57104587555 seconds\n",
      "epoch 174 takes 2.5775001049 seconds\n",
      "epoch 175 takes 2.58836102486 seconds\n",
      "epoch 176 takes 2.56641793251 seconds\n",
      "epoch 177 takes 2.29276514053 seconds\n",
      "epoch 178 takes 1.789675951 seconds\n",
      "epoch 179 takes 1.85485100746 seconds\n",
      "epoch 180 takes 1.81550192833 seconds\n",
      "epoch 181 takes 2.58354711533 seconds\n",
      "epoch 182 takes 2.52947688103 seconds\n",
      "epoch 183 takes 2.49124598503 seconds\n",
      "epoch 184 takes 2.32399702072 seconds\n",
      "epoch 185 takes 2.36441397667 seconds\n",
      "epoch 186 takes 2.41706490517 seconds\n",
      "epoch 187 takes 1.92907786369 seconds\n",
      "epoch 188 takes 2.3403198719 seconds\n",
      "epoch 189 takes 2.48706793785 seconds\n",
      "epoch 190 takes 1.99895000458 seconds\n",
      "epoch 191 takes 2.23599100113 seconds\n",
      "epoch 192 takes 2.59080505371 seconds\n",
      "epoch 193 takes 2.03568696976 seconds\n",
      "epoch 194 takes 2.33532500267 seconds\n",
      "epoch 195 takes 2.47064495087 seconds\n",
      "epoch 196 takes 2.12651491165 seconds\n",
      "epoch 197 takes 1.89202713966 seconds\n",
      "epoch 198 takes 1.82787418365 seconds\n",
      "epoch 199 takes 1.84630393982 seconds\n",
      "Train phase finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Initial Params =======================#\n",
    "import PGBN_sampler \n",
    "from scipy.special import gamma\n",
    "Params = {}\n",
    "\n",
    "# 1-th layer\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "# 2-th layer\n",
    "Params['Phi_2']  = 0.2 + 0.8*np.random.rand(Setting['K1'], Setting['K2'])\n",
    "Params['Phi_2']  = Params['Phi_2'] / np.sum(Params['Phi_2'], axis=0)\n",
    "Params['Theta_2']= np.random.rand(Setting['N_train'], Setting['K2'])\n",
    "\n",
    "Params['c3_n']   = 1 * np.ones([Setting['N_train']])\n",
    "tmp = -log_max(1 - Params['p2_n'])\n",
    "Params['p3_n']   = (tmp / (tmp + Params['c3_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "# 3-th layer\n",
    "Params['Phi_3']  = 0.2 + 0.8*np.random.rand(Setting['K2'], Setting['K3'])\n",
    "Params['Phi_3']  = Params['Phi_3'] / np.sum(Params['Phi_3'], axis=0)\n",
    "Params['Theta_3']= np.random.rand(Setting['N_train'], Setting['K3'])\n",
    "\n",
    "Params['c4_n']   = 1 * np.ones([Setting['N_train']])\n",
    "tmp = -log_max(1 - Params['p3_n'])\n",
    "Params['p4_n']   = (tmp / (tmp + Params['c4_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K3'], 1]) / Setting['K3']\n",
    "\n",
    "# Collection\n",
    "W_train_1 = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "W_train_2 = np.zeros([Setting['N_train'], Setting['K2']])\n",
    "W_train_3 = np.zeros([Setting['N_train'], Setting['K3']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "import time\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "\n",
    "#========================== Gibbs ==========================#\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================#\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])    # Augmentation on D \n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1'])   # Augmentation on w\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32') \n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1 # padding\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "    \n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "    \n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64')                       # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')                        # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3),axis=2) # N*K1\n",
    "    \n",
    "    #========================== 2nd layer Augmentation ==========================#\n",
    "    M1_tmp = np.array(np.transpose(np.round(Params['W1_nk1_Aug_Pooling'])), dtype='float64', order='C')\n",
    "    Theta2_tmp = np.array(np.transpose(Params['Theta_2']), dtype='float64', order='C')\n",
    "    Xt_to_t1_2,WSZS_2 = PGBN_sampler.Crt_Multirnd_Matrix(M1_tmp, Params['Phi_2'], Theta2_tmp)\n",
    "    \n",
    "    #========================== 3rd layer Augmentation ==========================#\n",
    "    M2_tmp = np.array(np.round(Xt_to_t1_2), dtype='float64', order='C')\n",
    "    Theta3_tmp = np.array(np.transpose(Params['Theta_3']), dtype='float64', order='C')\n",
    "    Xt_to_t1_3,WSZS_3 = PGBN_sampler.Crt_Multirnd_Matrix(M2_tmp, Params['Phi_3'], Theta3_tmp)\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update D,Phi\n",
    "    for k1 in range(Setting['K1']):\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :]\n",
    "        X_k1_34_tmp = np.random.gamma(X_k1_34 + SuperParams['eta'])\n",
    "        D1_k1_s     = X_k1_34_tmp / np.sum(X_k1_34_tmp)\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "        \n",
    "    Phi_2_tmp       = np.random.gamma(WSZS_2 + SuperParams['eta'])\n",
    "    Params['Phi_2'] = Phi_2_tmp / np.sum(Phi_2_tmp, axis=0)\n",
    "    \n",
    "    Phi_3_tmp       = np.random.gamma(WSZS_3 + SuperParams['eta'])\n",
    "    Params['Phi_3'] = Phi_3_tmp / np.sum(Phi_3_tmp, axis=0)\n",
    "    \n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_2'], Params['Theta_2'].T),0)) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    Params['c3_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_3'], Params['Theta_3'].T),0)) \n",
    "    Params['c3_n']     = Params['c3_n'] / (SuperParams['f0'] + np.sum(Params['Theta_2'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p2_n'])\n",
    "    Params['p3_n']     = tmp / (Params['c3_n'] + tmp)\n",
    "    \n",
    "    Params['c4_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c4_n']     = Params['c4_n'] / (SuperParams['f0'] + np.sum(Params['Theta_3'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p3_n'])\n",
    "    Params['p4_n']     = tmp / (Params['c4_n'] + tmp)\n",
    "    \n",
    "    # Update w_j\n",
    "    W_k3_sn = np.random.gamma(Params['Gamma'] + Xt_to_t1_3) / (-np.log(1-Params['p3_n']) + Params['c4_n']) # V*N\n",
    "    Params['Theta_3'] = np.transpose(W_k3_sn)\n",
    "    \n",
    "    shape2 = np.dot(Params['Phi_3'], Params['Theta_3'].T)\n",
    "    W_k2_sn = np.random.gamma(shape2 + Xt_to_t1_2) / (-np.log(1-Params['p2_n']) + Params['c3_n']) # V*N\n",
    "    Params['Theta_2'] = np.transpose(W_k2_sn)\n",
    "    \n",
    "    shape1 = np.dot(Params['Phi_2'], Params['Theta_2'].T)                                         # V*N\n",
    "    W_k1_sn = np.random.gamma(shape1 + Params['W1_nk1_Aug_Pooling'].T ) / (1 + Params['c2_n'])    # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn) \n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "\n",
    "    if t >=Setting['Burinin']:\n",
    "        W_train_1 = W_train_1 + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])\n",
    "        W_train_2 = W_train_2 + Params['Theta_2']\n",
    "        W_train_3 = W_train_3 + Params['Theta_3']\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    if t == 0:\n",
    "        Iter_time.append(end_time - start_time)\n",
    "    else:\n",
    "        Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "print \"Train phase finished\"\n",
    "W_train_1 =  W_train_1 / Setting['Collection']  \n",
    "W_train_2 =  W_train_2 / Setting['Collection']  \n",
    "W_train_3 =  W_train_3 / Setting['Collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess finished\n",
      "epoch 0 takes 0.158257007599 seconds\n",
      "epoch 1 takes 0.188157081604 seconds\n",
      "epoch 2 takes 0.147967815399 seconds\n",
      "epoch 3 takes 0.158229112625 seconds\n",
      "epoch 4 takes 0.150269031525 seconds\n",
      "epoch 5 takes 0.164437055588 seconds\n",
      "epoch 6 takes 0.159186840057 seconds\n",
      "epoch 7 takes 0.167989015579 seconds\n",
      "epoch 8 takes 0.151693105698 seconds\n",
      "epoch 9 takes 0.159118890762 seconds\n",
      "epoch 10 takes 0.152052879333 seconds\n",
      "epoch 11 takes 0.156960010529 seconds\n",
      "epoch 12 takes 0.166146993637 seconds\n",
      "epoch 13 takes 0.156549930573 seconds\n",
      "epoch 14 takes 0.157428979874 seconds\n",
      "epoch 15 takes 0.166656017303 seconds\n",
      "epoch 16 takes 0.165182113647 seconds\n",
      "epoch 17 takes 0.163312196732 seconds\n",
      "epoch 18 takes 0.169310092926 seconds\n",
      "epoch 19 takes 0.170492172241 seconds\n",
      "epoch 20 takes 0.161624193192 seconds\n",
      "epoch 21 takes 0.157107114792 seconds\n",
      "epoch 22 takes 0.155743122101 seconds\n",
      "epoch 23 takes 0.155972957611 seconds\n",
      "epoch 24 takes 0.157595157623 seconds\n",
      "epoch 25 takes 0.157824039459 seconds\n",
      "epoch 26 takes 0.153692960739 seconds\n",
      "epoch 27 takes 0.160506010056 seconds\n",
      "epoch 28 takes 0.158797979355 seconds\n",
      "epoch 29 takes 0.15580701828 seconds\n",
      "epoch 30 takes 0.155314922333 seconds\n",
      "epoch 31 takes 0.161185979843 seconds\n",
      "epoch 32 takes 0.156514883041 seconds\n",
      "epoch 33 takes 0.160141944885 seconds\n",
      "epoch 34 takes 0.162717103958 seconds\n",
      "epoch 35 takes 0.161469936371 seconds\n",
      "epoch 36 takes 0.164845943451 seconds\n",
      "epoch 37 takes 0.186245918274 seconds\n",
      "epoch 38 takes 0.172431945801 seconds\n",
      "epoch 39 takes 0.171867847443 seconds\n",
      "epoch 40 takes 0.165748119354 seconds\n",
      "epoch 41 takes 0.171683073044 seconds\n",
      "epoch 42 takes 0.175370931625 seconds\n",
      "epoch 43 takes 0.167131900787 seconds\n",
      "epoch 44 takes 0.176144838333 seconds\n",
      "epoch 45 takes 0.184912204742 seconds\n",
      "epoch 46 takes 0.174015045166 seconds\n",
      "epoch 47 takes 0.163556098938 seconds\n",
      "epoch 48 takes 0.160295963287 seconds\n",
      "epoch 49 takes 0.16157412529 seconds\n",
      "epoch 50 takes 0.157680988312 seconds\n",
      "epoch 51 takes 0.160737037659 seconds\n",
      "epoch 52 takes 0.15296292305 seconds\n",
      "epoch 53 takes 0.166049957275 seconds\n",
      "epoch 54 takes 0.157922029495 seconds\n",
      "epoch 55 takes 0.159973144531 seconds\n",
      "epoch 56 takes 0.154696941376 seconds\n",
      "epoch 57 takes 0.165275096893 seconds\n",
      "epoch 58 takes 0.152390003204 seconds\n",
      "epoch 59 takes 0.165979146957 seconds\n",
      "epoch 60 takes 0.155211925507 seconds\n",
      "epoch 61 takes 0.210891008377 seconds\n",
      "epoch 62 takes 0.17120218277 seconds\n",
      "epoch 63 takes 0.167927026749 seconds\n",
      "epoch 64 takes 0.172944784164 seconds\n",
      "epoch 65 takes 0.160559892654 seconds\n",
      "epoch 66 takes 0.194380998611 seconds\n",
      "epoch 67 takes 0.177201986313 seconds\n",
      "epoch 68 takes 0.148377180099 seconds\n",
      "epoch 69 takes 0.182085990906 seconds\n",
      "epoch 70 takes 0.158445119858 seconds\n",
      "epoch 71 takes 0.166527986526 seconds\n",
      "epoch 72 takes 0.154801130295 seconds\n",
      "epoch 73 takes 0.16034579277 seconds\n",
      "epoch 74 takes 0.161134004593 seconds\n",
      "epoch 75 takes 0.161386966705 seconds\n",
      "epoch 76 takes 0.177597999573 seconds\n",
      "epoch 77 takes 0.208260059357 seconds\n",
      "epoch 78 takes 0.20085811615 seconds\n",
      "epoch 79 takes 0.171434879303 seconds\n",
      "epoch 80 takes 0.174293994904 seconds\n",
      "epoch 81 takes 0.164800882339 seconds\n",
      "epoch 82 takes 0.199332952499 seconds\n",
      "epoch 83 takes 0.280180931091 seconds\n",
      "epoch 84 takes 0.229542970657 seconds\n",
      "epoch 85 takes 0.233921051025 seconds\n",
      "epoch 86 takes 0.214036941528 seconds\n",
      "epoch 87 takes 0.218820095062 seconds\n",
      "epoch 88 takes 0.217514038086 seconds\n",
      "epoch 89 takes 0.217371940613 seconds\n",
      "epoch 90 takes 0.202993869781 seconds\n",
      "epoch 91 takes 0.212216854095 seconds\n",
      "epoch 92 takes 0.216285943985 seconds\n",
      "epoch 93 takes 0.215306043625 seconds\n",
      "epoch 94 takes 0.209681034088 seconds\n",
      "epoch 95 takes 0.201944112778 seconds\n",
      "epoch 96 takes 0.217108011246 seconds\n",
      "epoch 97 takes 0.197198152542 seconds\n",
      "epoch 98 takes 0.210748910904 seconds\n",
      "epoch 99 takes 0.21417593956 seconds\n",
      "epoch 100 takes 0.210432052612 seconds\n",
      "epoch 101 takes 0.210929870605 seconds\n",
      "epoch 102 takes 0.209956169128 seconds\n",
      "epoch 103 takes 0.189871072769 seconds\n",
      "epoch 104 takes 0.240180969238 seconds\n",
      "epoch 105 takes 0.213388204575 seconds\n",
      "epoch 106 takes 0.215501070023 seconds\n",
      "epoch 107 takes 0.216989040375 seconds\n",
      "epoch 108 takes 0.215981960297 seconds\n",
      "epoch 109 takes 0.210012197495 seconds\n",
      "epoch 110 takes 0.201768875122 seconds\n",
      "epoch 111 takes 0.215373039246 seconds\n",
      "epoch 112 takes 0.213275909424 seconds\n",
      "epoch 113 takes 0.215936899185 seconds\n",
      "epoch 114 takes 0.22604393959 seconds\n",
      "epoch 115 takes 0.208720207214 seconds\n",
      "epoch 116 takes 0.235501050949 seconds\n",
      "epoch 117 takes 0.218436002731 seconds\n",
      "epoch 118 takes 0.217685937881 seconds\n",
      "epoch 119 takes 0.21666097641 seconds\n",
      "epoch 120 takes 0.260678052902 seconds\n",
      "epoch 121 takes 0.243986129761 seconds\n",
      "epoch 122 takes 0.214263916016 seconds\n",
      "epoch 123 takes 0.210331916809 seconds\n",
      "epoch 124 takes 0.217983961105 seconds\n",
      "epoch 125 takes 0.216299057007 seconds\n",
      "epoch 126 takes 0.216797113419 seconds\n",
      "epoch 127 takes 0.216930866241 seconds\n",
      "epoch 128 takes 0.215941905975 seconds\n",
      "epoch 129 takes 0.214780092239 seconds\n",
      "epoch 130 takes 0.21494102478 seconds\n",
      "epoch 131 takes 0.209592819214 seconds\n",
      "epoch 132 takes 0.209156036377 seconds\n",
      "epoch 133 takes 0.212924003601 seconds\n",
      "epoch 134 takes 0.208214998245 seconds\n",
      "epoch 135 takes 0.196010828018 seconds\n",
      "epoch 136 takes 0.210191965103 seconds\n",
      "epoch 137 takes 0.206215858459 seconds\n",
      "epoch 138 takes 0.206367015839 seconds\n",
      "epoch 139 takes 0.228955984116 seconds\n",
      "epoch 140 takes 0.208024978638 seconds\n",
      "epoch 141 takes 0.245223045349 seconds\n",
      "epoch 142 takes 0.197021007538 seconds\n",
      "epoch 143 takes 0.219015836716 seconds\n",
      "epoch 144 takes 0.214794874191 seconds\n",
      "epoch 145 takes 0.213786840439 seconds\n",
      "epoch 146 takes 0.214009046555 seconds\n",
      "epoch 147 takes 0.214236021042 seconds\n",
      "epoch 148 takes 0.217413902283 seconds\n",
      "epoch 149 takes 0.20338010788 seconds\n",
      "epoch 150 takes 0.217504024506 seconds\n",
      "epoch 151 takes 0.216686010361 seconds\n",
      "epoch 152 takes 0.1940741539 seconds\n",
      "epoch 153 takes 0.163736104965 seconds\n",
      "epoch 154 takes 0.158353090286 seconds\n",
      "epoch 155 takes 0.165579080582 seconds\n",
      "epoch 156 takes 0.156856060028 seconds\n",
      "epoch 157 takes 0.166420936584 seconds\n",
      "epoch 158 takes 0.157456159592 seconds\n",
      "epoch 159 takes 0.162504911423 seconds\n",
      "epoch 160 takes 0.163511037827 seconds\n",
      "epoch 161 takes 0.170644044876 seconds\n",
      "epoch 162 takes 0.158617019653 seconds\n",
      "epoch 163 takes 0.161293029785 seconds\n",
      "epoch 164 takes 0.158915996552 seconds\n",
      "epoch 165 takes 0.16509103775 seconds\n",
      "epoch 166 takes 0.160711050034 seconds\n",
      "epoch 167 takes 0.165854930878 seconds\n",
      "epoch 168 takes 0.161489009857 seconds\n",
      "epoch 169 takes 0.166550159454 seconds\n",
      "epoch 170 takes 0.162143945694 seconds\n",
      "epoch 171 takes 0.157090902328 seconds\n",
      "epoch 172 takes 0.166131019592 seconds\n",
      "epoch 173 takes 0.15354180336 seconds\n",
      "epoch 174 takes 0.161417961121 seconds\n",
      "epoch 175 takes 0.164814949036 seconds\n",
      "epoch 176 takes 0.16303896904 seconds\n",
      "epoch 177 takes 0.187582015991 seconds\n",
      "epoch 178 takes 0.162948846817 seconds\n",
      "epoch 179 takes 0.166311979294 seconds\n",
      "epoch 180 takes 0.162616014481 seconds\n",
      "epoch 181 takes 0.167226076126 seconds\n",
      "epoch 182 takes 0.15634894371 seconds\n",
      "epoch 183 takes 0.170292854309 seconds\n",
      "epoch 184 takes 0.162710905075 seconds\n",
      "epoch 185 takes 0.180424928665 seconds\n",
      "epoch 186 takes 0.15917301178 seconds\n",
      "epoch 187 takes 0.167141914368 seconds\n",
      "epoch 188 takes 0.159448862076 seconds\n",
      "epoch 189 takes 0.162906169891 seconds\n",
      "epoch 190 takes 0.163510084152 seconds\n",
      "epoch 191 takes 0.161144018173 seconds\n",
      "epoch 192 takes 0.160567045212 seconds\n",
      "epoch 193 takes 0.160357952118 seconds\n",
      "epoch 194 takes 0.166299104691 seconds\n",
      "epoch 195 takes 0.165990114212 seconds\n",
      "epoch 196 takes 0.162572860718 seconds\n",
      "epoch 197 takes 0.163792133331 seconds\n",
      "epoch 198 takes 0.165328025818 seconds\n",
      "epoch 199 takes 0.161375999451 seconds\n",
      "Test phase finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_test_list)):\n",
    "    \n",
    "    x_single = np.reshape(data_test_list_index[i], [len(data_test_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_test_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0)\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_test_label[i]])),axis=0)\n",
    "        \n",
    "batch_len_te        = batch_len\n",
    "batch_rows_te       = batch_rows\n",
    "batch_cols_te       = batch_cols\n",
    "batch_file_index_te = batch_file_index\n",
    "batch_value_te      = batch_value\n",
    "batch_label_te      = batch_label\n",
    "\n",
    "print 'Preprocess finished'\n",
    "\n",
    "#======================= Initial =======================#\n",
    "# 1-th layer\n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_test'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_test']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "# 2-th layer\n",
    "Params['Theta_2']= np.random.rand(Setting['N_test'], Setting['K2'])\n",
    "\n",
    "Params['c3_n']   = 1 * np.ones([Setting['N_test']])\n",
    "tmp = -log_max(1 - Params['p2_n'])\n",
    "Params['p3_n']   = (tmp / (tmp + Params['c3_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "# 3-th layer\n",
    "Params['Theta_3']= np.random.rand(Setting['N_test'], Setting['K3'])\n",
    "\n",
    "Params['c4_n']   = 1 * np.ones([Setting['N_test']])\n",
    "tmp = -log_max(1 - Params['p3_n'])\n",
    "Params['p4_n']   = (tmp / (tmp + Params['c4_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K3'], 1]) / Setting['K3']\n",
    "\n",
    "# Collection\n",
    "W_test_1 = np.zeros([Setting['N_test'], Setting['K1']])\n",
    "W_test_2 = np.zeros([Setting['N_test'], Setting['K2']])\n",
    "W_test_3 = np.zeros([Setting['N_test'], Setting['K3']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "#======================= Gibbs =======================#\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================#\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])  \n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1']) \n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32') \n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "    \n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "    \n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64')                       # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')                        # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3),axis=2) # N*K1\n",
    "    \n",
    "    #========================== 2nd layer Augmentation ==========================#\n",
    "    M1_tmp = np.array(np.transpose(np.round(Params['W1_nk1_Aug_Pooling'])), dtype='float64', order='C')\n",
    "    Theta2_tmp = np.array(np.transpose(Params['Theta_2']), dtype='float64', order='C')\n",
    "    Xt_to_t1_2,WSZS_2 = PGBN_sampler.Crt_Multirnd_Matrix(M1_tmp, Params['Phi_2'], Theta2_tmp)\n",
    "    \n",
    "    #========================== 3rd layer Augmentation ==========================#\n",
    "    M2_tmp = np.array(np.round(Xt_to_t1_2), dtype='float64', order='C')\n",
    "    Theta3_tmp = np.array(np.transpose(Params['Theta_3']), dtype='float64', order='C')\n",
    "    Xt_to_t1_3,WSZS_3 = PGBN_sampler.Crt_Multirnd_Matrix(M2_tmp, Params['Phi_3'], Theta3_tmp)\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_2'], Params['Theta_2'].T),0)) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    Params['c3_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_3'], Params['Theta_3'].T),0)) \n",
    "    Params['c3_n']     = Params['c3_n'] / (SuperParams['f0'] + np.sum(Params['Theta_2'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p2_n'])\n",
    "    Params['p3_n']     = tmp / (Params['c3_n'] + tmp)\n",
    "    \n",
    "    Params['c4_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c4_n']     = Params['c4_n'] / (SuperParams['f0'] + np.sum(Params['Theta_3'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p3_n'])\n",
    "    Params['p4_n']     = tmp / (Params['c4_n'] + tmp)\n",
    "    \n",
    "    # Update w_j\n",
    "    W_k3_sn = np.random.gamma(Params['Gamma'] + Xt_to_t1_3) / (-np.log(1-Params['p3_n']) + Params['c4_n']) # V*N\n",
    "    Params['Theta_3'] = np.transpose(W_k3_sn)\n",
    "    \n",
    "    shape2 = np.dot(Params['Phi_3'], Params['Theta_3'].T)\n",
    "    W_k2_sn = np.random.gamma(shape2 + Xt_to_t1_2) / (-np.log(1-Params['p2_n']) + Params['c3_n']) # V*N\n",
    "    Params['Theta_2'] = np.transpose(W_k2_sn)\n",
    "    \n",
    "    shape1 = np.dot(Params['Phi_2'], Params['Theta_2'].T) # V*N\n",
    "    W_k1_sn = np.random.gamma(shape1 + Params['W1_nk1_Aug_Pooling'].T ) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn) \n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "\n",
    "    if t >=Setting['Burinin']:\n",
    "        W_test_1 = W_test_1 + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])\n",
    "        W_test_2 = W_test_2 + Params['Theta_2']\n",
    "        W_test_3 = W_test_3 + Params['Theta_3']\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "print \"Test phase finished\"\n",
    "W_test_1 =  W_test_1 / Setting['Collection']  \n",
    "W_test_2 =  W_test_2 / Setting['Collection']  \n",
    "W_test_3 =  W_test_3 / Setting['Collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7586206896551724\n",
      "0.756\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#========================== Normlization ==========================#\n",
    "# W_train_1 = np.concatenate([W_train_1, W_train_2, W_train_3], axis = 1)\n",
    "# W_test_1 = np.concatenate([W_test_1, W_test_2, W_test_3], axis = 1)\n",
    "\n",
    "# W_train = (W_train - np.reshape(np.mean(W_train,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_train, axis=0),[1,Setting['K1']])\n",
    "# W_test = (W_test - np.reshape(np.mean(W_test,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_test, axis=0),[1,Setting['K1']])\n",
    "\n",
    "clf = svm.SVC()                    # class\n",
    "clf.fit(W_train_1, batch_label_tr)            # training the svc model \n",
    "\n",
    "print clf.score(W_train_1, batch_label_tr)    # training the svc model \n",
    "print clf.score(W_test_1,  batch_label_te)    # training the svc model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
