{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "kernel intial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "# load data\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import gamma\n",
    "import PGBN_sampler\n",
    "import time\n",
    "realmin = 2.2e-10\n",
    "np.random.RandomState(1)\n",
    "\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "#=======================Multi Sampler=======================#\n",
    "def MultRnd(value, MultRate):\n",
    "    MultRate = MultRate / np.sum(MultRate)\n",
    "    MultRate_Sum = np.reshape(MultRate, [-1])\n",
    "    N = len(MultRate_Sum)\n",
    "    Amnk = np.zeros([N])\n",
    "\n",
    "    if N == 1:\n",
    "        Amnk = value\n",
    "    else:\n",
    "        for i in range(1, N, 1):  # 从第二个开始\n",
    "            MultRate_Sum[i] = MultRate_Sum[i] + MultRate_Sum[i - 1]\n",
    "\n",
    "        Uni_Rnd = np.random.rand(np.int64(value))  # 0 - 1\n",
    "        flag_new = Uni_Rnd <= MultRate_Sum[0]\n",
    "        Amnk[0] = np.sum(flag_new)\n",
    "\n",
    "        for i in range(1, N, 1):  # 从第二个开始\n",
    "            flag_old = flag_new\n",
    "            flag_new = Uni_Rnd <= MultRate_Sum[i]\n",
    "            Amnk[i] = np.sum(~flag_old & flag_new)\n",
    "\n",
    "        Amnk = np.reshape(Amnk, MultRate.shape)\n",
    "        \n",
    "    return Amnk\n",
    "\n",
    "#=======================Disp Dictionary=======================#\n",
    "def Dis_Dic(D):\n",
    "    [K, K1, K2] = D.shape\n",
    "    w_n = np.ceil(np.sqrt(K))\n",
    "    h_n = np.ceil(K / w_n)\n",
    "    weight = w_n * K2\n",
    "    height = h_n * K1\n",
    "    Dic = np.zeros([np.int32(weight), np.int32(height)])\n",
    "    count = 0\n",
    "    for k1 in range(np.int32(w_n)):\n",
    "        for k2 in range(np.int32(h_n)):\n",
    "            Dic[k1 * K1: (k1 + 1) * K1, k2 * K2: (k2 + 1) * K2] = D[count, :, :]\n",
    "            count += 1\n",
    "            if count == K:\n",
    "                break\n",
    "        if count == K:\n",
    "            break\n",
    "    plt.figure\n",
    "    plt.imshow(Dic)\n",
    "    plt.show()\n",
    "\n",
    "def Conv_Aug(Kernel, Score_Shape):\n",
    "    [K1, K2] = Score_Shape\n",
    "    [K3, K4] = Kernel.shape\n",
    "    V1 = K1 + K3 - 1\n",
    "    V2 = K2 + K4 - 1\n",
    "\n",
    "    # Padding\n",
    "    Kernel_Pad = np.zeros([2 * V1 - K3, 2 * V2 - K4])  ## Pad [V1 - K3, V2 - K4]\n",
    "    Kernel_Pad[V1 - K3: V1, V2 - K4: V2] = Kernel\n",
    "    Kernel_Pad = Kernel_Pad.T\n",
    "    M, N = Kernel_Pad.shape\n",
    "    # Parameters\n",
    "    col_extent = N - K1 + 1\n",
    "    row_extent = M - K2 + 1\n",
    "\n",
    "    # Get Starting block indices\n",
    "    start_idx = np.arange(K2)[:, None] * N + np.arange(K1)\n",
    "    # Get offsetted indices across the height and width of input array\n",
    "    offset_idx = np.arange(row_extent)[:, None] * N + np.arange(col_extent)\n",
    "    # Get all actual indices & index into input array for final output\n",
    "    out = np.take(Kernel_Pad, start_idx.ravel()[:, None] + offset_idx.ravel())\n",
    "\n",
    "    return np.flip(out.T, axis=1)\n",
    "\n",
    "#=======================Load data=======================#\n",
    "import numpy as np\n",
    "import cPickle\n",
    "\n",
    "TREC = cPickle.load(open(\"./TREC_8k.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = TREC['Vocabulary']\n",
    "data_vab_count_list    = TREC['Vab_count']\n",
    "data_vab_length        = TREC['Vab_Size']\n",
    "data_label             = TREC['Label']\n",
    "data_train_list        = TREC['Train_Origin']\n",
    "data_train_label       = np.array(TREC['Train_Label'])\n",
    "data_train_split       = TREC['Train_Word_Split']\n",
    "data_train_list_index  = TREC['Train_Word2Index']\n",
    "data_test_list         = TREC['Test_Origin']\n",
    "data_test_label        = np.array(TREC['Test_Label'])\n",
    "data_test_split        = TREC['Test_Word_Split']\n",
    "data_test_list_index   = TREC['Test_Word2Index']\n",
    "data_value             = 50\n",
    "\n",
    "print 'load data'\n",
    "\n",
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_train_list)): # 所有样本遍历\n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0)\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "        \n",
    "print 'preprocess finished'\n",
    "\n",
    "batch_len_tr        = batch_len\n",
    "batch_rows_tr       = batch_rows\n",
    "batch_cols_tr       = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "#======================= Setting =======================#\n",
    "Setting = {}\n",
    "Setting['N_train'] = len(data_train_list) - delete_count\n",
    "Setting['N_test']  = len(data_test_list)\n",
    "# 1-th layer\n",
    "Setting['K1']      = 200\n",
    "Setting['K1_V1']   = TREC['Vab_Size']\n",
    "Setting['K1_V2']   = np.max(batch_len) + 2  # 所有输入padding + 2!!!!!!!!\n",
    "Setting['K1_S3']   = TREC['Vab_Size']\n",
    "Setting['K1_S4']   = 3\n",
    "Setting['K1_S1']   = Setting['K1_V1'] + 1 - Setting['K1_S3']\n",
    "Setting['K1_S2']   = Setting['K1_V2'] + 1 - Setting['K1_S4']\n",
    "# 2-th layer\n",
    "Setting['K2']      = 100\n",
    "Setting['K2_V1']   = Setting['K1_S1']       # featuremap经过CRT后作为输入\n",
    "Setting['K2_V2']   = Setting['K1_S2']  + 2  # 所有输入padding + 2!!!!!!!!\n",
    "Setting['K2_S3']   = 1\n",
    "Setting['K2_S4']   = 3\n",
    "Setting['K2_S1']   = Setting['K2_V1'] + 1 - Setting['K2_S3']\n",
    "Setting['K2_S2']   = Setting['K2_V2'] + 1 - Setting['K2_S4']\n",
    "# 3-th layer\n",
    "Setting['K3']      = 50\n",
    "Setting['K3_V1']   = Setting['K2_S1']       # featuremap经过CRT后作为输入\n",
    "Setting['K3_V2']   = Setting['K2_S2']  + 2  # 所有输入padding + 2!!!!!!!!\n",
    "Setting['K3_S3']   = 1\n",
    "Setting['K3_S4']   = 3\n",
    "Setting['K3_S1']   = Setting['K3_V1'] + 1 - Setting['K3_S3']\n",
    "Setting['K3_S2']   = Setting['K3_V2'] + 1 - Setting['K3_S4']\n",
    "\n",
    "Setting['Iter']       = 400\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "#======================= SuperParamsSetting =======================#\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.05 # Phi\n",
    "\n",
    "#======================= Initial Graph =======================#\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) #HWC\n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#======================= Initial CUDA =======================#\n",
    "# 注意， cuda初始化中间不要加入cuda的操作,例如tensorflow!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# cuda par\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"kernel intial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 2.27783799171 seconds\n",
      "epoch 1 takes 2.18184494972 seconds\n",
      "epoch 2 takes 2.39402413368 seconds\n",
      "epoch 3 takes 2.35422205925 seconds\n",
      "epoch 4 takes 2.2150080204 seconds\n",
      "epoch 5 takes 2.18559002876 seconds\n",
      "epoch 6 takes 2.14704394341 seconds\n",
      "epoch 7 takes 2.18161988258 seconds\n",
      "epoch 8 takes 2.23967599869 seconds\n",
      "epoch 9 takes 2.24648118019 seconds\n",
      "epoch 10 takes 2.16373801231 seconds\n",
      "epoch 11 takes 2.16654491425 seconds\n",
      "epoch 12 takes 2.13400602341 seconds\n",
      "epoch 13 takes 2.17213392258 seconds\n",
      "epoch 14 takes 2.10739922523 seconds\n",
      "epoch 15 takes 2.12728500366 seconds\n",
      "epoch 16 takes 2.09686207771 seconds\n",
      "epoch 17 takes 2.12622404099 seconds\n",
      "epoch 18 takes 2.11999106407 seconds\n",
      "epoch 19 takes 2.15334486961 seconds\n",
      "epoch 20 takes 2.10908198357 seconds\n",
      "epoch 21 takes 2.1244020462 seconds\n",
      "epoch 22 takes 2.13620090485 seconds\n",
      "epoch 23 takes 2.12416410446 seconds\n",
      "epoch 24 takes 2.15084314346 seconds\n",
      "epoch 25 takes 2.11313796043 seconds\n",
      "epoch 26 takes 2.13891196251 seconds\n",
      "epoch 27 takes 2.14731001854 seconds\n",
      "epoch 28 takes 2.15485405922 seconds\n",
      "epoch 29 takes 2.11875486374 seconds\n",
      "epoch 30 takes 2.17945790291 seconds\n",
      "epoch 31 takes 2.14007997513 seconds\n",
      "epoch 32 takes 2.15281200409 seconds\n",
      "epoch 33 takes 2.16044807434 seconds\n",
      "epoch 34 takes 2.14678907394 seconds\n",
      "epoch 35 takes 2.12381291389 seconds\n",
      "epoch 36 takes 2.15480613708 seconds\n",
      "epoch 37 takes 2.16344499588 seconds\n",
      "epoch 38 takes 2.21856284142 seconds\n",
      "epoch 39 takes 2.347645998 seconds\n",
      "epoch 40 takes 2.58751487732 seconds\n",
      "epoch 41 takes 2.49599194527 seconds\n",
      "epoch 42 takes 2.171626091 seconds\n",
      "epoch 43 takes 2.12414598465 seconds\n",
      "epoch 44 takes 2.17639303207 seconds\n",
      "epoch 45 takes 2.16659402847 seconds\n",
      "epoch 46 takes 2.17346906662 seconds\n",
      "epoch 47 takes 2.29453587532 seconds\n",
      "epoch 48 takes 2.2683160305 seconds\n",
      "epoch 49 takes 2.44010305405 seconds\n",
      "epoch 50 takes 2.19636702538 seconds\n",
      "epoch 51 takes 2.16797685623 seconds\n",
      "epoch 52 takes 2.15413498878 seconds\n",
      "epoch 53 takes 2.1731710434 seconds\n",
      "epoch 54 takes 2.29443788528 seconds\n",
      "epoch 55 takes 2.16504001617 seconds\n",
      "epoch 56 takes 2.18052196503 seconds\n",
      "epoch 57 takes 2.17045402527 seconds\n",
      "epoch 58 takes 2.14781117439 seconds\n",
      "epoch 59 takes 2.15427303314 seconds\n",
      "epoch 60 takes 2.15815496445 seconds\n",
      "epoch 61 takes 2.19408297539 seconds\n",
      "epoch 62 takes 2.13231396675 seconds\n",
      "epoch 63 takes 2.1931040287 seconds\n",
      "epoch 64 takes 2.15646910667 seconds\n",
      "epoch 65 takes 2.16643095016 seconds\n",
      "epoch 66 takes 2.17120599747 seconds\n",
      "epoch 67 takes 2.15758085251 seconds\n",
      "epoch 68 takes 2.19186210632 seconds\n",
      "epoch 69 takes 2.18154907227 seconds\n",
      "epoch 70 takes 2.17399597168 seconds\n",
      "epoch 71 takes 2.18339705467 seconds\n",
      "epoch 72 takes 2.14496588707 seconds\n",
      "epoch 73 takes 2.21401715279 seconds\n",
      "epoch 74 takes 2.14797902107 seconds\n",
      "epoch 75 takes 2.23280405998 seconds\n",
      "epoch 76 takes 2.20600605011 seconds\n",
      "epoch 77 takes 2.14695096016 seconds\n",
      "epoch 78 takes 2.17701792717 seconds\n",
      "epoch 79 takes 2.18051886559 seconds\n",
      "epoch 80 takes 2.17586112022 seconds\n",
      "epoch 81 takes 2.19571685791 seconds\n",
      "epoch 82 takes 2.21848106384 seconds\n",
      "epoch 83 takes 2.19559407234 seconds\n",
      "epoch 84 takes 2.15153312683 seconds\n",
      "epoch 85 takes 2.15448904037 seconds\n",
      "epoch 86 takes 2.14373207092 seconds\n",
      "epoch 87 takes 2.1477560997 seconds\n",
      "epoch 88 takes 2.2499461174 seconds\n",
      "epoch 89 takes 2.19056010246 seconds\n",
      "epoch 90 takes 2.13893008232 seconds\n",
      "epoch 91 takes 2.14821386337 seconds\n",
      "epoch 92 takes 2.17228889465 seconds\n",
      "epoch 93 takes 2.19269704819 seconds\n",
      "epoch 94 takes 2.16106891632 seconds\n",
      "epoch 95 takes 2.15234613419 seconds\n",
      "epoch 96 takes 2.22034096718 seconds\n",
      "epoch 97 takes 2.24446487427 seconds\n",
      "epoch 98 takes 2.30402612686 seconds\n",
      "epoch 99 takes 2.17497396469 seconds\n",
      "epoch 100 takes 2.17866897583 seconds\n",
      "epoch 101 takes 2.16843104362 seconds\n",
      "epoch 102 takes 2.17446279526 seconds\n",
      "epoch 103 takes 2.16225910187 seconds\n",
      "epoch 104 takes 2.21620607376 seconds\n",
      "epoch 105 takes 2.16626000404 seconds\n",
      "epoch 106 takes 2.13861584663 seconds\n",
      "epoch 107 takes 2.15655779839 seconds\n",
      "epoch 108 takes 2.18000912666 seconds\n",
      "epoch 109 takes 2.17573595047 seconds\n",
      "epoch 110 takes 2.16877102852 seconds\n",
      "epoch 111 takes 2.27828192711 seconds\n",
      "epoch 112 takes 3.23965001106 seconds\n",
      "epoch 113 takes 2.19513511658 seconds\n",
      "epoch 114 takes 2.15624785423 seconds\n",
      "epoch 115 takes 2.20956993103 seconds\n",
      "epoch 116 takes 2.16151094437 seconds\n",
      "epoch 117 takes 2.16055893898 seconds\n",
      "epoch 118 takes 2.1502969265 seconds\n",
      "epoch 119 takes 2.16250014305 seconds\n",
      "epoch 120 takes 2.16934108734 seconds\n",
      "epoch 121 takes 2.15436315536 seconds\n",
      "epoch 122 takes 2.3013920784 seconds\n",
      "epoch 123 takes 3.2501540184 seconds\n",
      "epoch 124 takes 2.16442394257 seconds\n",
      "epoch 125 takes 2.17299199104 seconds\n",
      "epoch 126 takes 2.16472601891 seconds\n",
      "epoch 127 takes 2.19474291801 seconds\n",
      "epoch 128 takes 2.19321107864 seconds\n",
      "epoch 129 takes 2.1589641571 seconds\n",
      "epoch 130 takes 2.15871906281 seconds\n",
      "epoch 131 takes 2.13676691055 seconds\n",
      "epoch 132 takes 2.13655090332 seconds\n",
      "epoch 133 takes 2.28870201111 seconds\n",
      "epoch 134 takes 3.26018404961 seconds\n",
      "epoch 135 takes 2.21055698395 seconds\n",
      "epoch 136 takes 2.19906687737 seconds\n",
      "epoch 137 takes 2.18036794662 seconds\n",
      "epoch 138 takes 2.16312003136 seconds\n",
      "epoch 139 takes 2.20321202278 seconds\n",
      "epoch 140 takes 2.20203089714 seconds\n",
      "epoch 141 takes 2.16640400887 seconds\n",
      "epoch 142 takes 2.18449902534 seconds\n",
      "epoch 143 takes 2.18532586098 seconds\n",
      "epoch 144 takes 2.1661670208 seconds\n",
      "epoch 145 takes 2.17524385452 seconds\n",
      "epoch 146 takes 2.62991499901 seconds\n",
      "epoch 147 takes 3.09131503105 seconds\n",
      "epoch 148 takes 2.16685390472 seconds\n",
      "epoch 149 takes 2.24762296677 seconds\n",
      "epoch 150 takes 2.17872095108 seconds\n",
      "epoch 151 takes 2.18995094299 seconds\n",
      "epoch 152 takes 2.15523314476 seconds\n",
      "epoch 153 takes 2.1776509285 seconds\n",
      "epoch 154 takes 2.2064640522 seconds\n",
      "epoch 155 takes 2.58738088608 seconds\n",
      "epoch 156 takes 3.12327313423 seconds\n",
      "epoch 157 takes 2.17195391655 seconds\n",
      "epoch 158 takes 2.18971800804 seconds\n",
      "epoch 159 takes 2.32637000084 seconds\n",
      "epoch 160 takes 2.1516699791 seconds\n",
      "epoch 161 takes 2.22625803947 seconds\n",
      "epoch 162 takes 2.37572813034 seconds\n",
      "epoch 163 takes 3.24501681328 seconds\n",
      "epoch 164 takes 2.15843915939 seconds\n",
      "epoch 165 takes 2.25240015984 seconds\n",
      "epoch 166 takes 2.17348408699 seconds\n",
      "epoch 167 takes 2.15828084946 seconds\n",
      "epoch 168 takes 3.24995613098 seconds\n",
      "epoch 169 takes 2.42039585114 seconds\n",
      "epoch 170 takes 2.12827587128 seconds\n",
      "epoch 171 takes 2.15656709671 seconds\n",
      "epoch 172 takes 2.17361712456 seconds\n",
      "epoch 173 takes 2.68881392479 seconds\n",
      "epoch 174 takes 3.02804088593 seconds\n",
      "epoch 175 takes 2.19963693619 seconds\n",
      "epoch 176 takes 2.17697811127 seconds\n",
      "epoch 177 takes 2.14427185059 seconds\n",
      "epoch 178 takes 2.12581515312 seconds\n",
      "epoch 179 takes 2.20443081856 seconds\n",
      "epoch 180 takes 2.16666793823 seconds\n",
      "epoch 181 takes 2.16187214851 seconds\n",
      "epoch 182 takes 2.17200303078 seconds\n",
      "epoch 183 takes 2.15244197845 seconds\n",
      "epoch 184 takes 2.17453813553 seconds\n",
      "epoch 185 takes 2.15191197395 seconds\n",
      "epoch 186 takes 2.15671920776 seconds\n",
      "epoch 187 takes 2.17106509209 seconds\n",
      "epoch 188 takes 2.21574497223 seconds\n",
      "epoch 189 takes 2.22640180588 seconds\n",
      "epoch 190 takes 2.1858830452 seconds\n",
      "epoch 191 takes 2.1552760601 seconds\n",
      "epoch 192 takes 2.17060494423 seconds\n",
      "epoch 193 takes 2.18383288383 seconds\n",
      "epoch 194 takes 2.19799304008 seconds\n",
      "epoch 195 takes 2.16852712631 seconds\n",
      "epoch 196 takes 2.17440390587 seconds\n",
      "epoch 197 takes 2.18829894066 seconds\n",
      "epoch 198 takes 2.14824819565 seconds\n",
      "epoch 199 takes 2.18839287758 seconds\n",
      "epoch 200 takes 2.17522287369 seconds\n",
      "epoch 201 takes 2.27257204056 seconds\n",
      "epoch 202 takes 2.21940803528 seconds\n",
      "epoch 203 takes 2.19107317924 seconds\n",
      "epoch 204 takes 2.20343494415 seconds\n",
      "epoch 205 takes 2.18785095215 seconds\n",
      "epoch 206 takes 2.17237401009 seconds\n",
      "epoch 207 takes 2.20664906502 seconds\n",
      "epoch 208 takes 2.17756581306 seconds\n",
      "epoch 209 takes 2.15527701378 seconds\n",
      "epoch 210 takes 2.1445980072 seconds\n",
      "epoch 211 takes 2.37476396561 seconds\n",
      "epoch 212 takes 2.13242506981 seconds\n",
      "epoch 213 takes 2.17978000641 seconds\n",
      "epoch 214 takes 2.17181611061 seconds\n",
      "epoch 215 takes 2.18542098999 seconds\n",
      "epoch 216 takes 2.21823406219 seconds\n",
      "epoch 217 takes 2.17491292953 seconds\n",
      "epoch 218 takes 2.16849398613 seconds\n",
      "epoch 219 takes 2.16093993187 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 220 takes 2.16385793686 seconds\n",
      "epoch 221 takes 2.17907691002 seconds\n",
      "epoch 222 takes 2.17316198349 seconds\n",
      "epoch 223 takes 2.15888595581 seconds\n",
      "epoch 224 takes 2.18077802658 seconds\n",
      "epoch 225 takes 2.15671396255 seconds\n",
      "epoch 226 takes 2.18477201462 seconds\n",
      "epoch 227 takes 2.17771697044 seconds\n",
      "epoch 228 takes 2.15832996368 seconds\n",
      "epoch 229 takes 2.17656493187 seconds\n",
      "epoch 230 takes 2.18700504303 seconds\n",
      "epoch 231 takes 2.18056201935 seconds\n",
      "epoch 232 takes 2.14964079857 seconds\n",
      "epoch 233 takes 2.19138097763 seconds\n",
      "epoch 234 takes 2.17033982277 seconds\n",
      "epoch 235 takes 2.16362977028 seconds\n",
      "epoch 236 takes 2.1630859375 seconds\n",
      "epoch 237 takes 2.20103979111 seconds\n",
      "epoch 238 takes 2.16496896744 seconds\n",
      "epoch 239 takes 2.16212010384 seconds\n",
      "epoch 240 takes 2.18788099289 seconds\n",
      "epoch 241 takes 2.19210910797 seconds\n",
      "epoch 242 takes 2.18078112602 seconds\n",
      "epoch 243 takes 2.14986014366 seconds\n",
      "epoch 244 takes 2.2799551487 seconds\n",
      "epoch 245 takes 2.16138005257 seconds\n",
      "epoch 246 takes 2.19839406013 seconds\n",
      "epoch 247 takes 2.17974114418 seconds\n",
      "epoch 248 takes 2.18147802353 seconds\n",
      "epoch 249 takes 2.15574097633 seconds\n",
      "epoch 250 takes 2.16590213776 seconds\n",
      "epoch 251 takes 2.15344500542 seconds\n",
      "epoch 252 takes 2.17403197289 seconds\n",
      "epoch 253 takes 2.15985107422 seconds\n",
      "epoch 254 takes 2.15629315376 seconds\n",
      "epoch 255 takes 2.15854406357 seconds\n",
      "epoch 256 takes 2.22744393349 seconds\n",
      "epoch 257 takes 2.30681800842 seconds\n",
      "epoch 258 takes 2.15233182907 seconds\n",
      "epoch 259 takes 2.18538308144 seconds\n",
      "epoch 260 takes 2.16655898094 seconds\n",
      "epoch 261 takes 2.20910096169 seconds\n",
      "epoch 262 takes 2.15808987617 seconds\n",
      "epoch 263 takes 2.19462990761 seconds\n",
      "epoch 264 takes 2.16121506691 seconds\n",
      "epoch 265 takes 2.19498085976 seconds\n",
      "epoch 266 takes 2.17291688919 seconds\n",
      "epoch 267 takes 2.18997216225 seconds\n",
      "epoch 268 takes 2.1742401123 seconds\n",
      "epoch 269 takes 2.13996505737 seconds\n",
      "epoch 270 takes 2.17460298538 seconds\n",
      "epoch 271 takes 2.2074971199 seconds\n",
      "epoch 272 takes 2.14854598045 seconds\n",
      "epoch 273 takes 2.17268395424 seconds\n",
      "epoch 274 takes 2.16309690475 seconds\n",
      "epoch 275 takes 2.16398000717 seconds\n",
      "epoch 276 takes 2.1788649559 seconds\n",
      "epoch 277 takes 2.17416596413 seconds\n",
      "epoch 278 takes 2.16597700119 seconds\n",
      "epoch 279 takes 2.19619512558 seconds\n",
      "epoch 280 takes 2.15376114845 seconds\n",
      "epoch 281 takes 2.15129184723 seconds\n",
      "epoch 282 takes 2.16157507896 seconds\n",
      "epoch 283 takes 2.15993189812 seconds\n",
      "epoch 284 takes 2.13705706596 seconds\n",
      "epoch 285 takes 2.16641902924 seconds\n",
      "epoch 286 takes 2.23495793343 seconds\n",
      "epoch 287 takes 2.17236399651 seconds\n",
      "epoch 288 takes 2.17751407623 seconds\n",
      "epoch 289 takes 2.20740008354 seconds\n",
      "epoch 290 takes 2.15335011482 seconds\n",
      "epoch 291 takes 2.12811183929 seconds\n",
      "epoch 292 takes 2.18782615662 seconds\n",
      "epoch 293 takes 2.15758085251 seconds\n",
      "epoch 294 takes 2.158878088 seconds\n",
      "epoch 295 takes 2.18242001534 seconds\n",
      "epoch 296 takes 2.13809418678 seconds\n",
      "epoch 297 takes 2.14627599716 seconds\n",
      "epoch 298 takes 2.17808413506 seconds\n",
      "epoch 299 takes 2.18217086792 seconds\n",
      "epoch 300 takes 2.13653206825 seconds\n",
      "epoch 301 takes 2.16431283951 seconds\n",
      "epoch 302 takes 2.1914358139 seconds\n",
      "epoch 303 takes 2.19033503532 seconds\n",
      "epoch 304 takes 2.15924310684 seconds\n",
      "epoch 305 takes 2.15387296677 seconds\n",
      "epoch 306 takes 2.16748285294 seconds\n",
      "epoch 307 takes 2.18355107307 seconds\n",
      "epoch 308 takes 2.19405794144 seconds\n",
      "epoch 309 takes 2.16815686226 seconds\n",
      "epoch 310 takes 2.17868304253 seconds\n",
      "epoch 311 takes 2.18485879898 seconds\n",
      "epoch 312 takes 2.18845582008 seconds\n",
      "epoch 313 takes 2.15716695786 seconds\n",
      "epoch 314 takes 2.13785219193 seconds\n",
      "epoch 315 takes 2.16825199127 seconds\n",
      "epoch 316 takes 2.19143104553 seconds\n",
      "epoch 317 takes 2.13754701614 seconds\n",
      "epoch 318 takes 2.16170620918 seconds\n",
      "epoch 319 takes 2.15097403526 seconds\n",
      "epoch 320 takes 2.18618202209 seconds\n",
      "epoch 321 takes 2.16444396973 seconds\n",
      "epoch 322 takes 2.2006790638 seconds\n",
      "epoch 323 takes 2.19149494171 seconds\n",
      "epoch 324 takes 2.17915010452 seconds\n",
      "epoch 325 takes 2.19195914268 seconds\n",
      "epoch 326 takes 2.16378092766 seconds\n",
      "epoch 327 takes 2.17139387131 seconds\n",
      "epoch 328 takes 2.18984699249 seconds\n",
      "epoch 329 takes 2.18044185638 seconds\n",
      "epoch 330 takes 2.45310783386 seconds\n",
      "epoch 331 takes 3.23565292358 seconds\n",
      "epoch 332 takes 2.14959812164 seconds\n",
      "epoch 333 takes 2.16844797134 seconds\n",
      "epoch 334 takes 2.16691613197 seconds\n",
      "epoch 335 takes 2.18131399155 seconds\n",
      "epoch 336 takes 2.16374897957 seconds\n",
      "epoch 337 takes 2.31581306458 seconds\n",
      "epoch 338 takes 3.24895405769 seconds\n",
      "epoch 339 takes 2.21939992905 seconds\n",
      "epoch 340 takes 2.19364500046 seconds\n",
      "epoch 341 takes 2.17795801163 seconds\n",
      "epoch 342 takes 2.15440607071 seconds\n",
      "epoch 343 takes 3.17870616913 seconds\n",
      "epoch 344 takes 2.50149703026 seconds\n",
      "epoch 345 takes 2.14390587807 seconds\n",
      "epoch 346 takes 2.21224498749 seconds\n",
      "epoch 347 takes 2.15859889984 seconds\n",
      "epoch 348 takes 2.73526000977 seconds\n",
      "epoch 349 takes 2.95041394234 seconds\n",
      "epoch 350 takes 2.15422487259 seconds\n",
      "epoch 351 takes 2.15221691132 seconds\n",
      "epoch 352 takes 2.19000887871 seconds\n",
      "epoch 353 takes 2.17258405685 seconds\n",
      "epoch 354 takes 2.15556097031 seconds\n",
      "epoch 355 takes 2.18138217926 seconds\n",
      "epoch 356 takes 2.16673016548 seconds\n",
      "epoch 357 takes 2.18731093407 seconds\n",
      "epoch 358 takes 2.23461818695 seconds\n",
      "epoch 359 takes 3.16843485832 seconds\n",
      "epoch 360 takes 2.55572199821 seconds\n",
      "epoch 361 takes 2.18385791779 seconds\n",
      "epoch 362 takes 2.20637798309 seconds\n",
      "epoch 363 takes 2.22757601738 seconds\n",
      "epoch 364 takes 2.80463290215 seconds\n",
      "epoch 365 takes 2.93376398087 seconds\n",
      "epoch 366 takes 2.17996788025 seconds\n",
      "epoch 367 takes 2.20187902451 seconds\n",
      "epoch 368 takes 2.15179800987 seconds\n",
      "epoch 369 takes 2.22335100174 seconds\n",
      "epoch 370 takes 2.15660405159 seconds\n",
      "epoch 371 takes 2.65904712677 seconds\n",
      "epoch 372 takes 3.02219605446 seconds\n",
      "epoch 373 takes 2.18905806541 seconds\n",
      "epoch 374 takes 2.18394994736 seconds\n",
      "epoch 375 takes 2.20021390915 seconds\n",
      "epoch 376 takes 2.36093997955 seconds\n",
      "epoch 377 takes 3.25881004333 seconds\n",
      "epoch 378 takes 2.19545507431 seconds\n",
      "epoch 379 takes 2.16782784462 seconds\n",
      "epoch 380 takes 2.17133617401 seconds\n",
      "epoch 381 takes 2.19297885895 seconds\n",
      "epoch 382 takes 3.22196888924 seconds\n",
      "epoch 383 takes 2.49125814438 seconds\n",
      "epoch 384 takes 2.164041996 seconds\n",
      "epoch 385 takes 2.17692494392 seconds\n",
      "epoch 386 takes 2.18412089348 seconds\n",
      "epoch 387 takes 2.7625348568 seconds\n",
      "epoch 388 takes 2.94831895828 seconds\n",
      "epoch 389 takes 2.18085193634 seconds\n",
      "epoch 390 takes 2.17558503151 seconds\n",
      "epoch 391 takes 2.28575205803 seconds\n",
      "epoch 392 takes 2.17702794075 seconds\n",
      "epoch 393 takes 2.17901396751 seconds\n",
      "epoch 394 takes 2.68748617172 seconds\n",
      "epoch 395 takes 3.01090097427 seconds\n",
      "epoch 396 takes 2.1981959343 seconds\n",
      "epoch 397 takes 2.19130897522 seconds\n",
      "epoch 398 takes 2.17333984375 seconds\n",
      "epoch 399 takes 2.31025314331 seconds\n",
      "train phase finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Initial Params =======================#\n",
    "Params = {}\n",
    "\n",
    "# 1-th layer\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "# 2-th layer\n",
    "Params['Phi_2']  = 0.2 + 0.8*np.random.rand(Setting['K1'], Setting['K2'])\n",
    "Params['Phi_2']  = Params['Phi_2'] / np.sum(Params['Phi_2'], axis=0)\n",
    "Params['Theta_2']= np.random.rand(Setting['N_train'], Setting['K2'])\n",
    "\n",
    "Params['c3_n']   = 1 * np.ones([Setting['N_train']])\n",
    "tmp = -log_max(1 - Params['p2_n'])\n",
    "Params['p3_n']   = (tmp / (tmp + Params['c3_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "# 3-th layer\n",
    "Params['Phi_3']  = 0.2 + 0.8*np.random.rand(Setting['K2'], Setting['K3'])\n",
    "Params['Phi_3']  = Params['Phi_3'] / np.sum(Params['Phi_3'], axis=0)\n",
    "Params['Theta_3']= np.random.rand(Setting['N_train'], Setting['K3'])\n",
    "\n",
    "Params['c4_n']   = 1 * np.ones([Setting['N_train']])\n",
    "tmp = -log_max(1 - Params['p3_n'])\n",
    "Params['p4_n']   = (tmp / (tmp + Params['c4_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K3'], 1]) / Setting['K3']\n",
    "\n",
    "# Collection\n",
    "W_train_1 = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "W_train_2 = np.zeros([Setting['N_train'], Setting['K2']])\n",
    "W_train_3 = np.zeros([Setting['N_train'], Setting['K3']])\n",
    "\n",
    "# 转化为GPU的输入形式\n",
    "\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "\n",
    "# Gibbs\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #==========================第一层增广==========================＃\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])  # 增广矩阵用于更新s34维度上增广\n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1']) # 增广矩阵用于更新s12维度上增广\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32') \n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "    \n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "    \n",
    "    # 第一层增广的结果\n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64') # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')  # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3),axis=2) # N*K1\n",
    "    \n",
    "    #==========================第二层增广==========================＃\n",
    "    M1_tmp = np.array(np.transpose(np.round(Params['W1_nk1_Aug_Pooling'])), dtype='float64', order='C')\n",
    "    Theta2_tmp = np.array(np.transpose(Params['Theta_2']), dtype='float64', order='C')\n",
    "    # 第二层增广的结果\n",
    "    Xt_to_t1_2,WSZS_2 = PGBN_sampler.Crt_Multirnd_Matrix(M1_tmp, Params['Phi_2'], Theta2_tmp)\n",
    "    \n",
    "    #==========================第三层增广==========================＃\n",
    "    M2_tmp = np.array(np.round(Xt_to_t1_2), dtype='float64', order='C')\n",
    "    Theta3_tmp = np.array(np.transpose(Params['Theta_3']), dtype='float64', order='C')\n",
    "    # 第三层增广的结果\n",
    "    Xt_to_t1_3,WSZS_3 = PGBN_sampler.Crt_Multirnd_Matrix(M2_tmp, Params['Phi_3'], Theta3_tmp)\n",
    "    \n",
    "    #==========================更新参数==========================＃\n",
    "    # 更新D,Phi\n",
    "    for k1 in range(Setting['K1']):\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] # 按三四维增广的矩阵\n",
    "        X_k1_34_tmp = np.random.gamma(X_k1_34 + SuperParams['eta'])\n",
    "        D1_k1_s     = X_k1_34_tmp / np.sum(X_k1_34_tmp, axis=0, keepdims=1)\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "        \n",
    "    Phi_2_tmp       = np.random.gamma(WSZS_2 + SuperParams['eta'])\n",
    "    Params['Phi_2'] = Phi_2_tmp / np.sum(Phi_2_tmp, axis=0)\n",
    "    \n",
    "    Phi_3_tmp       = np.random.gamma(WSZS_3 + SuperParams['eta'])\n",
    "    Params['Phi_3'] = Phi_3_tmp / np.sum(Phi_3_tmp, axis=0)\n",
    "    \n",
    "    # 更新c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_2'], Params['Theta_2'].T),0)) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    Params['c3_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_3'], Params['Theta_3'].T),0)) \n",
    "    Params['c3_n']     = Params['c3_n'] / (SuperParams['f0'] + np.sum(Params['Theta_2'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p2_n'])\n",
    "    Params['p3_n']     = tmp / (Params['c3_n'] + tmp)\n",
    "    \n",
    "    Params['c4_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c4_n']     = Params['c4_n'] / (SuperParams['f0'] + np.sum(Params['Theta_3'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p3_n'])\n",
    "    Params['p4_n']     = tmp / (Params['c4_n'] + tmp)\n",
    "    \n",
    "    # 更新w_j\n",
    "    W_k3_sn = np.random.gamma(Params['Gamma'] + Xt_to_t1_3) / (-np.log(1-Params['p3_n']) + Params['c4_n']) # V*N\n",
    "    Params['Theta_3'] = np.transpose(W_k3_sn)\n",
    "    \n",
    "    shape2 = np.dot(Params['Phi_3'], Params['Theta_3'].T)\n",
    "    W_k2_sn = np.random.gamma(shape2 + Xt_to_t1_2) / (-np.log(1-Params['p2_n']) + Params['c3_n']) # V*N\n",
    "    Params['Theta_2'] = np.transpose(W_k2_sn)\n",
    "    \n",
    "    shape1 = np.dot(Params['Phi_2'], Params['Theta_2'].T) # V*N\n",
    "    W_k1_sn = np.random.gamma(shape1 + Params['W1_nk1_Aug_Pooling'].T ) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn) \n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        \n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "#     if t == 0:\n",
    "#         Iter_time.append(end_time - start_time)\n",
    "#     else:\n",
    "#         Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    # Likelyhood\n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "    if t >=Setting['Burinin']:\n",
    "        \n",
    "        W_train_1 = W_train_1 + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])\n",
    "        W_train_2 = W_train_2 + Params['Theta_2']\n",
    "        W_train_3 = W_train_3 + Params['Theta_3']\n",
    "        \n",
    "#     if np.mod(t,1) == 0:\n",
    "        \n",
    "#         likelyhood = 0\n",
    "#         start_time = time.time()\n",
    "#         Orgin_X = np.zeros([Setting['N_train'], Setting['K1_V1'], Setting['K1_V2']])\n",
    "#         Orgin_X[[batch_file_index, batch_rows, batch_cols+1]] = batch_value\n",
    "\n",
    "#         for i in range(Setting['N_train']):\n",
    "            \n",
    "#             Phi_tmp = np.transpose(np.reshape(Params['D1_k1'],[Setting['K1'], Setting['K1_S3'], Setting['K1_S4'], 1]),[1,2,3,0])\n",
    "#             Theta_tmp = np.transpose(Params['W1_nk1'][i:i+1,:,:,:], [0,2,3,1])\n",
    "#             PhiTheta_1= sess.run(X_1, feed_dict={Phi_1:P-hi_tmp.astype(np.float32), Theta_1:Theta_tmp.astype(np.float32)})\n",
    "#             PhiTheta_1= PhiTheta_1.astype(np.float64)\n",
    "#             likelyhood = likelyhood + np.sum(Orgin_X[i,:,:] * log_max(PhiTheta_1[0,:,:,0]) - PhiTheta_1[0,:,:,0] - log_max(gamma(Orgin_X[i,:,:] + 1)))  \n",
    "#         end_time = time.time()\n",
    "#         print \"Likelihood \" + str(likelyhood / Setting['N_train']) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "#         Iter_lh.append(likelyhood / Setting['N_train'])\n",
    "        \n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(Iter_time ,Iter_lh)\n",
    "# plt.show()    \n",
    "\n",
    "# sio.savemat('TREC_Layer3_Pooling.mat',{'Time_Layer3_Pooling':Iter_time, 'LH_Layer3_Pooling':Iter_lh})\n",
    "# print 'save mat finished'\n",
    "    \n",
    "print \"train phase finished\"\n",
    "\n",
    "W_train_1 =  W_train_1 / Setting['Collection']  \n",
    "W_train_2 =  W_train_2 / Setting['Collection']  \n",
    "W_train_3 =  W_train_3 / Setting['Collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1st\n",
    "f = open(\"./TREC_layer1_3.txt\", \"w\")\n",
    "for k in range(200):\n",
    "    for i in range(3):\n",
    "        a = np.argsort(-Params['D1_k1'][k, :, i])\n",
    "        line = str(k) + \" \" + \" \".join([data_vab_list[l-1] for l in a[:10]])\n",
    "        f.write(line)\n",
    "        f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "# 2nd\n",
    "f = open(\"./TREC_layer2_3.txt\", \"w\")\n",
    "x = np.dot(Params['Phi_2'].T, Params['D1_k1'].reshape(200, -1)).reshape(100, 8000, 3)\n",
    "for k in range(100):\n",
    "    f.write(\"***************************\\n\")\n",
    "    f.write(\"layer2: \"+str(k)+\"th topic\\n\")\n",
    "    for i in range(3):\n",
    "        a = np.argsort(-x[k, :, i])\n",
    "        line = \" \".join([data_vab_list[l-1] for l in a[:10]])\n",
    "        f.write(line)\n",
    "        f.write(\"\\n\")\n",
    "    f.write(\"\\n\"+\"layer1: \\n\")\n",
    "    wei = Params['Phi_2'][:, k]\n",
    "    a = np.argsort(-wei)\n",
    "    for i in a[:5]:\n",
    "        f.write(str(i) + \" \" + str(wei[i]))\n",
    "        f.write(\"\\n\")\n",
    "        for j in range(3):\n",
    "            b = np.argsort(-Params['D1_k1'][i, :, j])\n",
    "            line = \" \".join([data_vab_list[l-1] for l in b[:10]])\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    f.write(\"***************************\\n\")\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "# 3rd\n",
    "f = open(\"./TREC_layer3_3.txt\", \"w\")\n",
    "x = np.dot(Params['Phi_2'].T, Params['D1_k1'].reshape(200, -1)).reshape(100, 8000, 3)\n",
    "x1 = np.dot(Params['Phi_3'].T, x.reshape(100, -1)).reshape(50, 8000, 3)\n",
    "for k in range(50):\n",
    "    f.write(\"***************************\\n\")\n",
    "    f.write(\"layer3: \"+str(k)+\"th topic\\n\")\n",
    "    for i in range(3):\n",
    "        a = np.argsort(-x1[k, :, i])\n",
    "        line = \" \".join([data_vab_list[l-1] for l in a[:10]])\n",
    "        f.write(line)\n",
    "        f.write(\"\\n\")\n",
    "    f.write(\"\\n\"+\"layer2: \\n\")\n",
    "    wei = Params['Phi_3'][:, k]\n",
    "    a = np.argsort(-wei)\n",
    "    for i in a[:5]:\n",
    "        f.write(str(i) + \" \" + str(wei[i]))\n",
    "        f.write(\"\\n\")\n",
    "        for j in range(3):\n",
    "            b = np.argsort(-x[i, :, j])\n",
    "            line = \" \".join([data_vab_list[l-1] for l in b[:10]])\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")  \n",
    "    f.write(\"***************************\\n\")\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess finished\n",
      "epoch 0 takes 1.24795293808 seconds\n",
      "epoch 1 takes 1.25392580032 seconds\n",
      "epoch 2 takes 1.25309896469 seconds\n",
      "epoch 3 takes 1.23823904991 seconds\n",
      "epoch 4 takes 1.24816989899 seconds\n",
      "epoch 5 takes 1.25756502151 seconds\n",
      "epoch 6 takes 1.24933099747 seconds\n",
      "epoch 7 takes 1.24836301804 seconds\n",
      "epoch 8 takes 1.25531506538 seconds\n",
      "epoch 9 takes 1.24104309082 seconds\n",
      "epoch 10 takes 1.25229787827 seconds\n",
      "epoch 11 takes 1.25768709183 seconds\n",
      "epoch 12 takes 1.25303888321 seconds\n",
      "epoch 13 takes 1.25480294228 seconds\n",
      "epoch 14 takes 1.23720097542 seconds\n",
      "epoch 15 takes 1.25226211548 seconds\n",
      "epoch 16 takes 1.24526786804 seconds\n",
      "epoch 17 takes 1.25012898445 seconds\n",
      "epoch 18 takes 1.24178195 seconds\n",
      "epoch 19 takes 1.24595189095 seconds\n",
      "epoch 20 takes 1.24510383606 seconds\n",
      "epoch 21 takes 1.24511003494 seconds\n",
      "epoch 22 takes 1.23635911942 seconds\n",
      "epoch 23 takes 1.25932383537 seconds\n",
      "epoch 24 takes 1.24570584297 seconds\n",
      "epoch 25 takes 1.25293684006 seconds\n",
      "epoch 26 takes 1.24677681923 seconds\n",
      "epoch 27 takes 1.25158309937 seconds\n",
      "epoch 28 takes 1.23738598824 seconds\n",
      "epoch 29 takes 1.25460481644 seconds\n",
      "epoch 30 takes 1.24997901917 seconds\n",
      "epoch 31 takes 1.25633716583 seconds\n",
      "epoch 32 takes 1.2616250515 seconds\n",
      "epoch 33 takes 1.25881886482 seconds\n",
      "epoch 34 takes 1.25184106827 seconds\n",
      "epoch 35 takes 1.24850797653 seconds\n",
      "epoch 36 takes 1.2439520359 seconds\n",
      "epoch 37 takes 1.24720287323 seconds\n",
      "epoch 38 takes 1.28746819496 seconds\n",
      "epoch 39 takes 1.28195118904 seconds\n",
      "epoch 40 takes 1.26378011703 seconds\n",
      "epoch 41 takes 1.32067203522 seconds\n",
      "epoch 42 takes 1.32712221146 seconds\n",
      "epoch 43 takes 1.3054151535 seconds\n",
      "epoch 44 takes 1.28536486626 seconds\n",
      "epoch 45 takes 1.28213381767 seconds\n",
      "epoch 46 takes 1.26593494415 seconds\n",
      "epoch 47 takes 1.28896808624 seconds\n",
      "epoch 48 takes 1.29381799698 seconds\n",
      "epoch 49 takes 1.25283694267 seconds\n",
      "epoch 50 takes 1.23964881897 seconds\n",
      "epoch 51 takes 1.24274992943 seconds\n",
      "epoch 52 takes 1.2378911972 seconds\n",
      "epoch 53 takes 1.24818682671 seconds\n",
      "epoch 54 takes 1.25362586975 seconds\n",
      "epoch 55 takes 1.2555809021 seconds\n",
      "epoch 56 takes 1.24387383461 seconds\n",
      "epoch 57 takes 1.26389908791 seconds\n",
      "epoch 58 takes 1.2443959713 seconds\n",
      "epoch 59 takes 1.2427740097 seconds\n",
      "epoch 60 takes 1.24686098099 seconds\n",
      "epoch 61 takes 1.23860001564 seconds\n",
      "epoch 62 takes 1.237429142 seconds\n",
      "epoch 63 takes 1.24855995178 seconds\n",
      "epoch 64 takes 1.25403189659 seconds\n",
      "epoch 65 takes 1.25221896172 seconds\n",
      "epoch 66 takes 1.23917102814 seconds\n",
      "epoch 67 takes 1.25576901436 seconds\n",
      "epoch 68 takes 1.24595689774 seconds\n",
      "epoch 69 takes 1.25205302238 seconds\n",
      "epoch 70 takes 1.25930905342 seconds\n",
      "epoch 71 takes 1.23842692375 seconds\n",
      "epoch 72 takes 1.24952292442 seconds\n",
      "epoch 73 takes 1.24462890625 seconds\n",
      "epoch 74 takes 1.24407100677 seconds\n",
      "epoch 75 takes 1.24125695229 seconds\n",
      "epoch 76 takes 1.24476289749 seconds\n",
      "epoch 77 takes 1.24270606041 seconds\n",
      "epoch 78 takes 1.24018597603 seconds\n",
      "epoch 79 takes 1.25014400482 seconds\n",
      "epoch 80 takes 1.24141216278 seconds\n",
      "epoch 81 takes 1.26394486427 seconds\n",
      "epoch 82 takes 1.40935301781 seconds\n",
      "epoch 83 takes 1.29369187355 seconds\n",
      "epoch 84 takes 1.25667905807 seconds\n",
      "epoch 85 takes 1.32269906998 seconds\n",
      "epoch 86 takes 1.32076382637 seconds\n",
      "epoch 87 takes 1.29621291161 seconds\n",
      "epoch 88 takes 1.30204892159 seconds\n",
      "epoch 89 takes 1.26300287247 seconds\n",
      "epoch 90 takes 1.24122905731 seconds\n",
      "epoch 91 takes 1.25333213806 seconds\n",
      "epoch 92 takes 1.25121212006 seconds\n",
      "epoch 93 takes 1.24829101562 seconds\n",
      "epoch 94 takes 1.24900889397 seconds\n",
      "epoch 95 takes 1.24795007706 seconds\n",
      "epoch 96 takes 1.24824404716 seconds\n",
      "epoch 97 takes 1.24204707146 seconds\n",
      "epoch 98 takes 1.24884104729 seconds\n",
      "epoch 99 takes 1.24353194237 seconds\n"
     ]
    }
   ],
   "source": [
    "#=============preprocess==============#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_test_list)): # 所有样本遍历\n",
    "    \n",
    "    x_single = np.reshape(data_test_list_index[i], [len(data_test_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_test_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0)\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_test_label[i]])),axis=0)\n",
    "        \n",
    "batch_len_te        = batch_len\n",
    "batch_rows_te       = batch_rows\n",
    "batch_cols_te       = batch_cols\n",
    "batch_file_index_te = batch_file_index\n",
    "batch_value_te      = batch_value\n",
    "batch_label_te      = batch_label\n",
    "\n",
    "print 'preprocess finished'\n",
    "\n",
    "# Initial Params\n",
    "\n",
    "# 1-th layer\n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_test'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_test']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "# 2-th layer\n",
    "Params['Theta_2']= np.random.rand(Setting['N_test'], Setting['K2'])\n",
    "\n",
    "Params['c3_n']   = 1 * np.ones([Setting['N_test']])\n",
    "tmp = -log_max(1 - Params['p2_n'])\n",
    "Params['p3_n']   = (tmp / (tmp + Params['c3_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "# 3-th layer\n",
    "Params['Theta_3']= np.random.rand(Setting['N_test'], Setting['K3'])\n",
    "\n",
    "Params['c4_n']   = 1 * np.ones([Setting['N_test']])\n",
    "tmp = -log_max(1 - Params['p3_n'])\n",
    "Params['p4_n']   = (tmp / (tmp + Params['c4_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K3'], 1]) / Setting['K3']\n",
    "\n",
    "# Collection\n",
    "W_test_1 = np.zeros([Setting['N_test'], Setting['K1']])\n",
    "W_test_2 = np.zeros([Setting['N_test'], Setting['K2']])\n",
    "W_test_3 = np.zeros([Setting['N_test'], Setting['K3']])\n",
    "\n",
    "# 转化为GPU的输入形式\n",
    "\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "# Gibbs\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #==========================增广==========================＃\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])  # 增广矩阵用于更新s34维度上增广\n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1']) # 增广矩阵用于更新s12维度上增广\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32') \n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "    \n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "    \n",
    "    # 第一层增广的结果\n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64') # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')  # K1*S3*S4\n",
    "    \n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3),axis=2) # N*K1\n",
    "    \n",
    "    #==========================第二层增广==========================＃\n",
    "    \n",
    "    M1_tmp = np.array(np.transpose(np.round(Params['W1_nk1_Aug_Pooling'])), dtype='float64', order='C')\n",
    "    Theta2_tmp = np.array(np.transpose(Params['Theta_2']), dtype='float64', order='C')\n",
    "    Xt_to_t1_2,WSZS_2 = PGBN_sampler.Crt_Multirnd_Matrix(M1_tmp, Params['Phi_2'], Theta2_tmp)\n",
    "    \n",
    "    #==========================第三层增广==========================＃\n",
    "    M2_tmp = np.array(np.round(Xt_to_t1_2), dtype='float64', order='C')\n",
    "    Theta3_tmp = np.array(np.transpose(Params['Theta_3']), dtype='float64', order='C')\n",
    "    Xt_to_t1_3,WSZS_3 = PGBN_sampler.Crt_Multirnd_Matrix(M2_tmp, Params['Phi_3'], Theta3_tmp)\n",
    "    \n",
    "    #==========================更新参数==========================＃\n",
    "    # 更新第一层的Phi\n",
    "    for k1 in range(Setting['K1']):\n",
    "        # update 1th D\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] # 按三四维增广的矩阵\n",
    "        X_k1_34_tmp = np.random.gamma(X_k1_34 + SuperParams['eta'])\n",
    "        D1_k1_s     = X_k1_34_tmp / np.sum(X_k1_34_tmp)\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "        \n",
    "    Phi_2_tmp       = np.random.gamma(WSZS_2 + SuperParams['eta'])\n",
    "    Params['Phi_2'] = Phi_2_tmp / np.sum(Phi_2_tmp, axis=0)\n",
    "    \n",
    "    Phi_3_tmp       = np.random.gamma(WSZS_3 + SuperParams['eta'])\n",
    "    Params['Phi_3'] = Phi_3_tmp / np.sum(Phi_3_tmp, axis=0)\n",
    "    \n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_2'], Params['Theta_2'].T),0)) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    Params['c3_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_3'], Params['Theta_3'].T),0)) \n",
    "    Params['c3_n']     = Params['c3_n'] / (SuperParams['f0'] + np.sum(Params['Theta_2'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p2_n'])\n",
    "    Params['p3_n']     = tmp / (Params['c3_n'] + tmp)\n",
    "    \n",
    "    Params['c4_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c4_n']     = Params['c4_n'] / (SuperParams['f0'] + np.sum(Params['Theta_3'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p3_n'])\n",
    "    Params['p4_n']     = tmp / (Params['c4_n'] + tmp)\n",
    "    \n",
    "    W_k3_sn = np.random.gamma(Params['Gamma'] + Xt_to_t1_3) / (-np.log(1-Params['p3_n']) + Params['c4_n']) # V*N\n",
    "    Params['Theta_3'] = np.transpose(W_k3_sn)\n",
    "    \n",
    "    shape2 = np.dot(Params['Phi_3'], Params['Theta_3'].T)\n",
    "    W_k2_sn = np.random.gamma(shape2 + Xt_to_t1_2) / (-np.log(1-Params['p2_n']) + Params['c3_n']) # V*N\n",
    "    Params['Theta_2'] = np.transpose(W_k2_sn)\n",
    "    \n",
    "    shape1 = np.dot(Params['Phi_2'], Params['Theta_2'].T) # V*N\n",
    "    W_k1_sn = np.random.gamma(shape1 + Params['W1_nk1_Aug_Pooling'].T ) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn) \n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        \n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Likelyhood\n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "    if t >=Setting['Burinin']:\n",
    "        \n",
    "        W_test_1 = W_test_1 + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])\n",
    "        W_test_2 = W_test_2 + Params['Theta_2']\n",
    "        W_test_3 = W_test_3 + Params['Theta_3']\n",
    "    \n",
    "#     if np.mod(t,100) == 0:\n",
    "        \n",
    "#         likelyhood = 0\n",
    "#         start_time = time.time()\n",
    "#         Orgin_X = np.zeros([Setting['N_test'], Setting['K1_V1'], Setting['K1_V2']])\n",
    "#         Orgin_X[[batch_file_index, batch_rows, batch_cols+1]] = batch_value\n",
    "\n",
    "#         for i in range(Setting['N_test']):\n",
    "            \n",
    "#             Phi_tmp = np.transpose(np.reshape(Params['D1_k1'],[Setting['K1'], Setting['K1_S3'], Setting['K1_S4'], 1]),[1,2,3,0])\n",
    "#             Theta_tmp = np.transpose(Params['W1_nk1'][i:i+1,:,:,:], [0,2,3,1])\n",
    "#             PhiTheta_1= sess.run(X_1, feed_dict={Phi_1:Phi_tmp.astype(np.float64), Theta_1:Theta_tmp.astype(np.float64)})\n",
    "\n",
    "#             likelyhood = likelyhood + np.sum(Orgin_X[i,:,:] * log_max(PhiTheta_1[0,:,:,0]) - PhiTheta_1[0,:,:,0] - log_max(gamma(Orgin_X[i,:,:] + 1)))  \n",
    "#         end_time = time.time()\n",
    "#         print \"Likelihood \" + str(likelyhood / Setting['N_test']) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "W_test_1 =  W_test_1 / Setting['Collection']  \n",
    "W_test_2 =  W_test_2 / Setting['Collection']  \n",
    "W_test_3 =  W_test_3 / Setting['Collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9477256052824652\n",
      "0.724\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# W_train_1 = np.concatenate([W_train_1, W_train_2, W_train_3], axis = 1)\n",
    "# W_test_1 = np.concatenate([W_test_1, W_test_2, W_test_3], axis = 1)\n",
    "\n",
    "# W_train = (W_train - np.reshape(np.mean(W_train,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_train, axis=0),[1,Setting['K1']])\n",
    "# W_test = (W_test - np.reshape(np.mean(W_test,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_test, axis=0),[1,Setting['K1']])\n",
    "\n",
    "\n",
    "clf = svm.SVC()                    # class\n",
    "clf.fit(W_train_1, batch_label_tr)            # training the svc model \n",
    "\n",
    "print clf.score(W_train_1, batch_label_tr)    # training the svc model \n",
    "print clf.score(W_test_1,  batch_label_te)    # training the svc model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_train = np.concatenate([W_train_1, W_train_2, W_train_3], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
