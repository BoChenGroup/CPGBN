{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "kernel intial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import gamma\n",
    "import time\n",
    "np.random.RandomState(1)\n",
    "realmin = 2.2e-10\n",
    "\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "#=======================Multi Sampler=======================#\n",
    "def MultRnd(value, MultRate):\n",
    "    MultRate = MultRate / np.sum(MultRate)\n",
    "    MultRate_Sum = np.reshape(MultRate, [-1])\n",
    "    N = len(MultRate_Sum)\n",
    "    Amnk = np.zeros([N])\n",
    "\n",
    "    if N == 1:\n",
    "        Amnk = value\n",
    "    else:\n",
    "        for i in range(1, N, 1):  # 从第二个开始\n",
    "            MultRate_Sum[i] = MultRate_Sum[i] + MultRate_Sum[i - 1]\n",
    "\n",
    "        Uni_Rnd = np.random.rand(np.int64(value))  # 0 - 1\n",
    "        flag_new = Uni_Rnd <= MultRate_Sum[0]\n",
    "        Amnk[0] = np.sum(flag_new)\n",
    "\n",
    "        for i in range(1, N, 1):  # 从第二个开始\n",
    "            flag_old = flag_new\n",
    "            flag_new = Uni_Rnd <= MultRate_Sum[i]\n",
    "            Amnk[i] = np.sum(~flag_old & flag_new)\n",
    "\n",
    "        Amnk = np.reshape(Amnk, MultRate.shape)\n",
    "        \n",
    "    return Amnk\n",
    "\n",
    "#=======================Disp Dictionary=======================#\n",
    "def Dis_Dic(D):\n",
    "    [K, K1, K2] = D.shape\n",
    "    w_n = np.ceil(np.sqrt(K))\n",
    "    h_n = np.ceil(K / w_n)\n",
    "    weight = w_n * K2\n",
    "    height = h_n * K1\n",
    "    Dic = np.zeros([np.int32(weight), np.int32(height)])\n",
    "    count = 0\n",
    "    for k1 in range(np.int32(w_n)):\n",
    "        for k2 in range(np.int32(h_n)):\n",
    "            Dic[k1 * K1: (k1 + 1) * K1, k2 * K2: (k2 + 1) * K2] = D[count, :, :]\n",
    "            count += 1\n",
    "            if count == K:\n",
    "                break\n",
    "        if count == K:\n",
    "            break\n",
    "    plt.figure\n",
    "    plt.imshow(Dic)\n",
    "    plt.show()\n",
    "\n",
    "#=======================Toeplitz Conversion=======================#\n",
    "def Conv_Aug(Kernel, Score_Shape):\n",
    "    [K1, K2] = Score_Shape\n",
    "    [K3, K4] = Kernel.shape\n",
    "    V1 = K1 + K3 - 1\n",
    "    V2 = K2 + K4 - 1\n",
    "\n",
    "    # Padding\n",
    "    Kernel_Pad = np.zeros([2 * V1 - K3, 2 * V2 - K4])  ## Pad [V1 - K3, V2 - K4]\n",
    "    Kernel_Pad[V1 - K3: V1, V2 - K4: V2] = Kernel\n",
    "    Kernel_Pad = Kernel_Pad.T\n",
    "    M, N = Kernel_Pad.shape\n",
    "    # Parameters\n",
    "    col_extent = N - K1 + 1\n",
    "    row_extent = M - K2 + 1\n",
    "\n",
    "    # Get Starting block indices\n",
    "    start_idx = np.arange(K2)[:, None] * N + np.arange(K1)\n",
    "    # Get offsetted indices across the height and width of input array\n",
    "    offset_idx = np.arange(row_extent)[:, None] * N + np.arange(col_extent)\n",
    "    # Get all actual indices & index into input array for final output\n",
    "    out = np.take(Kernel_Pad, start_idx.ravel()[:, None] + offset_idx.ravel())\n",
    "\n",
    "    return np.flip(out.T, axis=1)\n",
    "\n",
    "#=======================Load data=======================#\n",
    "import numpy as np\n",
    "import cPickle\n",
    "\n",
    "TREC = cPickle.load(open(\"./TREC_8k.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = TREC['Vocabulary']\n",
    "data_vab_count_list    = TREC['Vab_count']\n",
    "data_vab_length        = TREC['Vab_Size']\n",
    "data_label             = TREC['Label']\n",
    "data_train_list        = TREC['Train_Origin']\n",
    "data_train_label       = np.array(TREC['Train_Label'])\n",
    "data_train_split       = TREC['Train_Word_Split']\n",
    "data_train_list_index  = TREC['Train_Word2Index']\n",
    "data_test_list         = TREC['Test_Origin']\n",
    "data_test_label        = np.array(TREC['Test_Label'])\n",
    "data_test_split        = TREC['Test_Word_Split']\n",
    "data_test_list_index   = TREC['Test_Word2Index']\n",
    "data_value             = 50\n",
    "\n",
    "print 'load data'\n",
    "\n",
    "#=======================Preprocess=======================#\n",
    "delete_count = 0\n",
    "\n",
    "# 这里因为padding　所以每个index的cols都右移动一位　\n",
    "for i in range(len(data_train_list)): # 所有样本遍历\n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)                                         # 这里因为padding　所以每个index的cols都右移动一位　\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0) # 这里因为padding　所以每个index的cols都右移动一位　\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "\n",
    "print 'preprocess finished'\n",
    "\n",
    "batch_len_tr  = batch_len\n",
    "batch_rows_tr = batch_rows\n",
    "batch_cols_tr = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "# Setting\n",
    "Setting = {}\n",
    "Setting['N_train']    = len(data_train_list) - delete_count #大于等于10的2194个样本\n",
    "Setting['K1']         = 200\n",
    "Setting['K1_V1']      = TREC['Vab_Size']\n",
    "Setting['K1_V2']      = np.max(batch_len) + 2                      # 这里因为padding　所以加上２　\n",
    "Setting['K1_S3']      = TREC['Vab_Size']\n",
    "Setting['K1_S4']      = 3\n",
    "Setting['K1_S1']      = Setting['K1_V1'] + 1 - Setting['K1_S3']\n",
    "Setting['K1_S2']      = Setting['K1_V2'] + 1 - Setting['K1_S4']    # 这里因为padding 所以和原来np.max(batch_len)一样\n",
    "Setting['Iter']       = 100\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "# SuperParamsSetting\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.1 # Phi\n",
    "\n",
    "# Initial Graph\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) #HWC\n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 注意， cuda初始化中间不要加入cuda的操作,例如tensorflow!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# cuda par\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"kernel intial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 0.873236894608 seconds\n",
      "epoch 1 takes 0.795001029968 seconds\n",
      "epoch 2 takes 0.821462869644 seconds\n",
      "epoch 3 takes 0.82005405426 seconds\n",
      "epoch 4 takes 0.86566400528 seconds\n",
      "epoch 5 takes 0.788272857666 seconds\n",
      "epoch 6 takes 0.810852050781 seconds\n",
      "epoch 7 takes 0.809748888016 seconds\n",
      "epoch 8 takes 0.766607999802 seconds\n",
      "epoch 9 takes 0.75909614563 seconds\n",
      "epoch 10 takes 0.778878927231 seconds\n",
      "epoch 11 takes 0.749942064285 seconds\n",
      "epoch 12 takes 0.767052173615 seconds\n",
      "epoch 13 takes 0.755780935287 seconds\n",
      "epoch 14 takes 0.759402990341 seconds\n",
      "epoch 15 takes 0.774953126907 seconds\n",
      "epoch 16 takes 0.76395201683 seconds\n",
      "epoch 17 takes 0.792777061462 seconds\n",
      "epoch 18 takes 0.758210897446 seconds\n",
      "epoch 19 takes 0.772186040878 seconds\n",
      "epoch 20 takes 0.748733997345 seconds\n",
      "epoch 21 takes 0.760271072388 seconds\n",
      "epoch 22 takes 0.828788995743 seconds\n",
      "epoch 23 takes 0.736881017685 seconds\n",
      "epoch 24 takes 0.760843992233 seconds\n",
      "epoch 25 takes 0.762158870697 seconds\n",
      "epoch 26 takes 0.767997980118 seconds\n",
      "epoch 27 takes 0.777122020721 seconds\n",
      "epoch 28 takes 0.748533964157 seconds\n",
      "epoch 29 takes 0.819375991821 seconds\n",
      "epoch 30 takes 0.774257183075 seconds\n",
      "epoch 31 takes 0.753014087677 seconds\n",
      "epoch 32 takes 0.757775068283 seconds\n",
      "epoch 33 takes 0.775532007217 seconds\n",
      "epoch 34 takes 0.74844789505 seconds\n",
      "epoch 35 takes 0.730292081833 seconds\n",
      "epoch 36 takes 0.747121810913 seconds\n",
      "epoch 37 takes 0.764279127121 seconds\n",
      "epoch 38 takes 0.765654802322 seconds\n",
      "epoch 39 takes 0.772829055786 seconds\n",
      "epoch 40 takes 0.782469034195 seconds\n",
      "epoch 41 takes 0.782766819 seconds\n",
      "epoch 42 takes 0.751998901367 seconds\n",
      "epoch 43 takes 0.776864051819 seconds\n",
      "epoch 44 takes 0.747923851013 seconds\n",
      "epoch 45 takes 0.772079944611 seconds\n",
      "epoch 46 takes 0.783853054047 seconds\n",
      "epoch 47 takes 0.806307792664 seconds\n",
      "epoch 48 takes 0.769083976746 seconds\n",
      "epoch 49 takes 0.740921020508 seconds\n",
      "epoch 50 takes 0.747370958328 seconds\n",
      "epoch 51 takes 0.757081031799 seconds\n",
      "epoch 52 takes 0.780781984329 seconds\n",
      "epoch 53 takes 0.77664399147 seconds\n",
      "epoch 54 takes 0.755985021591 seconds\n",
      "epoch 55 takes 0.790179014206 seconds\n",
      "epoch 56 takes 0.754945993423 seconds\n",
      "epoch 57 takes 0.794279813766 seconds\n",
      "epoch 58 takes 0.751907110214 seconds\n",
      "epoch 59 takes 0.785606145859 seconds\n",
      "epoch 60 takes 0.774038076401 seconds\n",
      "epoch 61 takes 0.764136075974 seconds\n",
      "epoch 62 takes 0.771606206894 seconds\n",
      "epoch 63 takes 0.760405063629 seconds\n",
      "epoch 64 takes 0.783338069916 seconds\n",
      "epoch 65 takes 0.770467042923 seconds\n",
      "epoch 66 takes 0.759374856949 seconds\n",
      "epoch 67 takes 0.778618097305 seconds\n",
      "epoch 68 takes 0.760077953339 seconds\n",
      "epoch 69 takes 0.758652925491 seconds\n",
      "epoch 70 takes 0.782949924469 seconds\n",
      "epoch 71 takes 0.779992103577 seconds\n",
      "epoch 72 takes 0.729696035385 seconds\n",
      "epoch 73 takes 0.758777141571 seconds\n",
      "epoch 74 takes 0.749260902405 seconds\n",
      "epoch 75 takes 0.769949197769 seconds\n",
      "epoch 76 takes 0.774475812912 seconds\n",
      "epoch 77 takes 0.748514175415 seconds\n",
      "epoch 78 takes 0.758496046066 seconds\n",
      "epoch 79 takes 0.7651450634 seconds\n",
      "epoch 80 takes 0.772629976273 seconds\n",
      "epoch 81 takes 0.756895065308 seconds\n",
      "epoch 82 takes 0.754243135452 seconds\n",
      "epoch 83 takes 0.746798992157 seconds\n",
      "epoch 84 takes 0.753700017929 seconds\n",
      "epoch 85 takes 0.759181022644 seconds\n",
      "epoch 86 takes 0.747094154358 seconds\n",
      "epoch 87 takes 0.767359972 seconds\n",
      "epoch 88 takes 0.768051862717 seconds\n",
      "epoch 89 takes 0.772391080856 seconds\n",
      "epoch 90 takes 0.79402422905 seconds\n",
      "epoch 91 takes 0.765033006668 seconds\n",
      "epoch 92 takes 0.747215986252 seconds\n",
      "epoch 93 takes 0.747746944427 seconds\n",
      "epoch 94 takes 0.747491836548 seconds\n",
      "epoch 95 takes 0.783753871918 seconds\n",
      "epoch 96 takes 0.758141040802 seconds\n",
      "epoch 97 takes 0.762392997742 seconds\n",
      "epoch 98 takes 0.764147043228 seconds\n",
      "epoch 99 takes 0.75249004364 seconds\n",
      "train phase finished\n"
     ]
    }
   ],
   "source": [
    "# Initial Params\n",
    "Params = {}\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K1'], 1]) / Setting['K1']\n",
    "\n",
    "# Params['Gamma']  = 0.1 * np.ones([Setting['K1'], Setting['K1_S1'], Setting['K1_S2']])\n",
    "# 转化为GPU的输入形式\n",
    "\n",
    "W_train = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "# Gibbs\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #==========================增广==========================＃\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])  # 增广矩阵用于更新s34维度上增广\n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1']) # 增广矩阵用于更新s12维度上增广\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32')\n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1  # padding所以加１\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "\n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape, dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "\n",
    "    # 第一层增广的结果\n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64') # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')  # K1*S3*S4\n",
    "    \n",
    "    # 这块取不取np.round很随意，取round相对近似的结果会差一些\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3), axis=2) # N*K1\n",
    "    #==========================采样==========================＃\n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        # update 1th D\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] # 按三四维增广的矩阵\n",
    "        D1_k1_s = (X_k1_34 + SuperParams['eta']) / np.sum(X_k1_34 + SuperParams['eta'])\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "    \n",
    "#     Params['p2_n_aug'] = np.sum(np.sum(np.sum(Params['W1_nk1_Aug'],axis=3),axis=2),axis=1)\n",
    "#     Params['p2_n']     = np.random.beta(SuperParams['a0'] + Params['p2_n_aug'], np.sum(Params['Gamma']) + SuperParams['b0'])\n",
    "#     Params['c2_n']     = (1 - Params['p2_n']) / Params['p2_n']\n",
    "\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    W_k1_sn = np.random.gamma(Params['W1_nk1_Aug_Pooling'].T + Params['Gamma']) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn)  # N*K1\n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        \n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "\n",
    "#     for k1 in range(Setting['K1']):\n",
    "#         # Multi\n",
    "#         X_k1_n12 = np.reshape(Params['W1_nk1_Aug'][:, k1, :, :], [Setting['N_train'], Setting['K1_S1'] * Setting['K1_S2']])\n",
    "#         X_k1_12n = np.transpose(X_k1_n12) # s12*N\n",
    "        \n",
    "#         # update 1th W\n",
    "#         W_k1_sn = np.random.gamma(np.transpose(Params['Gamma'][k1,:,:]) + X_k1_12n) / (1 + Params['c2_n']) # 假设r_k都相同\n",
    "#         Params['W1_nk1'][:, k1, :, :] = np.reshape(np.transpose(W_k1_sn), [Setting['N_train'], Setting['K1_S1'], Setting['K1_S2']])\n",
    " \n",
    "    end_time = time.time()\n",
    "\n",
    "#     if t == 0:\n",
    "#         Iter_time.append(end_time - start_time)\n",
    "#     else:\n",
    "#         Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    # Likelyhood\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "    if t >=Setting['Burinin']:\n",
    "        \n",
    "        W_train = W_train + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])        \n",
    "    \n",
    "#     if np.mod(t,100) == 0:\n",
    "        \n",
    "#         likelyhood = 0\n",
    "#         start_time = time.time()\n",
    "#         Orgin_X = np.zeros([Setting['N_train'], Setting['K1_V1'], Setting['K1_V2']])\n",
    "#         Orgin_X[[batch_file_index, batch_rows, batch_cols+1]] = batch_value\n",
    "\n",
    "#         for i in range(Setting['N_train']):\n",
    "            \n",
    "#             Phi_tmp = np.transpose(np.reshape(Params['D1_k1'],[Setting['K1'], Setting['K1_S3'], Setting['K1_S4'], 1]),[1,2,3,0])\n",
    "#             Theta_tmp = np.transpose(Params['W1_nk1'][i:i+1,:,:,:], [0,2,3,1])\n",
    "#             PhiTheta_1= sess.run(X_1, feed_dict={Phi_1:Phi_tmp.astype(np.float32), Theta_1:Theta_tmp.astype(np.float32)})\n",
    "\n",
    "#             likelyhood = likelyhood + np.sum(Orgin_X[i,:,:] * log_max(PhiTheta_1[0,:,:,0]) - PhiTheta_1[0,:,:,0] - log_max(gamma(Orgin_X[i,:,:] + 1)))  \n",
    "#         end_time = time.time()\n",
    "#         print \"Likelihood \" + str(likelyhood / Setting['N_train']) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "#         Iter_lh.append(likelyhood / Setting['N_train'])\n",
    "\n",
    "            \n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(Iter_time ,Iter_lh)\n",
    "# plt.show()       \n",
    "\n",
    "# import numpy as np\n",
    "# import scipy.io as sio    \n",
    "        \n",
    "    \n",
    "# sio.savemat('TREC_Layer1.mat',{'Time_Layer1':Iter_time, 'LH_Layer1':Iter_lh})\n",
    "\n",
    "# print 'save mat finished'\n",
    "        \n",
    "print \"train phase finished\" \n",
    "W_train =  W_train / Setting['Collection'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "f = open('Vab_3-14.txt','w')\n",
    "Vab = ['ukn'] + TREC['Vocabulary']\n",
    "for i in range(Params['D1_k1'].shape[0]):\n",
    "    i_str = str(i) + 'th kernel\\n'\n",
    "    f.write(i_str)\n",
    "    for j in range(Params['D1_k1'].shape[2]):\n",
    "        j_str = str(j) + 'th Volumn: '\n",
    "        f.write(j_str)\n",
    "        topic = Params['D1_k1'][i,:,j]\n",
    "        top_index = np.argsort(-topic)\n",
    "        for k in range(5):\n",
    "            f.write('  ' + Vab[top_index[k]])\n",
    "            \n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess finished\n",
      "epoch 0 takes 0.112673044205 seconds\n",
      "epoch 1 takes 0.110381126404 seconds\n",
      "epoch 2 takes 0.109769821167 seconds\n",
      "epoch 3 takes 0.0994350910187 seconds\n",
      "epoch 4 takes 0.101398944855 seconds\n",
      "epoch 5 takes 0.103450059891 seconds\n",
      "epoch 6 takes 0.107084035873 seconds\n",
      "epoch 7 takes 0.101216077805 seconds\n",
      "epoch 8 takes 0.1047270298 seconds\n",
      "epoch 9 takes 0.100533008575 seconds\n",
      "epoch 10 takes 0.104786157608 seconds\n",
      "epoch 11 takes 0.103674888611 seconds\n",
      "epoch 12 takes 0.1234998703 seconds\n",
      "epoch 13 takes 0.127584934235 seconds\n",
      "epoch 14 takes 0.107851028442 seconds\n",
      "epoch 15 takes 0.106584072113 seconds\n",
      "epoch 16 takes 0.120259046555 seconds\n",
      "epoch 17 takes 0.119405031204 seconds\n",
      "epoch 18 takes 0.139102935791 seconds\n",
      "epoch 19 takes 0.1322889328 seconds\n",
      "epoch 20 takes 0.144352912903 seconds\n",
      "epoch 21 takes 0.566050052643 seconds\n",
      "epoch 22 takes 0.126497030258 seconds\n",
      "epoch 23 takes 0.110385894775 seconds\n",
      "epoch 24 takes 0.100441932678 seconds\n",
      "epoch 25 takes 0.101525068283 seconds\n",
      "epoch 26 takes 0.104191064835 seconds\n",
      "epoch 27 takes 0.102304935455 seconds\n",
      "epoch 28 takes 0.104870080948 seconds\n",
      "epoch 29 takes 0.102103948593 seconds\n",
      "epoch 30 takes 0.102854967117 seconds\n",
      "epoch 31 takes 0.103306055069 seconds\n",
      "epoch 32 takes 0.104887962341 seconds\n",
      "epoch 33 takes 0.102810144424 seconds\n",
      "epoch 34 takes 0.105578899384 seconds\n",
      "epoch 35 takes 0.10604095459 seconds\n",
      "epoch 36 takes 0.102194070816 seconds\n",
      "epoch 37 takes 0.102128982544 seconds\n",
      "epoch 38 takes 0.108277082443 seconds\n",
      "epoch 39 takes 0.104491949081 seconds\n",
      "epoch 40 takes 0.10422205925 seconds\n",
      "epoch 41 takes 0.103903055191 seconds\n",
      "epoch 42 takes 0.117422103882 seconds\n",
      "epoch 43 takes 0.13951921463 seconds\n",
      "epoch 44 takes 0.138409852982 seconds\n",
      "epoch 45 takes 0.127513885498 seconds\n",
      "epoch 46 takes 0.121065855026 seconds\n",
      "epoch 47 takes 0.19267988205 seconds\n",
      "epoch 48 takes 0.233927965164 seconds\n",
      "epoch 49 takes 0.146969795227 seconds\n",
      "epoch 50 takes 0.189285993576 seconds\n",
      "epoch 51 takes 0.218639135361 seconds\n",
      "epoch 52 takes 0.113265991211 seconds\n",
      "epoch 53 takes 0.101891994476 seconds\n",
      "epoch 54 takes 0.104408025742 seconds\n",
      "epoch 55 takes 0.099750995636 seconds\n",
      "epoch 56 takes 0.106156110764 seconds\n",
      "epoch 57 takes 0.104460000992 seconds\n",
      "epoch 58 takes 0.103428125381 seconds\n",
      "epoch 59 takes 0.103615045547 seconds\n",
      "epoch 60 takes 0.112711906433 seconds\n",
      "epoch 61 takes 0.115810155869 seconds\n",
      "epoch 62 takes 0.108597040176 seconds\n",
      "epoch 63 takes 0.103370904922 seconds\n",
      "epoch 64 takes 0.102277040482 seconds\n",
      "epoch 65 takes 0.104686021805 seconds\n",
      "epoch 66 takes 0.108803033829 seconds\n",
      "epoch 67 takes 0.108266115189 seconds\n",
      "epoch 68 takes 0.107808113098 seconds\n",
      "epoch 69 takes 0.105983972549 seconds\n",
      "epoch 70 takes 0.121278047562 seconds\n",
      "epoch 71 takes 0.127533912659 seconds\n",
      "epoch 72 takes 0.117066860199 seconds\n",
      "epoch 73 takes 0.110488891602 seconds\n",
      "epoch 74 takes 0.125054836273 seconds\n",
      "epoch 75 takes 0.394994020462 seconds\n",
      "epoch 76 takes 0.129449129105 seconds\n",
      "epoch 77 takes 0.120931148529 seconds\n",
      "epoch 78 takes 0.123922109604 seconds\n",
      "epoch 79 takes 0.337483882904 seconds\n",
      "epoch 80 takes 0.11066699028 seconds\n",
      "epoch 81 takes 0.10155916214 seconds\n",
      "epoch 82 takes 0.104401111603 seconds\n",
      "epoch 83 takes 0.103068828583 seconds\n",
      "epoch 84 takes 0.103610992432 seconds\n",
      "epoch 85 takes 0.102066993713 seconds\n",
      "epoch 86 takes 0.108726978302 seconds\n",
      "epoch 87 takes 0.103787899017 seconds\n",
      "epoch 88 takes 0.106369972229 seconds\n",
      "epoch 89 takes 0.101909160614 seconds\n",
      "epoch 90 takes 0.107647895813 seconds\n",
      "epoch 91 takes 0.106207847595 seconds\n",
      "epoch 92 takes 0.109957933426 seconds\n",
      "epoch 93 takes 0.104115962982 seconds\n",
      "epoch 94 takes 0.108507156372 seconds\n",
      "epoch 95 takes 0.119014978409 seconds\n",
      "epoch 96 takes 0.119109153748 seconds\n",
      "epoch 97 takes 0.112155199051 seconds\n",
      "epoch 98 takes 0.115760087967 seconds\n",
      "epoch 99 takes 0.145358800888 seconds\n",
      "test phase finished\n"
     ]
    }
   ],
   "source": [
    "#=============preprocess==============#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_test_list)): # 所有样本遍历\n",
    "    \n",
    "    x_single = np.reshape(data_test_list_index[i], [len(data_test_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_test_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0)\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_test_label[i]])),axis=0)\n",
    "\n",
    "batch_len_te        = batch_len\n",
    "batch_rows_te       = batch_rows\n",
    "batch_cols_te       = batch_cols\n",
    "batch_file_index_te = batch_file_index\n",
    "batch_value_te      = batch_value\n",
    "batch_label_te      = batch_label\n",
    "\n",
    "print 'preprocess finished'\n",
    "\n",
    "Setting['N_test']   = len(batch_len) #大于等于10的2194个样本        \n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_test'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_test']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "W_test = np.zeros([Setting['N_test'], Setting['K1']])\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K1'], 1]) / Setting['K1']\n",
    "\n",
    "# Gibbs\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #==========================增广==========================＃\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])  # 增广矩阵用于更新s34维度上增广\n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1']) # 增广矩阵用于更新s12维度上增广\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32')\n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32') + 1\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "\n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1        = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1         = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug    = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug     = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "\n",
    "    # 第一层增广的结果\n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64') # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')  # K1*S3*S4\n",
    "    \n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3), axis=2) # N*K1\n",
    "    \n",
    "    #==========================采样==========================＃\n",
    "    \n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "#     Params['p2_n_aug'] = np.sum(np.sum(np.sum(Params['W1_nk1_Aug'],axis=3),axis=2),axis=1)\n",
    "#     Params['p2_n']     = np.random.beta(SuperParams['a0'] + Params['p2_n_aug'], np.sum(Params['Gamma']) + SuperParams['b0'])\n",
    "#     Params['c2_n']     = (1 - Params['p2_n']) / Params['p2_n']\n",
    "    \n",
    "    W_k1_sn = np.random.gamma(Params['W1_nk1_Aug_Pooling'].T + Params['Gamma']) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn)  # N*K1\n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        \n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "\n",
    "#     for k1 in range(Setting['K1']):\n",
    "#         # Multi\n",
    "#         X_k1_n12 = np.reshape(Params['W1_nk1_Aug'][:, k1, :, :], [Setting['N_test'], Setting['K1_S1'] * Setting['K1_S2']])\n",
    "#         X_k1_12n = np.transpose(X_k1_n12) # s12*N\n",
    "        \n",
    "#         # update 1th W\n",
    "#         W_k1_sn = np.random.gamma(np.transpose(Params['Gamma'][k1,:,:]) + X_k1_12n) / (1 + Params['c2_n']) # 假设r_k都相同\n",
    "#         Params['W1_nk1'][:, k1, :, :] = np.reshape(np.transpose(W_k1_sn), [Setting['N_test'], Setting['K1_S1'], Setting['K1_S2']])\n",
    " \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Likelyhood\n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "    if t >=Setting['Burinin']:\n",
    "        \n",
    "        W_test = W_test + np.sum(Params['W1_nk1'][:,:,0,:],axis=2) / np.reshape(batch_len, [batch_len.shape[0], 1])\n",
    "        \n",
    "#     if np.mod(t+1,100) == 0:\n",
    "        \n",
    "#         likelyhood = 0\n",
    "#         start_time = time.time()\n",
    "#         Orgin_X = np.zeros([Setting['N_test'], Setting['K1_V1'], Setting['K1_V2']])\n",
    "#         Orgin_X[[batch_file_index_te, batch_rows_te, batch_cols_te+1]] = batch_value_te\n",
    "\n",
    "#         for i in range(Setting['N_test']):\n",
    "            \n",
    "#             Phi_tmp = np.transpose(np.reshape(Params['D1_k1'],[Setting['K1'], Setting['K1_S3'], Setting['K1_S4'], 1]),[1,2,3,0])\n",
    "#             Theta_tmp = np.transpose(Params['W1_nk1'][i:i+1,:,:,:], [0,2,3,1])\n",
    "#             PhiTheta_1= sess.run(X_1, feed_dict={Phi_1:Phi_tmp.astype(np.float64), Theta_1:Theta_tmp.astype(np.float64)})\n",
    "\n",
    "#             likelyhood = likelyhood + np.sum(Orgin_X[i,:,:] * log_max(PhiTheta_1[0,:,:,0]) - PhiTheta_1[0,:,:,0] - log_max(gamma(Orgin_X[i,:,:] + 1)))  \n",
    "#         end_time = time.time()\n",
    "#         print \"Likelihood \" + str(likelyhood / Setting['N_test']) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "\n",
    "    \n",
    "print \"test phase finished\"\n",
    "W_test =  W_test / Setting['Collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967351430667645\n",
      "0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# W_train = (W_train - np.reshape(np.mean(W_train,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_train, axis=0),[1,Setting['K1']])\n",
    "# W_test = (W_test - np.reshape(np.mean(W_test,axis=0),[1,Setting['K1']])) / np.reshape(np.std(W_test, axis=0),[1,Setting['K1']])\n",
    "\n",
    "\n",
    "clf = svm.SVC()                    # class\n",
    "clf.fit(W_train, batch_label_tr)            # training the svc model \n",
    "\n",
    "print clf.score(W_train, batch_label_tr)    # training the svc model \n",
    "print clf.score(W_test,  batch_label_te)    # training the svc model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48332183894378405"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.std([68.6,68.4,66.6,69,68.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
