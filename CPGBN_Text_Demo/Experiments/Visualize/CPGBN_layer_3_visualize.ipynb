{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "Preprocess finished\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Tensorflow initial finished\n",
      "CUDA initial finish\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "# checked  Chaojie Wang 2018-8-3\n",
    "\"\"\"\n",
    "Created on Wed Jan 10 22:41:31 2018\n",
    "\n",
    "@author: wangchaojie\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.RandomState(1)\n",
    "\n",
    "realmin = 2.2e-10\n",
    "def log_max(x):\n",
    "    return np.log(np.maximum(x, realmin))\n",
    "\n",
    "#====================== Load data ======================#\n",
    "import cPickle\n",
    "\n",
    "DATA = cPickle.load(open(\"./TREC_8k.pkl\",\"r\"))\n",
    "\n",
    "data_vab_list          = DATA['Vocabulary']\n",
    "data_vab_count_list    = DATA['Vab_count']\n",
    "data_vab_length        = DATA['Vab_Size']\n",
    "data_label             = DATA['Label']\n",
    "data_train_list        = DATA['Train_Origin']\n",
    "data_train_label       = np.array(DATA['Train_Label'])\n",
    "data_train_split       = DATA['Train_Word_Split']\n",
    "data_train_list_index  = DATA['Train_Word2Index']\n",
    "data_test_list         = DATA['Test_Origin']\n",
    "data_test_label        = np.array(DATA['Test_Label'])\n",
    "data_test_split        = DATA['Test_Word_Split']\n",
    "data_test_list_index   = DATA['Test_Word2Index']\n",
    "data_value             = 50\n",
    "\n",
    "print 'Load data'\n",
    "\n",
    "#======================= Preprocess =======================#\n",
    "delete_count = 0\n",
    "\n",
    "for i in range(len(data_train_list)):\n",
    "    \n",
    "    x_single = np.reshape(data_train_list_index[i], [len(data_train_list_index[i])]).astype(np.int32)\n",
    "    x_len    = x_single.shape[0]\n",
    "    \n",
    "    if x_len < 4:\n",
    "        delete_count = delete_count + 1\n",
    "        continue\n",
    "        \n",
    "    i_index = i - delete_count\n",
    "    if i_index == 0:\n",
    "        batch_len  = np.array([x_len])\n",
    "        batch_rows = x_single\n",
    "        batch_cols = np.arange(x_len)\n",
    "        batch_file_index = np.ones_like(x_single) * i_index\n",
    "        batch_value      = np.ones_like(x_single) * data_value\n",
    "        batch_label      = np.array([data_train_label[i]])\n",
    "    else:\n",
    "        batch_len  = np.concatenate((batch_len, np.array([x_len])), axis=0)\n",
    "        batch_rows = np.concatenate((batch_rows, x_single), axis=0)\n",
    "        batch_cols = np.concatenate((batch_cols, np.arange(x_len)), axis = 0)\n",
    "        batch_file_index = np.concatenate((batch_file_index, np.ones_like(x_single) * i_index), axis=0)\n",
    "        batch_value      = np.concatenate((batch_value, np.ones_like(x_single) * data_value), axis=0)\n",
    "        batch_label      = np.concatenate((batch_label,np.array([data_train_label[i]])),axis=0)\n",
    "        \n",
    "print 'Preprocess finished'\n",
    "\n",
    "batch_len_tr        = batch_len\n",
    "batch_rows_tr       = batch_rows\n",
    "batch_cols_tr       = batch_cols\n",
    "batch_file_index_tr = batch_file_index\n",
    "batch_value_tr      = batch_value\n",
    "batch_label_tr      = batch_label\n",
    "\n",
    "#======================= Setting =======================#\n",
    "Setting = {}\n",
    "Setting['N_train'] = len(data_train_list) - delete_count\n",
    "Setting['N_test']  = len(data_test_list)\n",
    "# 1-th layer\n",
    "Setting['K1']      = 200\n",
    "Setting['K1_V1']   = DATA['Vab_Size']\n",
    "Setting['K1_V2']   = np.max(batch_len)\n",
    "Setting['K1_S3']   = DATA['Vab_Size']\n",
    "Setting['K1_S4']   = 3\n",
    "Setting['K1_S1']   = Setting['K1_V1'] + 1 - Setting['K1_S3']\n",
    "Setting['K1_S2']   = Setting['K1_V2'] + 1 - Setting['K1_S4']\n",
    "# 2-th layer\n",
    "Setting['K2']      = 100\n",
    "Setting['K2_V1']   = Setting['K1_S1']\n",
    "Setting['K2_V2']   = Setting['K1_S2']\n",
    "Setting['K2_S3']   = 1\n",
    "Setting['K2_S4']   = 3\n",
    "Setting['K2_S1']   = Setting['K2_V1'] + 1 - Setting['K2_S3']\n",
    "Setting['K2_S2']   = Setting['K2_V2'] + 1 - Setting['K2_S4']\n",
    "# 3-th layer\n",
    "Setting['K3']      = 50\n",
    "Setting['K3_V1']   = Setting['K2_S1']\n",
    "Setting['K3_V2']   = Setting['K2_S2']\n",
    "Setting['K3_S3']   = 1\n",
    "Setting['K3_S4']   = 3\n",
    "Setting['K3_S1']   = Setting['K3_V1'] + 1 - Setting['K3_S3']\n",
    "Setting['K3_S2']   = Setting['K3_V2'] + 1 - Setting['K3_S4']\n",
    "\n",
    "Setting['Iter']       = 500\n",
    "Setting['Burinin']    = 0.75*Setting['Iter']\n",
    "Setting['Collection'] = Setting['Iter'] - Setting['Burinin']\n",
    "\n",
    "#======================= SuperParams =======================#\n",
    "SuperParams = {}\n",
    "SuperParams['gamma0'] = 0.1  # r\n",
    "SuperParams['c0']     = 0.1\n",
    "SuperParams['a0']     = 0.1  # p\n",
    "SuperParams['b0']     = 0.1  \n",
    "SuperParams['e0']     = 0.1  # c\n",
    "SuperParams['f0']     = 0.1\n",
    "SuperParams['eta']    = 0.05 # Phi\n",
    "\n",
    "#======================= Tensorflow Initial =======================#\n",
    "import tensorflow as tf\n",
    "# H*W*Outchannel*Inchannel\n",
    "Phi_1   = tf.placeholder(tf.float32, shape = [Setting['K1_S3'], Setting['K1_S4'], 1, Setting['K1']]) #HWC\n",
    "# N*H*W*Inchannel\n",
    "Theta_1 = tf.placeholder(tf.float32, shape = [1, Setting['K1_S1'], Setting['K1_S2'], Setting['K1']])\n",
    "# Outshape N*H*W*Outchannel\n",
    "X_1     = tf.nn.conv2d_transpose(Theta_1, Phi_1, output_shape=[1, Setting['K1_V1'], Setting['K1_V2'], 1], strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# Initial\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print 'Tensorflow initial finished'\n",
    "\n",
    "#====================== CUDA Initial ======================#\n",
    "# Note， do not add any cuda operation among CUDA initial such as Tensorflow!!!!!!!!!!!!!!!!!!\n",
    "import pycuda.curandom as curandom\n",
    "import pycuda.driver as drv\n",
    "import pycuda.tools\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <stdio.h>\n",
    "__global__ void Multi_Sampler(int* para, float *word_aug_stack, float *MultRate_stack, int *row_index, int *column_index, int *page_index, float *value_index, float *Params_W1_nk1, float *Params_D1_k1, float *Params_W1_nk1_Aug, float *Params_D1_k1_Aug)\n",
    "{\n",
    "    int K1         = para[0];\n",
    "    int K1_K1      = para[1];\n",
    "    int K1_K2      = para[2];\n",
    "    int K1_K3      = para[3];\n",
    "    int K1_K4      = para[4];\n",
    "    int word_total = para[5];\n",
    "\n",
    "    int ix = blockDim.x * blockIdx.x + threadIdx.x; \n",
    "    int iy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    unsigned int idx = iy* blockDim.x *gridDim.x+ ix;\n",
    "    \n",
    "    if ((idx < word_total))\n",
    "    {\n",
    "        int v1 = row_index[idx];                 // row_index\n",
    "        int v2 = column_index[idx];              // col_index\n",
    "        int n  = page_index[idx];                // file_index\n",
    "        float value = value_index[idx];\n",
    "        \n",
    "        int word_k1_min = 0;\n",
    "        int word_k1_max = 0;\n",
    "        int word_k2_min = 0;\n",
    "        int word_k2_max = 0;\n",
    "        \n",
    "        // word_k1\n",
    "        if ((v1 - K1_K3 + 1) > 0)\n",
    "            word_k1_min = v1 - K1_K3 + 1;\n",
    "        else\n",
    "            word_k1_min = 0;\n",
    "\n",
    "        if (v1 > K1_K1 -1)\n",
    "            word_k1_max = K1_K1 -1;\n",
    "        else\n",
    "            word_k1_max = v1;\n",
    "\n",
    "        int l_word_k1 = word_k1_max - word_k1_min + 1;\n",
    "        int *word_k1  = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k1[i] = word_k1_min + i;\n",
    "\n",
    "        // word_k2\n",
    "        if ((v2 - K1_K4 + 1) > 0)\n",
    "            word_k2_min = v2 - K1_K4 + 1;\n",
    "        else\n",
    "            word_k2_min = 0;\n",
    "\n",
    "        if (v2 > K1_K2 -1)\n",
    "            word_k2_max = K1_K2 -1;\n",
    "        else\n",
    "            word_k2_max = v2;\n",
    "\n",
    "        int l_word_k2 = word_k2_max - word_k2_min + 1;\n",
    "        int *word_k2  = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k2[i] = word_k2_min + i;\n",
    "\n",
    "        // word_k3\n",
    "        int *word_k3 = new int[l_word_k1];\n",
    "        for (int i = 0; i < (l_word_k1); i++)\n",
    "            word_k3[i] = v1 - word_k1[i] ;\n",
    "\n",
    "        // word_k4\n",
    "        int *word_k4 = new int[l_word_k2];\n",
    "        for (int i = 0; i < (l_word_k2); i++)\n",
    "            word_k4[i] = v2 - word_k2[i] ;\n",
    "        \n",
    "        float MultRate_sum = 0;\n",
    "        //word_aug_stack\n",
    "        //MultRate_stack\n",
    "        //Params_W1_nk1\n",
    "        //Params_D1_k1\n",
    "        int stack_start = idx * K1_K4 * K1;\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    MultRate_stack[temp_c] = Params_W1_nk1[temp_a] * Params_D1_k1[temp_b];\n",
    "                    MultRate_sum = MultRate_sum + MultRate_stack[temp_c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for (int i = 0; i < K1; i++)\n",
    "        {\n",
    "            for (int k = 0; k < (l_word_k1); k++)\n",
    "            {\n",
    "                for (int j = 0; j < (l_word_k2); j++)\n",
    "                {\n",
    "                    int temp_a = (n) * K1 * K1_K1 * K1_K2 + (i) * K1_K1 * K1_K2 + word_k1[k] * K1_K2 + (word_k2[j]);\n",
    "                    int temp_b = (i) * K1_K3 * K1_K4 + word_k3[k] * K1_K4 + (word_k4[j]);\n",
    "                    int temp_c = stack_start + i*l_word_k1*l_word_k2 + k*l_word_k2 + j;\n",
    "                    \n",
    "                    if (MultRate_sum == 0)\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = 1.0 / (K1 * l_word_k1 * l_word_k2);\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "                    else\n",
    "                    {\n",
    "                        MultRate_stack[temp_c] = MultRate_stack[temp_c] / MultRate_sum;\n",
    "                        word_aug_stack[temp_c] = MultRate_stack[temp_c] * value;\n",
    "                    }\n",
    "\n",
    "                    atomicAdd(&Params_W1_nk1_Aug[temp_a], word_aug_stack[temp_c]);\n",
    "                    atomicAdd(&Params_D1_k1_Aug[temp_b], word_aug_stack[temp_c]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        delete[] word_k1;\n",
    "        delete[] word_k2;\n",
    "        delete[] word_k3;\n",
    "        delete[] word_k4; \n",
    "    }\n",
    "    \n",
    "}\n",
    " \"\"\")\n",
    "print \"CUDA initial finish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 takes 2.32822990417 seconds\n",
      "epoch 1 takes 2.11338686943 seconds\n",
      "epoch 2 takes 2.20896601677 seconds\n",
      "epoch 3 takes 2.21668982506 seconds\n",
      "epoch 4 takes 2.21355509758 seconds\n",
      "epoch 5 takes 2.14261198044 seconds\n",
      "epoch 6 takes 2.19739079475 seconds\n",
      "epoch 7 takes 2.21278810501 seconds\n",
      "epoch 8 takes 2.22491288185 seconds\n",
      "epoch 9 takes 2.32152795792 seconds\n",
      "epoch 10 takes 3.1936480999 seconds\n",
      "epoch 11 takes 2.59178590775 seconds\n",
      "epoch 12 takes 2.55431294441 seconds\n",
      "epoch 13 takes 2.20819497108 seconds\n",
      "epoch 14 takes 2.03696703911 seconds\n",
      "epoch 15 takes 2.09911108017 seconds\n",
      "epoch 16 takes 2.1425819397 seconds\n",
      "epoch 17 takes 2.18455719948 seconds\n",
      "epoch 18 takes 2.23447203636 seconds\n",
      "epoch 19 takes 2.05522489548 seconds\n",
      "epoch 20 takes 2.11959600449 seconds\n",
      "epoch 21 takes 2.08912682533 seconds\n",
      "epoch 22 takes 2.09787797928 seconds\n",
      "epoch 23 takes 2.92186903954 seconds\n",
      "epoch 24 takes 2.79774999619 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f9c8793abe18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mTheta3_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Theta_3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mXt_to_t1_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWSZS_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPGBN_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrt_Multirnd_Matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM2_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Phi_3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTheta3_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m#====================== Parameters Update ======================#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/xiaosucheng/Ubuntu 16.0/other/ICML_code/Code_Published/CPGBN_Text_Demo/Experiments/Visualize/PGBN_sampler.pyc\u001b[0m in \u001b[0;36mCrt_Multirnd_Matrix\u001b[0;34m(Xt_to_t1_t, Phi_t1, Theta_t1)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mWSZS_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mKt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mKt1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'double'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mCrt_Multi_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrt_Multi_Sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt_to_t1_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPhi_t1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTheta_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWSZS_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt_to_t1_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKt1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#======================= Initial Params =======================#\n",
    "import PGBN_sampler \n",
    "from scipy.special import gamma\n",
    "Params = {}\n",
    "\n",
    "# 1-th layer\n",
    "Params['D1_k1'] = np.random.rand(Setting['K1'], Setting['K1_S3'], Setting['K1_S4'])\n",
    "for k1 in range(Setting['K1']):\n",
    "    Params['D1_k1'][k1, :, :] = Params['D1_k1'][k1, :, :] / np.sum(Params['D1_k1'][k1, :, :])\n",
    "Params['W1_nk1'] = np.random.rand(Setting['N_train'], Setting['K1'], Setting['K1_S1'], Setting['K1_S2'])\n",
    "Params['W1_nk1_Pooling'] = np.sum(np.sum(Params['W1_nk1'], axis=3), axis=2)\n",
    "\n",
    "Params['c2_n']   = 1 * np.ones([Setting['N_train']])\n",
    "Params['p2_n']   = 1 / (1 + Params['c2_n'])\n",
    "\n",
    "# 2-th layer\n",
    "Params['Phi_2']  = 0.2 + 0.8*np.random.rand(Setting['K1'], Setting['K2'])\n",
    "Params['Phi_2']  = Params['Phi_2'] / np.sum(Params['Phi_2'], axis=0)\n",
    "Params['Theta_2']= np.random.rand(Setting['N_train'], Setting['K2'])\n",
    "\n",
    "Params['c3_n']   = 1 * np.ones([Setting['N_train']])\n",
    "tmp = -log_max(1 - Params['p2_n'])\n",
    "Params['p3_n']   = (tmp / (tmp + Params['c3_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "# 3-th layer\n",
    "Params['Phi_3']  = 0.2 + 0.8*np.random.rand(Setting['K2'], Setting['K3'])\n",
    "Params['Phi_3']  = Params['Phi_3'] / np.sum(Params['Phi_3'], axis=0)\n",
    "Params['Theta_3']= np.random.rand(Setting['N_train'], Setting['K3'])\n",
    "\n",
    "Params['c4_n']   = 1 * np.ones([Setting['N_train']])\n",
    "tmp = -log_max(1 - Params['p3_n'])\n",
    "Params['p4_n']   = (tmp / (tmp + Params['c4_n']))                # pj_3 - pj_T+1\n",
    "\n",
    "Params['Gamma']  = np.ones([Setting['K3'], 1]) / Setting['K3']\n",
    "\n",
    "# Collection\n",
    "W_train_1 = np.zeros([Setting['N_train'], Setting['K1']])\n",
    "W_train_2 = np.zeros([Setting['N_train'], Setting['K2']])\n",
    "W_train_3 = np.zeros([Setting['N_train'], Setting['K3']])\n",
    "\n",
    "# CUDA function\n",
    "fuc = mod.get_function(\"Multi_Sampler\")\n",
    "\n",
    "import time\n",
    "Iter_time = []\n",
    "Iter_lh   = []\n",
    "\n",
    "#========================== Gibbs ==========================＃\n",
    "for t in range(Setting['Iter']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #========================== 1st layer Augmentation ==========================＃\n",
    "    Params['D1_k1_Aug']  = np.zeros_like(Params['D1_k1'])  # Augmentation on D\n",
    "    Params['W1_nk1_Aug'] = np.zeros_like(Params['W1_nk1']) # Augmentation on w\n",
    "    \n",
    "    X_rows       = np.array(batch_rows, dtype = 'int32') \n",
    "    X_cols       = np.array(batch_cols, dtype = 'int32')\n",
    "    X_file_index = np.array(batch_file_index, dtype = 'int32')\n",
    "    X_value      = np.array(batch_value, dtype = 'float32')\n",
    "    \n",
    "    word_total     = len(X_rows)\n",
    "    word_aug_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    MultRate_stack = np.zeros((Setting['K1']*Setting['K1_S4']*word_total),dtype=np.float32)\n",
    "    Batch_Para     = np.array([Setting['K1'], Setting['K1_S1'], Setting['K1_S2'], Setting['K1_S3'], Setting['K1_S4'], word_total], dtype=np.int32)\n",
    "    \n",
    "    block_x = 128\n",
    "    grid_x  = 128\n",
    "    grid_y  = word_total / (block_x * grid_x) + 1\n",
    "    \n",
    "    W1_nk1     = np.array(Params['W1_nk1'], dtype = 'float32', order='C')\n",
    "    D1_k1      = np.array(Params['D1_k1'], dtype = 'float32', order='C')\n",
    "    W1_nk1_Aug = np.zeros(W1_nk1.shape, dtype = 'float32', order='C')\n",
    "    D1_k1_Aug  = np.zeros(D1_k1.shape,dtype = 'float32', order='C')\n",
    "    \n",
    "    fuc(drv.In(Batch_Para), drv.In(word_aug_stack), drv.In(MultRate_stack), drv.In(X_rows), drv.In(X_cols), drv.In(X_file_index), drv.In(X_value), drv.In(W1_nk1), drv.In(D1_k1), drv.InOut(W1_nk1_Aug), drv.InOut(D1_k1_Aug), grid =(grid_x, grid_y, 1)  ,block=(block_x,1,1))   # 一般最多512个并行线程\n",
    "    \n",
    "    Params['W1_nk1_Aug'] = np.array(W1_nk1_Aug, dtype='float64') # N*K1*S1*S2\n",
    "    Params['D1_k1_Aug']  = np.array(D1_k1_Aug, dtype='float64')  # K1*S3*S4\n",
    "    Params['W1_nk1_Aug_Pooling'] = np.sum(np.sum(Params['W1_nk1_Aug'], axis=3),axis=2) # N*K1\n",
    "    \n",
    "    #========================== 2nd layer Augmentation ==========================＃\n",
    "    M1_tmp = np.array(np.transpose(np.round(Params['W1_nk1_Aug_Pooling'])), dtype='float64', order='C')\n",
    "    Theta2_tmp = np.array(np.transpose(Params['Theta_2']), dtype='float64', order='C')\n",
    "\n",
    "    Xt_to_t1_2,WSZS_2 = PGBN_sampler.Crt_Multirnd_Matrix(M1_tmp, Params['Phi_2'], Theta2_tmp)\n",
    "    \n",
    "    #========================== 3rd layer Augmentation ==========================＃\n",
    "    M2_tmp = np.array(np.round(Xt_to_t1_2), dtype='float64', order='C')\n",
    "    Theta3_tmp = np.array(np.transpose(Params['Theta_3']), dtype='float64', order='C')\n",
    "\n",
    "    Xt_to_t1_3,WSZS_3 = PGBN_sampler.Crt_Multirnd_Matrix(M2_tmp, Params['Phi_3'], Theta3_tmp)\n",
    "    \n",
    "    #====================== Parameters Update ======================#\n",
    "    # Update D,Phi\n",
    "    for k1 in range(Setting['K1']):\n",
    "        X_k1_34 = Params['D1_k1_Aug'][k1, :, :] \n",
    "        X_k1_34_tmp = np.random.gamma(X_k1_34 + SuperParams['eta'])\n",
    "        D1_k1_s     = X_k1_34_tmp / np.sum(X_k1_34_tmp, axis=0, keepdims=1)\n",
    "        Params['D1_k1'][k1, :, :] = D1_k1_s\n",
    "        \n",
    "    Phi_2_tmp       = np.random.gamma(WSZS_2 + SuperParams['eta'])\n",
    "    Params['Phi_2'] = Phi_2_tmp / np.sum(Phi_2_tmp, axis=0)\n",
    "    \n",
    "    Phi_3_tmp       = np.random.gamma(WSZS_3 + SuperParams['eta'])\n",
    "    Params['Phi_3'] = Phi_3_tmp / np.sum(Phi_3_tmp, axis=0)\n",
    "    \n",
    "    # Update c_j,p_j\n",
    "    Params['c2_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_2'], Params['Theta_2'].T),0)) \n",
    "    Params['c2_n']     = Params['c2_n'] / (SuperParams['f0'] + np.sum(Params['W1_nk1_Pooling'], axis=1))\n",
    "    Params['p2_n']     = 1 / (Params['c2_n'] + 1)\n",
    "    \n",
    "    Params['c3_n']     = np.random.gamma(SuperParams['e0'] + np.sum(np.dot(Params['Phi_3'], Params['Theta_3'].T),0)) \n",
    "    Params['c3_n']     = Params['c3_n'] / (SuperParams['f0'] + np.sum(Params['Theta_2'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p2_n'])\n",
    "    Params['p3_n']     = tmp / (Params['c3_n'] + tmp)\n",
    "    \n",
    "    Params['c4_n']     = np.random.gamma(SuperParams['e0'] + np.sum(Params['Gamma'])) \n",
    "    Params['c4_n']     = Params['c4_n'] / (SuperParams['f0'] + np.sum(Params['Theta_3'],axis=1)) \n",
    "    tmp = -log_max(1 - Params['p3_n'])\n",
    "    Params['p4_n']     = tmp / (Params['c4_n'] + tmp)\n",
    "    \n",
    "    # Update w_j\n",
    "    W_k3_sn = np.random.gamma(Params['Gamma'] + Xt_to_t1_3) / (-np.log(1-Params['p3_n']) + Params['c4_n']) # V*N\n",
    "    Params['Theta_3'] = np.transpose(W_k3_sn)\n",
    "    \n",
    "    shape2 = np.dot(Params['Phi_3'], Params['Theta_3'].T)\n",
    "    W_k2_sn = np.random.gamma(shape2 + Xt_to_t1_2) / (-np.log(1-Params['p2_n']) + Params['c3_n']) # V*N\n",
    "    Params['Theta_2'] = np.transpose(W_k2_sn)\n",
    "    \n",
    "    shape1 = np.dot(Params['Phi_2'], Params['Theta_2'].T) # V*N\n",
    "    W_k1_sn = np.random.gamma(shape1 + Params['W1_nk1_Aug_Pooling'].T ) / (1 + Params['c2_n']) # V*N\n",
    "    Params['W1_nk1_Pooling'] = np.transpose(W_k1_sn) \n",
    "    \n",
    "    for k1 in range(Setting['K1']):\n",
    "        Params['W1_nk1'][:, k1, 0, :] = (Params['W1_nk1_Aug'][:,k1,0,:] / (Params['W1_nk1_Aug_Pooling'][:, k1:k1+1] + 0.0001)) * Params['W1_nk1_Pooling'][:, k1:k1+1]\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if t == 0:\n",
    "        Iter_time.append(end_time - start_time)\n",
    "    else:\n",
    "        Iter_time.append(end_time - start_time + Iter_time[-1])\n",
    "    \n",
    "    print \"epoch \" + str(t) + \" takes \" + str(end_time - start_time) + \" seconds\"\n",
    "    \n",
    "print \"train phase finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualize Finished\n"
     ]
    }
   ],
   "source": [
    "#======================= Visualization =======================#\n",
    "# 1st layer\n",
    "f = open(\"./TREC_layer3_1.txt\", \"w\")\n",
    "for k in range(Setting['K1']):\n",
    "    #======================= kth topic of 1st layer =======================#\n",
    "    for i in range(Setting['K1_S4']):\n",
    "        a = np.argsort(-Params['D1_k1'][k, :, i])           \n",
    "        line = str(k)\n",
    "        for l in range(10):\n",
    "            if a[l] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                line = line + \" \" + data_vab_list[a[l]-1]\n",
    "        f.write(line)\n",
    "        f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "# 2nd layer\n",
    "f = open(\"./TREC_layer3_2.txt\", \"w\")\n",
    "x = np.dot(Params['Phi_2'].T, Params['D1_k1'].reshape(Setting['K1'], -1)).reshape(Setting['K2'], Setting['K1_V1'], Setting['K1_S4'])\n",
    "for k in range(Setting['K2']):\n",
    "    #======================= kth topic of 2nd layer =======================#\n",
    "    f.write(\"***************************\\n\")\n",
    "    f.write(\"layer2: \"+str(k)+\"th topic\\n\")\n",
    "    for i in range(Setting['K1_S4']):\n",
    "        a = np.argsort(-x[k, :, i])\n",
    "        line = \" \"\n",
    "        for l in range(10):\n",
    "            if a[l] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                line = line + \" \" + data_vab_list[a[l]-1]\n",
    "                \n",
    "        f.write(line)\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "    #======================= top 5 relevant topics of 1st layer =======================#\n",
    "    f.write(\"\\n\"+\"layer1: \\n\")\n",
    "    wei = Params['Phi_2'][:, k]\n",
    "    a = np.argsort(-wei)\n",
    "    for i in a[:5]:\n",
    "        f.write(str(i) + \" \" + str(wei[i]))\n",
    "        f.write(\"\\n\")\n",
    "        for j in range(Setting['K1_S4']):\n",
    "            b = np.argsort(-Params['D1_k1'][i, :, j])\n",
    "            line = \" \"\n",
    "            for l in range(10):\n",
    "                if b[l] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    line = line + \" \" + data_vab_list[b[l]-1]\n",
    "            \n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    f.write(\"***************************\\n\")\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "# 3rd lyaer\n",
    "f = open(\"./TREC_layer3_3.txt\", \"w\")\n",
    "x = np.dot(Params['Phi_2'].T, Params['D1_k1'].reshape(Setting['K1'], -1)).reshape(Setting['K2'], Setting['K1_V1'], Setting['K1_S4'])\n",
    "x1 = np.dot(Params['Phi_3'].T, x.reshape(Setting['K2'], -1)).reshape(Setting['K3'], Setting['K1_V1'], Setting['K1_S4'])\n",
    "for k in range(Setting['K3']):\n",
    "    #======================= kth topic of 3rd layer =======================#\n",
    "    f.write(\"***************************\\n\")\n",
    "    f.write(\"layer3: \"+str(k)+\"th topic\\n\")\n",
    "    for i in range(Setting['K1_S4']):\n",
    "        a = np.argsort(-x1[k, :, i])\n",
    "        line = \" \"\n",
    "        for l in range(10):\n",
    "            if a[l] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                line = line + \" \" + data_vab_list[a[l]-1]\n",
    "                \n",
    "        f.write(line)\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "    #======================= top 5 relevant topics of 2nd layer =======================#\n",
    "    f.write(\"\\n\"+\"layer2: \\n\")\n",
    "    wei = Params['Phi_3'][:, k]\n",
    "    a = np.argsort(-wei)\n",
    "    for i in a[:5]:\n",
    "        f.write(str(i) + \" \" + str(wei[i]))\n",
    "        f.write(\"\\n\")\n",
    "        for j in range(3):\n",
    "            b = np.argsort(-x[i, :, j])\n",
    "            line = \" \"\n",
    "            for l in range(10):\n",
    "                if b[l] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    line = line + \" \" + data_vab_list[b[l]-1]\n",
    "\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")  \n",
    "    f.write(\"***************************\\n\")\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "print \"Visualize Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
