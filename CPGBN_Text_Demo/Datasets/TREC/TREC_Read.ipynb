{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate vocabulary finished\n",
      "train word2index finished\n",
      "test word2index finished\n",
      "save finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "\n",
    "# clean content\n",
    "def clean_content(imdb_str):\n",
    "    \n",
    "    # imdb_str = ' ' + imdb_str + ' '\n",
    "    imdb_word_split = []\n",
    "    count_flag      = 0\n",
    "    imdb_str        = imdb_str.lower() # 大小写转换\n",
    "    for i in range(len(imdb_str) - 1):\n",
    "\n",
    "        if(imdb_str[i] is ' ')and(imdb_str[i+1] is not ' ')and(count_flag is 0):\n",
    "            start_index = i+1\n",
    "            count_flag  = 1\n",
    "\n",
    "        if(imdb_str[i] is not ' ')and(imdb_str[i+1] is ' ')and(count_flag is 1):\n",
    "            end_index = i+1\n",
    "            imdb_word_split.append(imdb_str[start_index:end_index])\n",
    "            count_flag = 0\n",
    "    return imdb_word_split\n",
    "\n",
    "# file\n",
    "train_file_name = 'train_5500.label.txt'\n",
    "train_file  = []\n",
    "train_label = []\n",
    "with open(train_file_name,'r') as f:  #注意open 和 codecs.opem\n",
    "    lines = f.readlines()\n",
    "    for i in range(len(lines)):\n",
    "        lines_clean = lines[i].rstrip('\\n')\n",
    "        cut_index   = lines_clean.find(':')\n",
    "        train_label.append(lines_clean[0:cut_index])\n",
    "        train_file.append(' ' + lines_clean[cut_index+1:])  # 末尾的?,.这些暂时去掉\n",
    "        \n",
    "test_file_name  = 'TREC_10.label.txt'\n",
    "test_file  = []\n",
    "test_label = []\n",
    "\n",
    "with open(test_file_name,'r') as f:  #注意open 和 codecs.opem\n",
    "    lines = f.readlines()\n",
    "    for i in range(len(lines)):\n",
    "        lines_clean = lines[i].rstrip('\\n')\n",
    "        cut_index   = lines_clean.find(':')\n",
    "        test_label.append(lines_clean[0:cut_index])\n",
    "        test_file.append(' ' + lines_clean[cut_index+1:])\n",
    "\n",
    "# label\n",
    "class_num = []\n",
    "for i in range(len(train_label)):\n",
    "    if train_label[i] not in class_num:\n",
    "        class_num.append(train_label[i])\n",
    "        \n",
    "trec_train_label = []\n",
    "trec_train_split = []\n",
    "for i in range(len(train_label)):\n",
    "    trec_train_label.append(class_num.index(train_label[i]))\n",
    "    trec_train_split.append(clean_content(train_file[i]))\n",
    "    \n",
    "trec_test_label = []\n",
    "trec_test_split = []\n",
    "for i in range(len(test_label)):\n",
    "    trec_test_label.append(class_num.index(test_label[i]))\n",
    "    trec_test_split.append(clean_content(test_file[i]))\n",
    "\n",
    "# calculate vab\n",
    "trec_train_list = train_file\n",
    "trec_test_list  = test_file\n",
    "trec_vab_count  = {}\n",
    "\n",
    "for i in range(len(trec_train_split)):\n",
    "    trec_str = trec_train_split[i]\n",
    "    for word_index in range(len(trec_str)):\n",
    "        if (not trec_vab_count.has_key(trec_str[word_index])):\n",
    "            trec_vab_count.update({trec_str[word_index]: 1})\n",
    "        else:\n",
    "            trec_vab_count[trec_str[word_index]] = trec_vab_count[trec_str[word_index]] + 1 \n",
    "                \n",
    "del_stop_word = False\n",
    "if del_stop_word is True:\n",
    "    # stop word list\n",
    "    stop_word_list = []\n",
    "    with open('/home/xiaosucheng/Desktop/IMDB/IMDB_Read_Data/imdb_stop_word.txt','r') as f:  #注意open 和 codecs.opem\n",
    "        line = f.readlines()\n",
    "        for i in range(len(line)):\n",
    "            stop_word_list.append(line[i].rstrip('\\n'))\n",
    "\n",
    "    for i in range(len(stop_word_list)):\n",
    "        if trec_vab_count.has_key(stop_word_list[i]):\n",
    "            trec_vab_count.pop(stop_word_list[i])\n",
    "    print 'delete stop word' \n",
    "\n",
    "# sort vab\n",
    "trec_vab                  = {}\n",
    "trec_vab_list_orgin       = trec_vab_count.keys()   # list\n",
    "trec_vab_count_list_orgin = trec_vab_count.values() # list\n",
    "trec_vab_list             = []\n",
    "trec_vab_count_list       = []\n",
    "\n",
    "trec_vab_index            = np.argsort(-np.array(trec_vab_count_list_orgin))  # np.argsort(-x) 降序排序\n",
    "for i in range(len(trec_vab_list_orgin)):\n",
    "    trec_vab.update({trec_vab_list_orgin[trec_vab_index[i]]: i + 1})    # 0 给stop_word　所以+1\n",
    "    trec_vab_list.append(trec_vab_list_orgin[trec_vab_index[i]])\n",
    "    trec_vab_count_list.append(trec_vab_count_list_orgin[trec_vab_index[i]])\n",
    "\n",
    "print 'calculate vocabulary finished'\n",
    "\n",
    "vab_size = 8000\n",
    "# word2index 如果不含有该词或者index超过vab_size,则置成０\n",
    "stop_flag = 1\n",
    "trec_train_list_index = []\n",
    "for i in range(len(trec_train_split)):\n",
    "    trec_train_list_index.append([-1])  # 添加一个新的list\n",
    "    trec_str = trec_train_split[i]      # 把对应的str取出来\n",
    "    for word_index in range(len(trec_str)):\n",
    "        if (not trec_vab.has_key(trec_str[word_index])) or (trec_vab[trec_str[word_index]] >= vab_size): # 出现为０的值\n",
    "            if (word_index > 0 and stop_flag is not 0):  # 连续为0的值只取一个0\n",
    "                trec_train_list_index[i].append(0)\n",
    "                stop_flag = 0\n",
    "        else:\n",
    "            trec_train_list_index[i].append(trec_vab[trec_str[word_index]])\n",
    "            stop_flag = 1\n",
    "    trec_train_list_index[i].pop(0)  # 删除第一个元素\n",
    "print 'train word2index finished'\n",
    "\n",
    "stop_flag = 1\n",
    "trec_test_list_index = []\n",
    "for i in range(len(trec_test_split)):\n",
    "    trec_test_list_index.append([-1])\n",
    "    trec_str = trec_test_split[i]\n",
    "    for word_index in range(len(trec_str)):\n",
    "        if (not trec_vab.has_key(trec_str[word_index])) or (trec_vab[trec_str[word_index]] >= vab_size):\n",
    "            if (word_index > 0 and stop_flag is not 0):\n",
    "                trec_test_list_index[i].append(0)\n",
    "                stop_flag = 0 \n",
    "        else:\n",
    "            trec_test_list_index[i].append(trec_vab[trec_str[word_index]])\n",
    "            stop_flag = 1\n",
    "    trec_test_list_index[i].pop(0)\n",
    "print 'test word2index finished'\n",
    "\n",
    "# imdb_vab_list 　　　　　 词表的词\n",
    "# imdb_vab_count_list 　 词表词的词频\n",
    "\n",
    "import cPickle\n",
    "TREC = {}\n",
    "TREC['Vocabulary']       = trec_vab_list\n",
    "TREC['Vab_count']        = trec_vab_count_list\n",
    "TREC['Vab_Size']         = vab_size\n",
    "TREC['Label']            = class_num\n",
    "TREC['Train_Origin']     = trec_train_list\n",
    "TREC['Train_Label']      = trec_train_label\n",
    "TREC['Train_Word_Split'] = trec_train_split\n",
    "TREC['Train_Word2Index'] = trec_train_list_index\n",
    "TREC['Test_Origin']      = trec_test_list\n",
    "TREC['Test_Label']       = trec_test_label\n",
    "TREC['Test_Word_Split']  = trec_test_split\n",
    "TREC['Test_Word2Index']  = trec_test_list_index\n",
    "cPickle.dump(TREC, open('./TREC_8k.pkl','wb'))\n",
    "print 'save finished'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save mat finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "# ELEC['Train_BOW'] = np.zeros([30000, 25000])\n",
    "# for i in range(len(ELEC['Train_Word2Index'])):\n",
    "    \n",
    "#     elec_str = ELEC['Train_Word2Index'][i]\n",
    "#     for j in range(len(elec_str)):\n",
    "        \n",
    "#         ELEC['Train_BOW'][elec_str[j],i] = ELEC['Train_BOW'][elec_str[j],i] + 1\n",
    "        \n",
    "# ELEC['Test_BOW'] = np.zeros([30000, 25000])\n",
    "# for i in range(len(ELEC['Test_Word2Index'])):\n",
    "    \n",
    "#     elec_str = ELEC['Test_Word2Index'][i]\n",
    "#     for j in range(len(elec_str)):\n",
    "        \n",
    "#         ELEC['Test_BOW'][elec_str[j],i] = ELEC['Test_BOW'][elec_str[j],i] + 1        \n",
    "        \n",
    "    \n",
    "sio.savemat('TREC_8k.mat',{'TREC_Train_Origin':     TREC['Train_Origin'],\n",
    "                           'TREC_Test_Origin':      TREC['Test_Origin'],              \n",
    "                           'TREC_Train_Word2Index': TREC['Train_Word2Index'],\n",
    "                           'TREC_Vab':              TREC['Vocabulary'],\n",
    "                           'TREC_Vab_Len':          TREC['Vab_Size'],\n",
    "                           'TREC_Test_Word2Index':  TREC['Test_Word2Index'],\n",
    "                           'TREC_Train_Label':      list(TREC['Train_Label']),\n",
    "                           'TREC_Test_Label':       list(TREC['Test_Label'])\n",
    "                           })\n",
    "\n",
    "print 'save mat finished'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
